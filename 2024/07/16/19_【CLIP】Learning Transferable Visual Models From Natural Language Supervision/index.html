<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision | 我的博客</title><meta name="author" content="MiaoJieF"><meta name="copyright" content="MiaoJieF"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="CLIP 总结：与一些自监督的预训练的方法(如MoCo、MAE)相比，CLIP最大的贡献是实现了文本和图像的对齐。  损失函数和info nce的区别：作用类似，CLIP计算交叉熵，info nce就是计算相似度后计算交叉熵  12345678910111213141516171819202122232425262728import torchimport torch.nn.functional">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision">
<meta property="og:url" content="http://example.com/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/index.html">
<meta property="og:site_name" content="我的博客">
<meta property="og:description" content="CLIP 总结：与一些自监督的预训练的方法(如MoCo、MAE)相比，CLIP最大的贡献是实现了文本和图像的对齐。  损失函数和info nce的区别：作用类似，CLIP计算交叉熵，info nce就是计算相似度后计算交叉熵  12345678910111213141516171819202122232425262728import torchimport torch.nn.functional">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/mememe.jpg">
<meta property="article:published_time" content="2024-07-15T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-23T11:15:23.039Z">
<meta property="article:author" content="MiaoJieF">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/mememe.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision",
  "url": "http://example.com/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/",
  "image": "http://example.com/img/mememe.jpg",
  "datePublished": "2024-07-15T16:00:00.000Z",
  "dateModified": "2025-09-23T11:15:23.039Z",
  "author": [
    {
      "@type": "Person",
      "name": "MiaoJieF",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/mememe.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">我的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-15T16:00:00.000Z" title="发表于 2024-07-16 00:00:00">2024-07-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-23T11:15:23.039Z" title="更新于 2025-09-23 19:15:23">2025-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>CLIP</p>
<p>总结：与一些自监督的预训练的方法(如MoCo、MAE)相比，CLIP最大的贡献是实现了文本和图像的对齐。</p>
<ul>
<li>损失函数<br>和info nce的区别：作用类似，CLIP计算交叉熵，info nce就是计算相似度后计算交叉熵</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch import nn</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def get_logits(image_features, text_features, logit_scale):</span><br><span class="line">    # 计算image_features @ text_features.T相似度矩阵</span><br><span class="line">    logits_per_image = logit_scale * image_features @ text_features.T</span><br><span class="line">    logits_per_text = logit_scale * text_features @ image_features.T</span><br><span class="line">    return logits_per_image, logits_per_text</span><br><span class="line"></span><br><span class="line">def cal_clip_loss(image_features, text_features, logit_scale):</span><br><span class="line">    device = image_features.device</span><br><span class="line">    logits_per_image, logits_per_text = get_logits(image_features, text_features, logit_scale)</span><br><span class="line">    labels = torch.arange(logits_per_image.shape[0], device=device, dtype=torch.long)</span><br><span class="line">    total_loss = (</span><br><span class="line">                         F.cross_entropy(logits_per_image, labels) +</span><br><span class="line">                         F.cross_entropy(logits_per_text, labels)</span><br><span class="line">                 ) / 2</span><br><span class="line"></span><br><span class="line">    return &#123;&quot;contrastive_loss&quot;: total_loss&#125;</span><br><span class="line"></span><br><span class="line"># 实际代码：image_features =encode_image(image, normalize=True)</span><br><span class="line">image_features = torch.randn(32, 768)</span><br><span class="line"># 实际代码：text_features =encode_text(text, normalize=True)</span><br><span class="line">text_features = torch.randn(32, 768)</span><br><span class="line">logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)).exp()</span><br><span class="line">print(cal_clip_loss(image_features, text_features, logit_scale))</span><br></pre></td></tr></table></figure>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限制的监督形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从原始文本中学习图像是一种很有前途的选择，它利用了更广泛的监督来源。我们证明了预测哪个标题与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方法，可以在从互联网收集的4亿对(图像，文本)数据集上从头开始学习SOTA图像表示。在预训练之后，使用自然语言来参考学习到的视觉概念(或描述新的概念)，从而实现模型向下游任务的zero shot转移。我们通过对30多个不同的现有计算机视觉数据集进行基准测试来研究这种方法的性能，这些数据集涵盖了OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务。该模型不平凡地转移到大多数任务，并且通常与完全监督的基线竞争，而不需要任何数据集特定的训练。例如，我们在ImageNet zero-shot上匹配原始ResNet-50的精度，而不需要使用它所训练的128万个训练样本中的任何一个。我们在<a target="_blank" rel="noopener" href="https://github.com/OpenAI/CLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/OpenAI/CLIP上发布我们的代码和预训练的模型权重。</a></p>
<h2 id="1-Introduction-and-Motivating-Work"><a href="#1-Introduction-and-Motivating-Work" class="headerlink" title="1. Introduction and Motivating Work"></a>1. Introduction and Motivating Work</h2><p>在过去几年中，直接从原始文本中学习的预训练方法彻底改变了NLP (Dai &amp; Le, 2015;Peters et al.， 2018;Howard &amp; Ruder, 2018;Rad- ford et al.， 2018;Devlin et al.， 2018;拉斐尔等人，2019)。与任务无关的目标，如自回归和屏蔽语言建模，已经在计算、模型容量和数据方面扩展了许多数量级，稳步提高了能力。“文本到文本”作为标准化输入输出接口的发展(McCann et al.， 2018;Radford等人，2019;rafael等人，2019)使任务无关架构能够zero shot转移到下游数据集，从而消除了对专门输出头或数据集特定定制的需要。像GPT-3这样的旗舰系统(Brown et al.， 2020)现在在使用定制模型的许多任务中具有竞争力，而几乎不需要特定数据集的训练数据。</p>
<p>这些结果表明，在网络规模的文本集合中，现代预训练方法可获得的总体监督优于高质量的人群标记NLP数据集。然而，在计算机视觉等其他领域，在人群标记数据集(如ImageNet)上预训练模型仍然是标准做法(Deng et al.， 2009)。直接从网络文本中学习的可扩展预训练方法能否在计算机视觉领域取得类似的突破?先前的工作令人鼓舞。</p>
<p>20多年前，Mori等人(1999)通过训练一个模型来预测与图像配对的文本文档中的名词和形容词，探索了改进基于内容的图像检索。Quattoni等人(2007)证明，通过训练分类器的权重空间中的流形学习，可以学习更有效的数据图像表示，以预测与图像相关的标题中的单词。Srivastava和Salakhutdinov(2012)通过在低级图像和文本标签特征之上训练多模态深度玻尔兹曼机(multimodal deep Boltzmann Machines)来探索深度表示学习。Joulin等人(2016)对这方面的工作进行了现代化，并证明cnn训练用来预测图像标题中的单词可以学习有用的图像表示。他们将YFCC100M数据集中图像的标题、描述和标签元数据(Thomee et al.， 2016)转换为一个词袋多标签分类任务，并表明预训练AlexNet (Krizhevsky et al.， 2012)来预测这些标签学习表征，其表现与基于imagenet的预训练在迁移任务上的表现相似。Li等人(2017)随后将这种方法扩展到预测除了单个单词之外的短语n-gram，并展示了他们的系统zero shot转移到其他图像分类数据集的能力，方法是基于他们的学习视觉n-gram字典对目标类进行评分，并预测得分最高的类别。采用最新的架构和预训练方法，VirTex (Desai &amp; Johnson, 2020)、ICMLM (Bulent Sariyildiz等，2020)和ConVIRT (Zhang等，2020)最近展示了基于transformer的语言建模、掩码语言建模和对比目标从文本中学习图像表示的潜力。</p>
<p>虽然作为概念证明令人兴奋，但使用自然语言监督进行图像表示学习仍然很少。这可能是因为在通用基准测试上的演示性能比其他方法低得多。例如，Li等人(2017)在zero shot设置下在ImageNet上仅达到11.5%的准确率。这远低于目前技术水平的88.4%的准确率(Xie et al.， 2020)。它甚至低于经典计算机视觉方法的50%准确率(Deng et al.， 2012)。相反，范围更窄但目标更明确的弱监管的使用改善了绩效。Mahajan等人(2018)表明，预测Instagram图像上与imagenet相关的标签是一项有效的预训练任务。当对ImageNet进行微调时，这些预训练模型的准确率提高了5%以上，并改善了当时的整体技术水平。Kolesnikov等人(2019)和Dosovitskiy等人(2020)也通过预训练模型来预测带有噪声标记的JFT-300M数据集的类别，在更广泛的迁移基准集上取得了巨大的收益。</p>
<p>这条工作路线代表了当前实用主义的中间立场，即从有限数量的监督“黄金标签”和从几乎无限数量的原始文本中学习之间进行学习。然而，这并非没有妥协。两者的作品都经过精心设计，并在工艺限制下，他们的监督类别分别为1000类和18291类。自然语言能够通过其通用性来表达并监督更广泛的视觉概念。这两种方法还使用静态softmax分类器来执行预测，并且缺乏动态输出的机制。这严重削弱了它们的灵活性，限制了它们的“zero shot”能力。</p>
<p>这些弱监督模型与最近直接从自然语言中学习图像表示的探索之间的一个关键区别是规模。Mahajan等人(2018)和Kolesnikov等人(2019)在数百万到数十亿张图像上训练了他们的模型，而VirTex、ICMLM和ConVIRT在一到二十万张图像上训练了加速器日。在这项工作中，我们缩小了这一差距，并研究了大规模自然语言监督训练的图像分类器的行为。在互联网上大量公开可用的这种形式的数据的支持下，我们创建了一个包含4亿对(图像，文本)的新数据集，并证明了从头开始训练的简化版本的ConVIRT，我们称之为CLIP，即对比语言-图像预训练，是一种从自然语言监督中学习的有效方法。我们通过训练一系列8个模型来研究CLIP的可扩展性，这些模型跨越了近2个数量级的计算，并观察到传输性能是计算的平滑可预测函数(hetness等人，2017;Kaplan et al.， 2020)。我们发现CLIP，类似于GPT家族，在预训练期间学习执行广泛的任务，包括OCR，地理定位，动作识别等。我们通过在30多个现有数据集上对CLIP的零射击转移性能进行基准测试来衡量这一点，并发现它可以与先前的任务特定监督模型相竞争。我们还通过线性探针表示学习分析证实了这些发现，并表明CLIP优于公开可用的最佳ImageNet模型，同时也具有更高的计算效率。我们还发现zero shot CLIP模型比同等精度监督的ImageNet模型更健壮，这表明任务不可知模型的zero shot评估更能代表模型的能力。这些结果具有重要的政策和伦理意义，我们将在第7节中加以考虑。</p>
<p><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_4.png" alt="在这里插入图片描述"></p>
<h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2. Approach"></a>2. Approach</h2><h2 id="2-1-Natural-Language-Supervision"><a href="#2-1-Natural-Language-Supervision" class="headerlink" title="2.1 Natural Language Supervision"></a>2.1 Natural Language Supervision</h2><p>我们方法的核心思想是从自然语言的监督中学习感知。正如在引言中所讨论的，这并不是一个新想法，但是用于描述这一领域工作的术语是多种多样的，甚至看起来是矛盾的，并且所陈述的动机也是多种多样的。<strong>Zhang等人(2020)、Gomez等人(2017)、Joulin等人(2016)和Desai &amp; Johnson(2020)都引入了从与图像配对的文本中学习视觉表示的方法</strong>，但分别将其方法描述为无监督、自监督、弱监督和监督。</p>
<p>我们要强调的是，这一行的共同之处并不是使用特定方法的任何细节，而是对自然语言作为训练信号的欣赏。所有这些方法都是从自然语言监督中学习的。尽管早期的工作在使用主题模型和n-gram表示时与自然语言的复杂性作斗争，但深度上下文表示学习的改进表明，我们现在有了有效利用这种丰富的监督来源的工具(McCann等人，2017)。</p>
<p>与其他训练方法相比，从自然语言中学习有几个<strong>潜在的优势</strong>。与用于图像分类的标准众包标签相比，<strong>扩展自然语言监督要容易得多</strong>，因为它不要求注释采用经典的“机器学习兼容格式”，例如标准的1-of-N多数投票“金标”。相反，研究自然语言的方法可以被动地从互联网上大量文本的监督中学习。与大多数无监督或自监督学习方法相比，<strong>从自然语言中学习也有重要的优势，因为它不仅“只是”学习表征，而且还将该表征与语言联系起来，从而实现灵活的zero shot</strong>。在接下来的小节中，我们将详细介绍我们选定的具体方法。</p>
<h3 id="2-2-Creating-a-Sufficiently-Large-Dataset"><a href="#2-2-Creating-a-Sufficiently-Large-Dataset" class="headerlink" title="2.2 Creating a Sufficiently Large Dataset"></a>2.2 Creating a Sufficiently Large Dataset</h3><p>现有工作主要使用MS-COCO (Lin et al.， 2014)、Visual Genome (Krishna et al.， 2017)和YFCC100M (Thomee et al.， 2016)三个数据集。虽然MS-COCO和Visual Genome是高质量的人群标记数据集，但按照现代标准，它们的规模很小，每个数据集大约有10万张训练照片。相比之下，其他计算机视觉系统在多达35亿张照片上进行了训练(Mahajan et al.， 2018)。YFCC100M, 1亿张照片，是一个可能的替代方案，但每张图像的元数据稀疏且质量参差不齐。许多图像使用自动生成的文件名，如20160716 113957.JPG作为“标题”或包含相机曝光设置的“描述”。经过过滤，只保留带有自然语言标题和&#x2F;或英文描述的图像后，数据集缩小了6倍，只有1500万张照片。这与ImageNet的大小大致相同。</p>
<p>自然语言监督的一个主要动机是这种形式的大量数据在互联网上公开可用。由于现有的数据集不能充分反映这种可能性，因此只考虑这些数据集的结果会低估这一研究方向的潜力。为了解决这个问题，我们<strong>构建了一个新的数据集，其中包含4亿对(图像，文本)对</strong>，这些数据来自互联网上各种公开可用的资源。为了尝试覆盖尽可能广泛的视觉概念集，我们搜索(图像，文本)对作为构建过程的一部分，其文本包含500,000个查询集中的一个我们通过每个查询包含多达20,000对(图像、文本)来近似地平衡结果。结果数据集的总字数与用于训练GPT-2的WebText数据集相似。我们<strong>将此数据集称为WebImageText的WIT</strong>。</p>
<h3 id="2-3-Selecting-an-Efficient-Pre-Training-Method"><a href="#2-3-Selecting-an-Efficient-Pre-Training-Method" class="headerlink" title="2.3 Selecting an Efficient Pre-Training Method"></a>2.3 Selecting an Efficient Pre-Training Method</h3><p>最先进的计算机视觉系统使用了大量的计算。Mahajan等人(2018)需要19个GPU年来训练他们的ResNeXt101-32x48d, Xie等人(2020)需要33个TPUv3核心年来训练他们的Noisy Student EfficientNet-L2。当考虑到这两个系统都被训练为只能预测1000个ImageNet类时，从自然语言中学习一组开放的视觉概念的任务似乎令人望而生畏。在我们的努力过程中，我们发现训练效率是成功扩展自然语言监督的关键，我们基于这个指标选择了最终的预训练方法。</p>
<p>我们最初的方法类似于VirTex，从头开始联合训练图像CNN和文本转换器来预测图像的标题。然而，我们在有效扩展这种方法时遇到了困难。在图2中，我们展示了一个6300万个参数的transformer语言模型，它已经使用了两倍的ResNet-50图像编码器的计算，学习识别ImageNet类的速度比预测相同文本的单词包编码的简单基线慢三倍。</p>
<p>这两种方法有一个关键的相似之处。他们试图预测每幅图片的准确文字。这是一项艰巨的任务，因为与图像同时出现的描述、评论和相关文本种类繁多。最近在图像对比表征学习方面的工作发现，<strong>对比目标比其等效的预测目标可以更好地学习表征(Tian et al.， 2019)</strong>。其他研究发现，尽管图像的生成模型可以学习高质量的图像表示，但与具有相同性能的对比模型相比，它们需要超过一个数量级的计算(Chen等人，2020a)。注意到这些发现，我们探索训练一个系统来解决潜在的更容易的代理任务，即只预测哪个文本作为一个整体与哪个图像配对，而不是该文本的确切单词。从相同的词袋编码基线开始，我们将预测目标替换为图2中的对比目标，并观察到零射击转移到ImageNet的效率进一步提高了4倍。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_10.png" alt="在这里插入图片描述"><br>给定一批N对(图像、文本)对，CLIP被训练来预测N对×N可能的(图像、文本)对中哪对实际发生。为此，CLIP通过联合训练图像编码器和文本编码器来学习多模态嵌入空间，以最大化批处理中N对真实对的图像和文本嵌入的余弦相似度，同时最小化N2−N对错误对的嵌入的余弦相似度。我们在这些相似性得分上优化对称交叉熵损失。在图3中，我们包含了CLIP实现的核心伪代码。据我们所知，这种批量构建技术和目标首先被引入深度度量学习领域，作为多类n对损失Sohn(2016)，由Oord等人(2018)作为InfoNCE损失推广用于对比表示学习，最近被Zhang等人(2020)用于医学成像领域的对比(文本、图像)表示学习。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_2.png" alt="在这里插入图片描述"><br>由于我们的预训练数据集规模很大，过度拟合不是主要问题，并且与Zhang等人(2020)的实现相比，训练CLIP的细节得到了简化。我们从头开始训练CLIP，而不使用ImageNet权重初始化图像编码器或使用预训练权重初始化文本编码器。我们没有使用表征和对比嵌入空间之间的非线性投影，这种变化是由Bachman等人(2019)引入的，并由Chen等人(2020b)推广。我们只使用线性投影将每个编码器的表示映射到多模态嵌入空间。我们没有注意到两个版本之间的训练效率差异，并推测非线性投影可能仅在自监督表示学习方法中与当前图像的细节共同适应。我们还从Zhang等人(2020)中删除了文本转换函数tu，该函数从文本中均匀采样单个句子，因为CLIP预训练数据集中的许多(图像，文本)对仅为单个句子。我们还简化了图像变换函数tv。<strong>从调整大小的图像中随机裁剪是训练期间使用的唯一数据增强</strong>。最后，控制softmax中对数范围的温度参数τ在训练过程中被直接优化为对数参数化的乘法标量，以避免变成超参数。</p>
<h3 id="2-4-Choosing-and-Scaling-a-Model"><a href="#2-4-Choosing-and-Scaling-a-Model" class="headerlink" title="2.4 Choosing and Scaling a Model"></a>2.4 Choosing and Scaling a Model</h3><p>我们考虑了图像编码器的两种不同架构。首先，我们使用ResNet-50 (He et al.， 2016a)作为图像编码器的基础架构，因为它被广泛采用并且性能成熟。我们使用He等人(2019)的ResNetD改进和Zhang(2019)的抗锯齿rect-2模糊池对原始版本进行了一些修改。我们还将全局平均池化层替换为注意力池化机制。注意力池被实现为一个单层的“transformer-style”多头QKV注意力，其中查询是基于图像的全局平均池表示。对于第二个架构，我们使用最近引入的视觉变压器(ViT)进行实验(Dosovitskiy等人，2020)。我们密切关注它们的实现，仅对transformer之前的combined patch和position embeddings添加了额外的层规范化，并使用略有不同的初始化方案。</p>
<p>文本编码器是一个Transformer (Vaswani et al.， 2017)，其架构修改在Radford et al.(2019)中描述。作为基本尺寸，我们使用一个具有8个注意头的63m参数的12层512宽模型。转换器在49,152词汇表大小的文本的小写字节对编码(BPE)表示上运行(Sennrich et al.， 2015)。为了计算效率，最大序列长度被限制在76。<strong>文本序列用[SOS]和[EOS]令牌括起来</strong>，<strong>[EOS]令牌处变压器最高层的激活被视为文本的特征表示</strong>，文本被层归一化，然后线性投影到多模态嵌入空间中。在文本编码器中使用了隐藏的自注意，以保留使用预训练的语言模型进行初始化或添加语言建模作为辅助目标的能力，尽管对这方面的探索有待于未来的工作。</p>
<p>虽然以前的计算机视觉研究通常通过单独增加宽度(Mahajan et al.， 2018)或深度(He et al.， 2016a)来缩放模型，但对于ResNet图像编码器，我们采用了Tan和Le(2019)的方法，该方法发现在所有宽度、深度和分辨率上分配额外的计算比仅将其分配到模型的一个维度更好。虽然Tan和Le(2019)调整了分配给其EfficientNet架构的每个维度的计算比例，但我们使用了一个简单的基线，平均分配额外的计算来增加模型的宽度、深度和分辨率。对于文本编码器，我们只缩放模型的宽度，使其与ResNet宽度的计算增量成正比，而根本不缩放深度，因为我们发现<strong>CLIP的性能对文本编码器的容量不太敏感</strong>。</p>
<h3 id="2-5-Training"><a href="#2-5-Training" class="headerlink" title="2.5 Training"></a>2.5 Training</h3><p>我们训练了一系列的5个resnet和3个Vision transformer。对于resnet，我们训练了一个ResNet-50，一个ResNet-101，然后还有3个遵循效率网风格的模型缩放，并使用大约4倍，16倍和64倍的ResNet-50计算。分别记为RN50x4、RN50x16、RN50x64。对于视觉变压器，我们训练了一个vitb &#x2F;32，一个vitb &#x2F;16和一个vitl &#x2F;14。我们训练了32个epochs的所有模型。我们将<strong>Adam优化器</strong>(Kingma &amp; Ba, 2014)与d<strong>ecoupled weight decay</strong> regularization(Loshchilov &amp; Hutter, 2017)应用于所有非增益或偏差的权重，并使用余弦调度衰减学习率(Loshchilov &amp; Hutter, 2016)。初始超参数设置使用网格搜索，随机搜索和手动调整基线ResNet50模型的组合，当训练为1 epoch时。然后，由于计算约束，超参数被启发式地适应于更大的模型。<strong>可学习的温度参数τ被初始化为(Wu et al.， 2018)中的0.07，并被截断以防止对数缩放超过100</strong>，我们发现这是<strong>防止训练不稳定所必需的</strong>。我们使用非常大的32,768个小批量。混合精度(Micikevicius et al.， 2017)用于加速训练和节省内存。为了节省额外的内存，梯度检查点(Griewank &amp; Walther, 2000;Chen et al.， 2016)、半精度Adam统计(Dhariwal et al.， 2020)和半精度随机四舍五入文本编码器权重。嵌入相似度的计算也被分片，单个gpu只计算其局部批次嵌入所需的成对相似度的子集。最大的ResNet模型RN50x64在592个V100 gpu上花了18天的时间进行训练，而最大的Vision Transformer在256个V100 gpu上花了12天的时间。对于ViT-L&#x2F;14，我们还以更高的336像素分辨率预训练一个额外的epoch，以提高类似于FixRes的性能(Touvron等人，2019)。我们将此模型记为ViT-L&#x2F;14@336px。除非另有说明，<strong>本文报告的所有结果都是“CLIP”，使用我们发现表现最好的模型</strong>。</p>
<h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-Zero-Shot-Transfer"><a href="#3-1-Zero-Shot-Transfer" class="headerlink" title="3.1 Zero-Shot Transfer"></a>3.1 Zero-Shot Transfer</h3><h4 id="3-1-1-MOTIVATION"><a href="#3-1-1-MOTIVATION" class="headerlink" title="3.1.1 MOTIVATION"></a>3.1.1 MOTIVATION</h4><p>在计算机视觉中，zero shot学习通常是指对图像分类中未见过的物体类别进行泛化的研究(Lampert et al.， 2009)。相反，我们在更广泛的意义上使用这个术语，并研究对未见过的数据集的泛化。我们将其作为执行看不见的任务的代理，正如Larochelle等人(2008)在零数据学习论文中所期望的那样。虽然无监督学习领域的许多研究都集中在机器学习系统的表示学习能力上，但我们鼓励研究零迁移作为衡量机器学习系统任务学习能力的一种方法。在此视图中，数据集评估特定分布上任务的性能。然而，许多流行的计算机视觉数据集是由研究界创建的，主要是作为指导通用图像分类方法开发的基准，而不是衡量特定任务的性能。虽然可以合理地说SVHN数据集测量了谷歌街景照片分布上的街道号码转录任务，但尚不清楚CIFAR-10数据集测量的是什么“真正的”任务。然而，很明显，CIFAR-10是从TinyImages中提取的分布(Torralba et al.， 2008)。在这类数据集上，零概率迁移更多的是评价CLIP对分布偏移和领域泛化的鲁棒性，而不是任务泛化。请参见第3.3节对此进行分析。</p>
<p>据我们所知，Visual N-Grams (Li et al.， 2017)首先以上述方式研究了零镜头转移到现有图像分类数据集的方法。这也是我们所知道的唯一一项使用一般预训练模型研究零射击转移到标准图像分类数据集的工作，并作为上下文化CLIP的最佳参考点。他们的方法学习了一个包含142,806个视觉n-gram(跨越1到5克)的字典的参数，并使用Jelinek-Mercer平滑的不同版本来优化这些n-gram，以最大化给定图像中所有文本n-gram的概率。为了执行zero shot转移，他们首先将数据集的每个类名的文本转换为其n-gram表示，然后根据他们的模型计算其概率，预测得分最高的一个。</p>
<p>我们之所以关注zero shot作为任务学习的一种评价，是因为受到了NLP领域中任务学习研究的启发。据我们所知，Liu等人(2018)首先将任务学习确定为一种“意想不到的副作用”，即训练生成维基百科文章的语言模型学会了在语言之间可靠地音译名称。虽然GPT-1 (Radford et al.， 2018)专注于将预训练作为一种迁移学习方法来改进监督微调，但它还包括一项烧烧研究，该研究表明，四种启发式zero shot迁移方法的性能在预训练过程中稳步提高，无需任何监督适应。该分析是GPT-2 (Radford et al.， 2019)的基础，GPT-2专注于通过zero shot研究语言模型的任务学习能力。</p>
<h4 id="3-1-2-USING-CLIP-FOR-ZERO-SHOT-TRANSFER"><a href="#3-1-2-USING-CLIP-FOR-ZERO-SHOT-TRANSFER" class="headerlink" title="3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER"></a>3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER</h4><p>CLIP被预先训练来预测图像和文本片段是否在其数据集中配对在一起。为了执行zero shot分类，我们重用此功能。对于每个数据集，我们使用数据集中所有类的名称作为潜在文本配对的集合，并根据CLIP预测最可能的(图像，文本)配对。更详细地说，我们首先通过各自的编码器计算图像的特征嵌入和可能文本集的特征嵌入。然后计算这些嵌入的余弦相似度，通过温度参数τ缩放，并通过softmax归一化为概率分布。请注意，该预测层是一个多项逻辑回归分类器，具有l2归一化输入、l2归一化权重、无偏差和温度缩放。当以这种方式解释时，图像编码器是计算图像特征表示的计算机视觉骨干，文本编码器是一个超网络(Ha et al.， 2016)，它根据指定类所代表的视觉概念的文本生成线性分类器的权重。Lei Ba等人(2015)首先引入了这种形式的零采样图像分类器，而从自然语言生成分类器的想法至少可以追溯到Elhoseiny等人(2013)。继续这种解释，CLIP预训练的每一步都可以被视为优化随机创建的代理到计算机视觉数据集的性能，该数据集每个类包含1个示例，通过自然语言描述定义了32,768个类。对于zero shot评估，一旦文本编码器计算出zero shot分类器，我们就缓存它，并在所有后续预测中重用它。这允许在数据集中的所有预测中平摊生成它的成本。</p>
<h4 id="3-1-3-INITIAL-COMPARISON-TO-VISUAL-N-GRAMS"><a href="#3-1-3-INITIAL-COMPARISON-TO-VISUAL-N-GRAMS" class="headerlink" title="3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS"></a>3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS</h4><p>和基于概率统计的方法比较<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_1.png" alt="在这里插入图片描述"></p>
<h4 id="3-1-4-PROMPT-ENGINEERING-AND-ENSEMBLING"><a href="#3-1-4-PROMPT-ENGINEERING-AND-ENSEMBLING" class="headerlink" title="3.1.4 PROMPT ENGINEERING AND ENSEMBLING"></a>3.1.4 PROMPT ENGINEERING AND ENSEMBLING</h4><p>大多数标准图像分类数据集都将命名或描述类的信息视为事后的想法，这使得基于自然语言的零采样传输成为可能。绝大多数数据集仅使用标签的数字id对图像进行注释，并包含将这些id映射回其英文名称的文件。有些数据集，如Flowers102和GTSRB，在其发布版本中似乎根本不包括这种映射，从而完全阻止了zero shot传输对于许多数据集，我们观察到这些标签的选择可能有些随意，并且没有预料到与zero shot转移相关的问题，zero shot转移依赖于任务描述以成功转移。</p>
<p><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_11.png" alt="在这里插入图片描述"><br>一个常见的问题是一词多义。当类的名称是提供给CLIP的文本编码器的唯一信息时，由于缺乏上下文，它无法区分指的是哪个词义。在某些情况下，同一个单词的多个含义可能会作为不同的类包含在同一个数据集中!这发生在ImageNet中，它包含cranes和cranes that fly。另一个例子是在Oxford-IIIT Pet数据集的类中发现的，从上下文来看，单词boxer显然指的是一种狗，但对于缺乏上下文的文本编码器来说，它很可能指的是一种运动员。</p>
<p>我们遇到的另一个问题是，在我们的预训练数据集中，与图像配对的文本只有一个单词是相对罕见的。通常文本是一个完整的句子，以某种方式描述图像。为了帮助弥合这种分布差距，我们发现使用提示模板“A photo of A {label}.”是一个很好的默认值，可以帮助指定文本是关于图像的内容。与仅使用标签文本的基线相比，这通常可以提高性能。例如，仅仅使用这个提示符就可以将ImageNet上的准确率提高1.3%。</p>
<p>类似于围绕GPT3的“提示工程”讨论(Brown et al.， 2020;Gao et al.， 2020)，我们还观察到，通过为每个任务定制提示文本可以显著提高零射击性能。下面是一些不详尽的例子。我们在几个细粒度的图像分类数据集中发现，它有助于指定类别。例如，在Oxford-IIIT Pets中，使用“A photo of a {label}, a type of pet.”。来帮助提供背景，效果很好。同样，在Food101上指定一种食物类型和在FGVC飞机上指定一种飞机类型也有所帮助。对于OCR数据集，我们发现在要识别的文本或数字周围加上引号可以提高性能。最后，我们发现在卫星图像分类数据集中，它有助于指定图像是这种形式，我们使用“a satellite photo of a {label}.”的变体。</p>
<p>我们还尝试了在多个零射击分类器上进行集成，作为提高性能的另一种方法。这些分类器是通过使用不同的上下文提示来计算的，例如‘A photo of a big {label}”和“A photo of a small {label}”。我们<strong>在嵌入空间而不是概率空间上构造集成</strong>。这允许我们缓存一组平均文本嵌入，这样当在许多预测上平摊时，集成的计算成本与使用单个分类器相同。我们已经观察到在许多生成的zero shot分类器上进行集成，以可靠地提高性能，并将其用于大多数数据集。在ImageNet上，我们集成了80个不同的上下文提示，这比上面讨论的单个默认提示额外提高了3.5%的性能。当一起考虑时，prompt工程和集成可以将ImageNet的精度提高近5%。在图4中，我们可视化了与Li等人(2017)所做的直接嵌入类名的无上下文基线方法相比，快速工程和集成如何改变一组CLIP模型的性能。</p>
<h4 id="3-1-5-ANALYSIS-OF-ZERO-SHOT-CLIP-PERFORMANCE"><a href="#3-1-5-ANALYSIS-OF-ZERO-SHOT-CLIP-PERFORMANCE" class="headerlink" title="3.1.5 ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE"></a>3.1.5 ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE</h4><p>由于计算机视觉的任务无关的零射击分类器已经得到了充分的研究，CLIP提供了一个有希望的机会来更好地理解这种类型的模型。在本节中，我们对CLIP的zero shot分类器的各种属性进行了研究。作为第一个问题，我们简单地看看零命中率分类器的表现如何。为了说明这一点，我们比较了一个简单的现成基线的性能:在规范ResNet-50的特征上拟合一个完全监督的、正则化的逻辑回归分类器。在图5中，我们展示了跨27个数据集的比较。有关数据集和设置的详细信息，请参见附录A。(<strong>这里的resnet的checkpoint应该是再ImageNet上的，比较有点不公平</strong>)<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_12.png" alt="在这里插入图片描述"><br>Zero-shot CLIP在27个数据集中的16个数据集上的表现略优于此基线。查看单个数据集会发现一些有趣的行为。在细粒度分类任务上，我们观察到性能上的广泛差异。在其中两个数据集(Stanford Cars和Food101)上，零射击CLIP在ResNet-50特征上的表现比逻辑回归好20%以上，而在另外两个数据集(Flowers102和fgvc - aircraft)上，零射击CLIP的表现比逻辑回归差10%以上。在OxfordPets和Birdsnap上，二者的表现更为接近。我们怀疑这些差异主要是由于WIT和ImageNet之间每个任务监督的数量不同。在ImageNet、CIFAR10&#x2F;100、STL10和PascalVOC2007等“一般”对象分类数据集上，性能相对相似，在所有情况下，零射击CLIP都有轻微的优势。在STL10上，CLIP总体达到99.3%，尽管没有使用任何训练样例，但这似乎是一个新的SOTA。Zeroshot CLIP在视频中测量动作识别的两个数据集上显着优于ResNet-50。在Kinetics700上，CLIP的表现比ResNet-50高出14.5%。在UCF101上，Zeroshot CLIP的性能也比ResNet-50的性能高出7.7%。我们推测这是由于与ImageNet中以名词为中心的对象监督相比，自然语言对涉及动词的视觉概念提供了更广泛的监督。（表现不一致的原因值得分析一下）</p>
<p>看看零射击CLIP明显表现不佳的地方，我们看到零射击CLIP在几个专门的、复杂的或抽象的任务上相当弱，比如卫星图像分类(EuroSAT和RESISC45)、淋巴结肿瘤检测(PatchCamelyon)、合成场景中的物体计数(CLEVRCounts)、自动驾驶相关的任务，比如德国交通标志识别(GTSRB)、识别到最近的汽车的距离(KITTI distance)。这些结果突出了零射击CLIP在更复杂任务中的较差能力。相比之下，非专业的人类可以稳健地执行其中的一些任务，如计数、卫星图像分类和交通标志识别，这表明还有很大的改进空间。然而，我们需要提醒的是，目前尚不清楚对于学习者之前没有经验的困难任务，如几乎所有人的淋巴结肿瘤分类(可能还有CLIP)，测量零射击转移(而不是少射击转移)是否是有意义的评估。</p>
<p>虽然将零射击性能与完全监督模型进行比较可以将CLIP的任务学习能力置于上下文环境中，但与少射击方法进行比较是更直接的比较，因为零射击是其极限。在图6中，我们可视化了在许多图像模型(包括最好的公开可用的ImageNet模型、自监督学习方法和CLIP本身)的特征上，零采样CLIP与少采样逻辑回归的比较。虽然直观地认为零射击不如一射击，但我们发现零射击CLIP在相同的特征空间上与四射击逻辑回归的性能相匹配。这可能是由于零射击和少射击方法之间的重要区别。首先，CLIP的零射击分类器是通过自然语言生成的，它允许直接指定视觉概念(“交流”)。相比之下，“正常的”监督学习必须间接地从训练样本中推断概念。无上下文的基于实例的学习有一个缺点，即许多不同的假设可以与数据一致，特别是在一次性案例中。一张图片通常包含许多不同的视觉概念。虽然一个有能力的学习者能够利用视觉线索和启发式，比如假设被演示的概念是图像中的主要对象，但这并不能保证。</p>
<p><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_3.png" alt="在这里插入图片描述"><br>zero-shot和few-shot性能之间这种差异的一个潜在解决方案是使用CLIP的zero-shot分类器作为few-shot分类器权重的先验。虽然对生成的权重添加L2惩罚是这个想法的直接实现，但我们发现超参数优化通常会选择这个正则器的如此大的值，以至于得到的少射分类器“只是”零射分类器。研究将零弹迁移的强度与少弹学习的灵活性更好地结合起来的方法是未来工作的一个有希望的方向。</p>
<p>当在其他模型的特征上比较零射击CLIP和少射击逻辑回归时，零射击CLIP的性能大致与我们的评估套件中表现最好的16射击分类器的性能相匹配，该评估套件使用了在ImageNet-21K上训练的BiT-M ResNet-152x2的特征。我们确信，在JFT-300M上训练的BiT-L模型会表现得更好，但这些模型尚未公开发布。BiT-M ResNet-152x2在16次射击设置中表现最好，这有点令人惊讶，因为正如第3.2节所分析的那样，Noisy Student EfficientNet-L2在27个数据集的完全监督设置中平均比它高出近5%。</p>
<p>除了研究零射击CLIP和少射击逻辑回归的平均性能外，我们还研究了单个数据集的性能。在图7中，我们显示了相同特征空间上的逻辑回归分类器需要匹配零采样CLIP性能的每个类的标记示例数量的估计。由于零弹CLIP也是一个线性分类器，这估计了在这种设置下零弹传输的有效数据效率。为了避免训练数千个线性分类器，我们基于1、2、4、8、16个镜头(如果可能的话)的性能的对数插值和在每个数据集上训练的完全监督线性分类器来估计有效的数据效率。我们发现，在每个数据集上，从每个类不到1个标记样本到184个标记样本的效率差异很大。Flowers102和EuroSAT这两个数据集的表现不如一次性模型。一半的数据集需要每个类少于5个样本，中位数为5.4。然而，平均估计的数据效率是每个类20.8个样本。这是由于20%的数据集中，监督分类器为了匹配性能，每个类需要许多标记的示例。在ImageNet上，零射击CLIP与在相同特征空间上训练的16射击线性分类器的性能相匹配。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_5.png" alt="在这里插入图片描述"><br>如果我们假设评估数据集足够大，可以很好地估计在其上训练的线性分类器的参数，那么，由于CLIP的零射击分类器也是一个线性分类器，因此完全监督分类器的性能大致设置了零射击转移可以实现的上界。在图8中，我们将CLIP的零射击性能与跨数据集的完全监督线性分类器进行了比较。虚线，y &#x3D; x表示一个“最优”的零射击分类器，其性能与完全监督的等效分类器相匹配。对于大多数数据集，零射击分类器的性能仍然比完全监督分类器差10%到25%，这表明CLIP的任务学习和零射击转移能力仍有很大的提升空间。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_13.png" alt="在这里插入图片描述"><br>零射击性能与完全监督性能之间存在0.82 (p值&lt; 10−6)的正相关，这表明CLIP在将底层表征和任务学习与零射击迁移联系起来方面是相对一致的。然而，零射击CLIP仅在5个数据集上接近完全监督性能:STL10, CIFAR10, Food101, OxfordPets和Caltech101。在所有5个数据集上，零射击准确率和完全监督准确率都超过90%。这表明CLIP在零射击转移任务中可能更有效，因为它的底层表征也是高质量的。预测零投性能的线性回归模型的斜率作为完全监督性能的函数估计，在完全监督性能上每提高1%，零投性能就提高1.28%。然而，第95百分位置信区间仍然包括小于1的值(0.93-1.79)。</p>
<p>在过去的几年里，深度学习系统的实证研究已经证明，性能是可预测的，是训练计算和数据集大小等重要数量的函数(hetness等人，2017;Kaplan et al.， 2020)。到目前为止，GPT系列模型已经在训练计算中增加了1000倍，从而在零射击性能方面取得了一致的改进。在图9中，我们检查CLIP的零射击性能是否遵循类似的缩放模式。我们绘制了5个ResNet CLIP模型在36个不同数据集上的39个评估中的平均错误率，并发现CLIP在模型计算增加44倍的情况下具有类似的对数-对数线性缩放趋势。虽然总体趋势是平稳的，但我们发现，个人评估的表现可能会更加嘈杂。我们不确定这是由于子任务上的单个训练运行之间的高差异(如D ‘Amour等人(2020)所述)掩盖了稳步提高的趋势，还是由于性能实际上是非单调的，作为某些任务的计算函数。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_8.png" alt="在这里插入图片描述"></p>
<h3 id="3-2-Representation-Learning"><a href="#3-2-Representation-Learning" class="headerlink" title="3.2 Representation Learning"></a>3.2 Representation Learning</h3><p>虽然我们在上一节中通过零射击迁移广泛分析了CLIP的任务学习能力，但更常见的是研究模型的表示学习能力。有许多方法可以评估表征的质量，以及对“理想”表征应该具有哪些属性的分歧(Locatello et al.， 2020)。在从模型中提取的表示上拟合线性分类器并测量其在各种数据集上的性能是一种常见的方法。另一种方法是测量模型端到端微调的性能。这增加了灵活性，先前的工作已经令人信服地证明，在大多数图像分类数据集上，微调优于线性分类(Kornblith等人，2019;翟等人，2019)。虽然由于实际原因，微调的高性能激发了它的研究，但出于几个原因，我们仍然选择基于线性分类器的评估。我们的工作重点是开发一种高性能的任务和数据集不可知的预训练方法。微调，因为它在微调阶段适应每个数据集的表示，可以补偿并潜在地掩盖在预训练阶段学习通用和鲁棒表示的失败。线性分类器，由于其有限的灵活性，而是强调这些失败，并在开发过程中提供明确的反馈。对于CLIP，训练监督线性分类器还有一个额外的好处，它与用于零射击分类器的方法非常相似，可以在3.1节中进行广泛的比较和分析。最后，我们的目标是将CLIP与跨许多任务的一组全面的现有模型进行比较。在27个不同的数据集上研究66个不同的模型需要调整1782个不同的评估。 微调打开了一个更大的设计和超参数空间，这使得很难公平评估和计算昂贵，以比较其他大规模实证研究中讨论的各种技术(Lucic等人，2018;Choi et al., 2019).相比之下，线性分类器需要最小的超参数调优，并且具有标准化的实现和评估过程。有关评估的进一步详情，请参阅附录A。</p>
<p>图10总结了我们的发现。为了尽量减少可能引起确认或报告偏差担忧的选择效应，我们首先研究了Kornblith等人(2019)的12个数据集评估套件的性能。虽然像ResNet-50和ResNet-101这样的小型CLIP模型比在ImageNet-1K (BiT-S和原件)上训练的其他resnet表现更好，但它们比在ImageNet-21K (BiTM)上训练的resnet表现更差。这些小型CLIP模型在具有类似计算需求的情况下，也不如EfficientNet系列中的模型。然而，使用CLIP尺度训练的模型非常好，我们训练的最大模型(ResNet-50x64)在总体得分和计算效率方面略优于表现最好的现有模型(Noisy Student EfficientNet- L2)。我们还发现CLIP视觉变压器的计算效率比CLIP ResNets高3倍，这使我们能够在我们的计算预算内达到更高的整体性能。这些结果定性地重复了Dosovitskiy等人(2020)的研究结果，该研究报告称，在足够大的数据集上训练时，视觉转换器比convnets的计算效率更高。我们最好的整体模型是ViT-L&#x2F;14，它在我们的数据集上以336像素的更高分辨率进行了一次额外的epoch微调。该模型比该评估套件中最好的现有模型平均高出2.6%。<br><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_9.png" alt="在这里插入图片描述"><br>如图21定性所示，CLIP模型学习的任务比之前在随机初始化训练的单个计算机视觉模型中所展示的更广泛。这些任务包括地理定位、光学字符识别、面部情感识别和动作识别。Kornblith等人(2019)的评估套件中没有测量这些任务。这可以被认为是Kornblith等人(2019)对与ImageNet重叠的任务的研究中的一种选择偏差。为了解决这个问题，我们还在更广泛的27个数据集评估套件上测量性能。该评估套件(详见附录A)包括代表上述任务的数据集、德国交通标志识别基准(Stallkamp等人，2011)，以及改编自VTAB的其他几个数据集(Zhai等人，2019)。</p>
<p>略</p>
<h2 id="4-Comparison-to-Human-Performance"><a href="#4-Comparison-to-Human-Performance" class="headerlink" title="4. Comparison to Human Performance"></a>4. Comparison to Human Performance</h2><p>CLIP与人类表现和人类学习相比如何?为了更好地了解人类在与CLIP类似的评估设置中的表现，我们在一项任务中对人类进行了评估。我们想要了解人类在这些任务中的零射击表现有多强，以及如果向他们展示一两个图像样本，人类的表现会提高多少。这可以帮助我们比较人类和CLIP的任务难度，并确定它们之间的相关性和差异。</p>
<p>我们让五个不同的人查看牛津IIT宠物数据集(Parkhi等人，2012)的3669张图片中的每一张，并从37个猫或狗品种中选择最匹配图像的哪一个(或者“我不知道”，如果他们完全不确定)。在零命中率的情况下，人类没有得到任何品种的例子，并要求他们在没有互联网搜索的情况下尽其所能给它们贴上标签。在一次实验中，每个品种给人类一张样本图像，在两次实验中，每个品种给人类两张样本图像。</p>
<p>一个可能的担忧是，人类工人在零射击任务中没有足够的动力。人类在STL-10数据集上的准确率高达94% (Coates et al.， 2011)，在注意力检查图像子集上的准确率为97-100%，这增加了我们对人类工作人员的信任。</p>
<p><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_7.png" alt="在这里插入图片描述"><br>有趣的是，每个类只有一个训练样例，人类的平均性能从54%提高到76%，而额外的训练样例的边际收益是最小的。准确度从0到1的提高几乎完全是基于人类不确定的图像。这表明人类“知道他们不知道什么”，并且能够根据一个例子更新他们对最不确定的图像的先验。鉴于此，CLIP似乎是一种很有前途的零射击性能训练策略(图5)，并且在自然分布转移测试中表现良好(图13)，但人类从少数例子中学习的方式与本文中使用的少数射击方法之间存在很大差异。</p>
<p>这表明，如Lake等人(2016)等人所指出的，仍有算法改进有待改进，以缩小机器和人类样本效率之间的差距。由于CLIP的这些小镜头评估并没有有效地利用先验知识，而人类却利用了先验知识，因此我们推测，寻找一种将先验知识适当地整合到小镜头学习中的方法是改进CLIP算法的重要一步。据我们所知，在高质量预训练模型的特征之上使用线性分类器对于少镜头学习来说是最先进的(Tian et al.， 2020)，这表明最好的少镜头机器学习方法与人类少镜头学习之间存在差距。</p>
<p>如果我们将人类的精度与CLIP的零射击精度进行对比(图16)，我们会发现CLIP最难的问题对人类来说也很难。在某种程度上，误差是一致的，我们的假设是，这是由于至少两个因素:数据集中的噪声(包括错误标记的图像)和超出分布的图像对人类和模型来说都很困难。</p>
<p><img src="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/img_6.png" alt="在这里插入图片描述"><br>略</p>
<h2 id="6-Limitations"><a href="#6-Limitations" class="headerlink" title="6. Limitations"></a>6. Limitations</h2><p>CLIP仍然有许多限制。虽然其中一些将在各个部分中作为分析的一部分进行讨论，但我们在这里总结并收集它们。</p>
<p>在具有训练分割的数据集上，zero-shot CLIP的性能平均可与基于ResNet-50特征的线性分类器的简单监督基线相媲美。在大多数这些数据集上，该基线的性能现在远低于整体水平。在提高CLIP的任务学习和迁移能力方面还需要做大量的工作。虽然到目前为止，扩展已经稳步提高了性能，并为持续改进提供了一条途径，但我们估计，零射击CLIP要达到最先进的整体性能，需要大约1000倍的计算能力增加。用目前的硬件进行训练是不可行的。进一步研究提高CLIP的计算和数据效率是必要的。（算力要求高）</p>
<p>在3.1节的分析中发现，CLIP的zero shot性能在几种任务上仍然很弱。与特定任务模型相比，CLIP在几种类型的细粒度分类(如区分汽车模型、花卉种类和飞机变体)上的性能较差。CLIP还在处理更抽象和系统的任务，如计算图像中物体的数量。最后，对于不太可能包含在CLIP预训练数据集中的新任务，例如对照片中最近的汽车的距离进行分类，CLIP的性能可以接近随机。我们相信，在许多任务中，CLIP的零射击性能接近随机水平。</p>
<p>尽管在3.3节中研究了零射击CLIP对许多自然图像分布的泛化效果很好，但我们已经观察到零射击CLIP对真正不在其分布范围内的数据的泛化效果仍然很差。在附录e中报告的OCR任务中出现了一个说明性的测试示例。CLIP学习了高质量的语义OCR表示，该表示在数字渲染文本上表现良好，这在其预训练数据集中很常见，在渲染SST2上的性能证明了这一点。然而，CLIP在mnist的手写数字上只能达到88%的准确率。对原始像素进行简单的逻辑回归基线，其性能优于零射击CLIP。语义检索和近重复最近邻检索都验证了在我们的预训练数据集中几乎没有与MNIST数字相似的图像。这表明CLIP在解决深度学习模型的脆性泛化的潜在问题方面做得很少。相反，CLIP试图规避这个问题，并希望通过在如此庞大和多样化的数据集上进行训练，所有数据都将有效地分布。这是一个天真的假设，正如MNIST所证明的那样，很容易被违背。</p>
<p>尽管CLIP可以灵活地为各种各样的任务和数据集生成zero shot分类器，但CLIP仍然仅限于从给定zero shot分类器中的那些概念中进行选择。与真正灵活的方法(如图像字幕)相比，这是一个重大的限制，因为图像字幕可以产生新颖的输出。不幸的是，如2.3节所述，我们发现我们尝试的图像标题基线的计算效率远低于CLIP。一个值得尝试的简单想法是联合训练一个对比和生成目标，希望将CLIP的效率与标题模型的灵活性结合起来。作为另一种替代方案，可以在给定图像的许多自然语言解释的推理时间执行搜索，类似于使用潜在语言学习Andreas等人(2017)中提出的方法。（就是说CLIP不能够做生成）</p>
<p>CLIP也没有解决深度学习的数据效率差的问题。相反，CLIP通过使用一个可以扩展到数亿个训练示例的监督源来进行补偿。如果在CLIP模型的训练过程中看到的每张图像都以每秒一张的速度呈现，那么在32个训练时代中看到的128亿张图像将需要405年的时间来迭代。结合CLIP与自我监督(Henaff, 2020;Chen et al.， 2020c)和自我训练(Lee;Xie等人，2020)方法是一个有前途的方向，因为它们证明了比标准监督学习提高数据效率的能力。</p>
<p>我们的方法有几个明显的局限性。尽管我们关注的是zero shot转移，但我们在完整验证集上反复查询性能，以指导CLIP的开发。这些验证集通常有数千个示例，这对于真正的zero shot场景来说是不现实的。在半监督学习领域也提出了类似的担忧(Oliver et al.， 2018)。另一个潜在的问题是我们对评估数据集的选择。虽然我们已经报告了Kornblith等人(2019)的12个数据集评估套件作为标准化集合的结果，但我们的主要结果使用了27个数据集的随机集合，不可否认，这些数据集与CLIP的开发和能力是共同适应的。创建一个新的任务基准，明确地评估广泛的zero shot转移能力，而不是重新使用现有的监督数据集，将有助于解决这些问题。（测试基准缺乏）</p>
<p>CLIP是通过与互联网上的图像配对的文本进行训练的。这些图像-文本对未经过滤和管理，导致CLIP模型学习许多社会偏见。之前已经在图像标题模型中证明了这一点(Bhargava &amp; Forsyth, 2019)。我们请读者参阅第7节，了解针对CLIP的这些行为的详细分析和量化，以及对潜在缓解策略的讨论。（数据不干净）</p>
<p>虽然我们在整个工作中强调，通过自然语言指定图像分类器是一种灵活和通用的接口，但它有自己的局限性。许多复杂的任务和视觉概念很难通过文本来指定。实际的训练示例无疑是有用的，但CLIP并不直接优化few shot性能。在我们的工作中，我们回归到在CLIP的特征之上拟合线性分类器。这将导致从零射击过渡到少射击设置时性能的反直觉下降。正如第4节所讨论的，这与人类的表现明显不同，后者显示了从zero到one shot设置的大幅增加。未来的工作需要开发将CLIP强大的zero shot性能与有效的few shot学习相结合的方法。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">MiaoJieF</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/">http://example.com/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">我的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post-share"><div class="social-share" data-image="/img/mememe.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/07/21/20_%E3%80%90BLIP%E3%80%91Bootstrapping%20Language-Image%20Pre-training/" title="【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training</div></div><div class="info-2"><div class="info-item-1">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 总结 针对问题（1）基于编码器的模型不太容易直接转移到文本生成任务(例如图像caption)，而编码器-解码器模型尚未成功地用于图像-文本检索任务（2）现有方法使用的网络数据集存在噪声，带来次优结果 解决方法（1）提出编码器-解码器的多模态混合架构，结合检索和生成caption的功能（2）一种新的数据集bootstrap方法：先预训练，然后用精标注数据集微调，通过captioner生成数据，通过filter过滤数据 注意的问题和细节（1）ITM Loss的作用：细粒度上的对齐（2）为什么使用LM Loss而不使用MLM Loss：使用LM loss才能让模型具有生成能力（3）在常规训练时，文本编码器和文本解码器共享除SA层之外的所有参数：原因是编码和解码任务之间的差异最好由SA层捕获，特别是，编码器使用双向自注意来构建当前输入标记的表示，而解码器使用因果自注意来预测下一个标...</div></div></div></a><a class="pagination-related" href="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" title="【论文阅读】GWC-Net代码阅读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">【论文阅读】GWC-Net代码阅读</div></div><div class="info-2"><div class="info-item-1">代码中关于网络定义的部分其实还是挺好懂的，所以本文的重点是cancatenation volume和group-wise volume这部分（建议没有阅读过论文的阅读论文后再看）  1. Cost volume的构建1.1 在stereo matching中，构建cost volume的步骤：1）确定一系列离散的假设视差平面2）将提取的每个视图的 2D 特征warp到假设平面3）最后融合在一起构建 3D 或 4D cost volume 1.2 双目深度中的warp定义：由于左右图像都经过校正，坐标映射由x轴方向的偏移量决定：其中 Cr(d) 是指视差 d 处右视图的变换 x 轴坐标，而Xl是左视图的源 x 轴坐标。简单地就是说将右图沿x轴移动不同的视差d。  2. GWC网络结构看不懂的先看论文  3. Cost volumegroup-wise volume提供了特征向量匹配的相似度，而concat volume提供了语义信息作为补充，两者是互补的 下面结合代码理解concat volume和group-wise volume的具体构建 3.1 Feature volumeF...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/07/21/20_%E3%80%90BLIP%E3%80%91Bootstrapping%20Language-Image%20Pre-training/" title="【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-21</div><div class="info-item-2">【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training</div></div><div class="info-2"><div class="info-item-1">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 总结 针对问题（1）基于编码器的模型不太容易直接转移到文本生成任务(例如图像caption)，而编码器-解码器模型尚未成功地用于图像-文本检索任务（2）现有方法使用的网络数据集存在噪声，带来次优结果 解决方法（1）提出编码器-解码器的多模态混合架构，结合检索和生成caption的功能（2）一种新的数据集bootstrap方法：先预训练，然后用精标注数据集微调，通过captioner生成数据，通过filter过滤数据 注意的问题和细节（1）ITM Loss的作用：细粒度上的对齐（2）为什么使用LM Loss而不使用MLM Loss：使用LM loss才能让模型具有生成能力（3）在常规训练时，文本编码器和文本解码器共享除SA层之外的所有参数：原因是编码和解码任务之间的差异最好由SA层捕获，特别是，编码器使用双向自注意来构建当前输入标记的表示，而解码器使用因果自注意来预测下一个标...</div></div></div></a><a class="pagination-related" href="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" title="【论文阅读】GWC-Net代码阅读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-30</div><div class="info-item-2">【论文阅读】GWC-Net代码阅读</div></div><div class="info-2"><div class="info-item-1">代码中关于网络定义的部分其实还是挺好懂的，所以本文的重点是cancatenation volume和group-wise volume这部分（建议没有阅读过论文的阅读论文后再看）  1. Cost volume的构建1.1 在stereo matching中，构建cost volume的步骤：1）确定一系列离散的假设视差平面2）将提取的每个视图的 2D 特征warp到假设平面3）最后融合在一起构建 3D 或 4D cost volume 1.2 双目深度中的warp定义：由于左右图像都经过校正，坐标映射由x轴方向的偏移量决定：其中 Cr(d) 是指视差 d 处右视图的变换 x 轴坐标，而Xl是左视图的源 x 轴坐标。简单地就是说将右图沿x轴移动不同的视差d。  2. GWC网络结构看不懂的先看论文  3. Cost volumegroup-wise volume提供了特征向量匹配的相似度，而concat volume提供了语义信息作为补充，两者是互补的 下面结合代码理解concat volume和group-wise volume的具体构建 3.1 Feature volumeF...</div></div></div></a><a class="pagination-related" href="/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/" title="【论文阅读】【LLaVA】Visual Instruction Tuning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-25</div><div class="info-item-2">【论文阅读】【LLaVA】Visual Instruction Tuning</div></div><div class="info-2"><div class="info-item-1">Visual Instruction Tuning 总结 针对问题指令微调再多模态领域的探索较少 解决方法利用GPT-4来生成多模态语言图像instruction-following数据，然后进行微调。微调分为两步，第一步只微调embedding，第二步微调embedding和LLM。 注意问题和细节（1）和InstructBLIP的区别：InstructBLIP只微调Q-former，LLaVA在第二阶段微调了embedding和LLM；InstructBLIP会将instruction输入到Q-former来辅助视觉特征的抽取，LLaVA没有；InstructBLIP的指令都是人工设计的，LLaVA基于GPT-4，可能会更丰富。（2）LLaVA用的视觉模型和LLM：视觉模，型用的ViT-L，LLM用的是Vicuna（有7B和13B两个版本，13B的版本大概需要三十多G的显存；后续的LLaVA-NeXT支持了其他的LLM，比如LLama-3(8B)、Qwen-1.5(72B&#x2F;110B)）  Abstract使用机器生成的指令跟随数据对大型语言模型(LLM)进行指令调优...</div></div></div></a><a class="pagination-related" href="/2024/07/24/21_%E3%80%90InstructBLIP%E3%80%91Towards%20General-purpose%20Vision-Language%20Models%20with%20Instruction%20Tuning/" title="【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-24</div><div class="info-item-2">【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning</div></div><div class="info-2"><div class="info-item-1">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning 总结 针对问题视觉语言预训练已经得到了广泛的研究，但视觉语言指令调优仍未得到充分的探索 解决方法（1）收集了26个公开可用的数据集，涵盖了各种各样的任务和功能，并将它们转换为指令调优格式（2）还引入了一个指令感知的Query Transformer，它可以提取针对给定指令的信息特征 注意问题和细节（1）Query Transformer和BLIP-2中的区别：InstructBLIP中的QFormer增加了instruction，此外LLM的输入增加了instruction（2）亮点：instruction不仅会指导LLM生成文本，同时也会指导image encoder提取不同的视觉特征。这样的好处在于对于同一张图片，根据不同的instruction，可以得到基于instruction偏好更强的视觉特征，同时对于两个不一样的图片，基于instruction内嵌的通用知识，可以使得模型有更好的知识迁移效果。  Ab...</div></div></div></a><a class="pagination-related" href="/2024/09/20/23_%E3%80%90DETR%E3%80%91End-to-End%20Object%20Detection%20with%20Transformers/" title="【论文阅读】【DETR】End-to-End Object Detection with Transformers"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-20</div><div class="info-item-2">【论文阅读】【DETR】End-to-End Object Detection with Transformers</div></div><div class="info-2"><div class="info-item-1">End-to-End Object Detection with Transformers2024 总结 针对问题先前的目标检测方法流程复杂，需要anchor和nms操作，不是端到端的 解决方法将检测视为集合预测问题，简化了目标检测的整体流程，不需要anchor和nms操作，做到了真正的end-to-end。编码器部分很常规，关键部分在解码器，设置一系列可学习query，从编码的特征中进行查询，然后通过FFN回归得到box，推理的时候只需要根据置信度对框进行筛选。 注意的问题和细节（1）损失函数计算：通过匈牙利算法与GT进行匹配，没有类别的框则和背景匹配（2）可学习的query的数量：DETR中设置的是100  </div></div></div></a><a class="pagination-related" href="/2024/09/22/24_%E3%80%90BLIP-2%E3%80%91Bootstrapping%20Language-Image%20Pre-training/" title="【论文阅读】【BLIP-2】Bootstrapping Language-Image Pre-training"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">【论文阅读】【BLIP-2】Bootstrapping Language-Image Pre-training</div></div><div class="info-2"><div class="info-item-1">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models 总结 针对问题由于大规模模型的端到端训练，视觉和语言预训练的成本变得越来越高 解决方法提出了一种通用且高效的预训练策略BLIP-2，通过类似于Adapter的Qformer将图像特征对齐到文本空间，并提出了两阶段的训练策略，第一阶段从固定图像编码器中引导视觉语言表示学习，第二阶段从一个固定的语言模型中引导视觉到语言的生成学习。 注意问题和细节（1）query有多个，选哪个token算ITC loss和ITM Loss：由于Z包含多个输出嵌入(每个query嵌入一个)，首先计算每个query输出与t之间的成对相似度，然后选择最高的一个作为图像-文本相似度；将每个输出查询嵌入到一个两类线性分类器中以获得logit，并将所有查询的logit平均值作为输出匹配分数（2）和BLIP的区别：BLIP的目标是和原始CLIP相同的，让图像和文本特征对齐，BLIP-2则是让图像特征往文本方向对齐...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/mememe.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">MiaoJieF</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MiaoJieF"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction-and-Motivating-Work"><span class="toc-number">2.</span> <span class="toc-text">1. Introduction and Motivating Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Approach"><span class="toc-number">3.</span> <span class="toc-text">2. Approach</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Natural-Language-Supervision"><span class="toc-number">4.</span> <span class="toc-text">2.1 Natural Language Supervision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Creating-a-Sufficiently-Large-Dataset"><span class="toc-number">4.1.</span> <span class="toc-text">2.2 Creating a Sufficiently Large Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Selecting-an-Efficient-Pre-Training-Method"><span class="toc-number">4.2.</span> <span class="toc-text">2.3 Selecting an Efficient Pre-Training Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Choosing-and-Scaling-a-Model"><span class="toc-number">4.3.</span> <span class="toc-text">2.4 Choosing and Scaling a Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Training"><span class="toc-number">4.4.</span> <span class="toc-text">2.5 Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Experiments"><span class="toc-number">5.</span> <span class="toc-text">3. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Zero-Shot-Transfer"><span class="toc-number">5.1.</span> <span class="toc-text">3.1 Zero-Shot Transfer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-MOTIVATION"><span class="toc-number">5.1.1.</span> <span class="toc-text">3.1.1 MOTIVATION</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-USING-CLIP-FOR-ZERO-SHOT-TRANSFER"><span class="toc-number">5.1.2.</span> <span class="toc-text">3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-INITIAL-COMPARISON-TO-VISUAL-N-GRAMS"><span class="toc-number">5.1.3.</span> <span class="toc-text">3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-4-PROMPT-ENGINEERING-AND-ENSEMBLING"><span class="toc-number">5.1.4.</span> <span class="toc-text">3.1.4 PROMPT ENGINEERING AND ENSEMBLING</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-5-ANALYSIS-OF-ZERO-SHOT-CLIP-PERFORMANCE"><span class="toc-number">5.1.5.</span> <span class="toc-text">3.1.5 ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Representation-Learning"><span class="toc-number">5.2.</span> <span class="toc-text">3.2 Representation Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Comparison-to-Human-Performance"><span class="toc-number">6.</span> <span class="toc-text">4. Comparison to Human Performance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Limitations"><span class="toc-number">7.</span> <span class="toc-text">6. Limitations</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/23/26_LLM_mcp_pro/" title="【LLM学习】【Ollama】五、MCP进阶">【LLM学习】【Ollama】五、MCP进阶</a><time datetime="2025-09-22T16:00:00.000Z" title="发表于 2025-09-23 00:00:00">2025-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/04_LLM_mcp/" title="【LLM学习】【Ollama】四、MCP">【LLM学习】【Ollama】四、MCP</a><time datetime="2025-09-22T15:59:34.000Z" title="发表于 2025-09-22 23:59:34">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/03_LLM_rag/" title="【LLM学习】【Ollama】三、RAG">【LLM学习】【Ollama】三、RAG</a><time datetime="2025-09-22T15:58:40.000Z" title="发表于 2025-09-22 23:58:40">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/02_LLM_finetune/" title="【LLM学习】【Ollama】二、微调">【LLM学习】【Ollama】二、微调</a><time datetime="2025-09-22T15:58:34.000Z" title="发表于 2025-09-22 23:58:34">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/17/01_LLM_connect/" title="【LLM学习】【Ollama】一、本地运行与API调用">【LLM学习】【Ollama】一、本地运行与API调用</a><time datetime="2025-09-17T07:40:47.000Z" title="发表于 2025-09-17 15:40:47">2025-09-17</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2019 - 2025 By MiaoJieF</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>