<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】【LLaVA】Visual Instruction Tuning | 我的博客</title><meta name="author" content="MiaoJieF"><meta name="copyright" content="MiaoJieF"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Visual Instruction Tuning 总结 针对问题指令微调再多模态领域的探索较少 解决方法利用GPT-4来生成多模态语言图像instruction-following数据，然后进行微调。微调分为两步，第一步只微调embedding，第二步微调embedding和LLM。 注意问题和细节（1）和InstructBLIP的区别：InstructBLIP只微调Q-former，LLaVA">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】【LLaVA】Visual Instruction Tuning">
<meta property="og:url" content="http://example.com/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/index.html">
<meta property="og:site_name" content="我的博客">
<meta property="og:description" content="Visual Instruction Tuning 总结 针对问题指令微调再多模态领域的探索较少 解决方法利用GPT-4来生成多模态语言图像instruction-following数据，然后进行微调。微调分为两步，第一步只微调embedding，第二步微调embedding和LLM。 注意问题和细节（1）和InstructBLIP的区别：InstructBLIP只微调Q-former，LLaVA">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/me.jpg">
<meta property="article:published_time" content="2024-07-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-23T02:55:29.514Z">
<meta property="article:author" content="MiaoJieF">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/me.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【论文阅读】【LLaVA】Visual Instruction Tuning",
  "url": "http://example.com/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/",
  "image": "http://example.com/img/me.jpg",
  "datePublished": "2024-07-24T16:00:00.000Z",
  "dateModified": "2025-09-23T02:55:29.514Z",
  "author": [
    {
      "@type": "Person",
      "name": "MiaoJieF",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】【LLaVA】Visual Instruction Tuning',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/me.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">我的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">【论文阅读】【LLaVA】Visual Instruction Tuning</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】【LLaVA】Visual Instruction Tuning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-24T16:00:00.000Z" title="发表于 2024-07-25 00:00:00">2024-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-23T02:55:29.514Z" title="更新于 2025-09-23 10:55:29">2025-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>Visual Instruction Tuning</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>针对问题<br>指令微调再多模态领域的探索较少</li>
<li>解决方法<br>利用GPT-4来生成多模态语言图像instruction-following数据，然后进行微调。微调分为两步，第一步只微调embedding，第二步微调embedding和LLM。</li>
<li>注意问题和细节<br>（1）和InstructBLIP的区别：InstructBLIP只微调Q-former，LLaVA在第二阶段微调了embedding和LLM；InstructBLIP会将instruction输入到Q-former来辅助视觉特征的抽取，LLaVA没有；InstructBLIP的指令都是人工设计的，LLaVA基于GPT-4，可能会更丰富。<br>（2）LLaVA用的视觉模型和LLM：视觉模，型用的ViT-L，LLM用的是Vicuna（有7B和13B两个版本，13B的版本大概需要三十多G的显存；后续的LLaVA-NeXT支持了其他的LLM，比如LLama-3(8B)、Qwen-1.5(72B&#x2F;110B)）</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>使用机器生成的指令跟随数据对大型语言模型(LLM)进行指令调优已被证明可以提高新任务的零射击能力，但该想法在多模态领域的探索较少。我们首次尝试使用仅语言的GPT-4来生成多模态语言图像instruction-following数据。通过对这些生成的数据进行指令调优，我们引入了LLaVA:大型语言和视觉助理，这是一个端到端的训练大型多模态模型，它将视觉编码器和LLM连接起来，用于通用的视觉和语言理解。为了促进视觉指令跟随的进一步研究，我们构建了两个具有多样化和挑战性的应用导向任务的评估基准。我们的实验表明，LLaVA展示了令人印象深刻的多模态聊天能力，有时在看不见的图像&#x2F;指令上表现出多模态GPT-4的行为，与GPT-4在合成的多模态指令遵循数据集上相比，产生了85.1%的相对分数。当在Science QA上进行微调时，LLaVA和GPT-4的协同作用达到了92.53%的最新水平。我们让GPT-4生成的可视化指令调优数据、我们的模型和代码公开可用。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>人类通过视觉和语言等多种渠道与世界互动，因为每一种渠道在表达和交流某些概念方面都具有独特的优势，从而有助于更好地理解世界。人工智能的核心目标之一是开发一种通用的助手，它可以有效地遵循多模态视觉和语言指令，与人类的意图保持一致，在野外完成各种现实世界的任务[4,27,26]。</p>
<p>为此，社区对开发语言增强基础视觉模型产生了浓厚的兴趣[27,16]，该模型具有强大的开放世界视觉理解能力，如分类[40,21,57,54,39]、检测[29,62,33]、分割[25,63,58]和字幕[50,28]，以及视觉生成和编辑[42,43,56,15,44,30]。我们建议读者参考《Computer Vision in the Wild》的阅读清单，以获得更多最新的文献汇编[12]。在这条工作线中，每个任务由一个单一的大视觉模型独立解决，在模型设计中隐式地考虑任务指令。此外，语言仅用于描述图像内容。虽然这允许语言在将视觉信号映射到语言语义(人类交流的常见渠道)方面发挥重要作用，但它导致模型通常具有固定接口，交互性和对用户指令的适应性有限。</p>
<p>另一方面，大型语言模型(LLM)已经表明，语言可以发挥更广泛的作用:作为通用助手的通用接口，各种任务指令可以显式地用语言表示，并引导端到端训练的神经助手切换到感兴趣的任务来解决它。例如，最近ChatGPT[35]和GPT-4[36]的成功证明了对齐LLM在遵循人类指令方面的强大功能，并激发了开发开源LLM的巨大兴趣。其中，LLaMA[49]是一个与GPT-3性能相匹配的开源LLM。Alpaca[48]、Vicuna[9]、GPT-4-LLM[38]利用各种机器生成的高质量指令遵循样本来提高LLM的对齐能力，与专有LLM相比，报告了令人印象深刻的性能。重要的是，这一行工作是纯文本的。</p>
<p>在本文中，我们提出了视觉指令调谐，这是将指令调谐扩展到语言-图像多模态空间的第一次尝试，为构建通用的视觉助手铺平了道路。特别地，我们的论文做出了以下贡献:</p>
<ul>
<li>多模态指令跟随数据。一个关键的挑战是缺乏视觉语言指令跟随数据。我们使用ChatGPT&#x2F;GPT-4提出了一个数据转换视角和管道，将图像-文本对转换为适当的指令遵循格式。</li>
<li>大型多模态模型。我们通过将CLIP的开放集视觉编码器[40]与语言解码器Vicuna[9]连接起来，开发了一个大型多模态模型(LMM)，并对我们生成的教学视觉语言数据进行端到端的微调。我们的实证研究验证了使用生成数据进行LMM指令调优的有效性，并提出了构建通用指令遵循视觉代理的实用技巧。当与GPT-4集成时，我们的方法在Science QA[34]多模态推理数据集上实现了SoTA。</li>
<li>多模态指令遵循基准。我们现在的LLaVA-Bench有两个具有挑战性的基准，与配对的图像，说明和详细的注释的不同选择。</li>
<li>开源的。我们向公众发布了以下资产:生成的多模态指令数据、代码库、模型检查点和可视聊天演示。</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>多模态指令跟随代理。在计算机视觉中，现有的构建指令跟随智能体的工作可以大致分为两类:(i)端到端训练模型，它们分别针对每个特定的研究主题进行探索。例如，视觉语言导航任务[3,19]和Habitat[47]要求嵌入的AI智能体遵循自然语言指令，并采取一系列动作来完成视觉环境中的目标。在图像编辑领域，给定输入图像和告诉代理该做什么的书面指令，InstructPix2Pix[6]通过遵循人类指令来编辑图像。(ii)通过LangChain [1] &#x2F; LLMs[35]协调各种模型的系统，如Visual ChatGPT[53]、X-GPT[63]、MM-REACT[55]、VisProg[18]和ViperGPT[46]。虽然在构建指令遵循代理方面有相同的目标，但我们专注于为多个任务开发端到端的训练语言视觉多模态模型。</p>
<p>指令调优。在自然语言处理(NLP)领域，为了使GPT-3[7]、T5[41]、PaLM[10]和OPT[60]等LLM能够遵循自然语言指令并完成现实世界的任务，研究人员探索了LLM指令调优的方法[37,52,51]，从而分别产生了指令调优的对应物，如InstructGPT [37]&#x2F;ChatGPT[35]、FLAN-T5[11]、FLAN-PaLM[11]和OPT- iml[22]。结果表明，这种简单的方法可以有效地提高LLM的零弹和少弹泛化能力。因此，将NLP的思想借用到计算机视觉是很自然的。更广泛地说，基于基础模型的师生蒸馏思想已经在图像分类等其他主题中得到了研究[14]。Flamingo[2]可以被看作是多模态域的GPT-3时刻，因为它在零射击任务迁移和情境学习方面表现出色。其他在图像文本对上训练的LMM包括BLIP-2[28]、FROMAGe[24]和KOSMOS-1[20]。PaLM-E[13]是一个用于嵌入AI的LMM。基于最近“最好的”开源LLM LLaMA, OpenFlamingo[5]和LLaMA-Adapter[59]是开源的努力，使LLaMA能够使用图像输入，为构建开源多模态LLM铺平了道路。虽然这些模型表现出很好的任务转移泛化性能，但它们没有明确地与视觉语言指令数据进行调整，并且它们在多模态任务中的性能通常低于仅语言任务。本文旨在填补这一空白，并研究其有效性。最后，需要注意的是，视觉指令调优与视觉提示调优不同[23]:前者旨在提高模型的指令跟随能力，后者旨在提高模型自适应的参数效率。</p>
<h2 id="3-GPT-assisted-Visual-Instruction-Data-Generation"><a href="#3-GPT-assisted-Visual-Instruction-Data-Generation" class="headerlink" title="3. GPT-assisted Visual Instruction Data Generation"></a>3. GPT-assisted Visual Instruction Data Generation</h2><p>从CC[8]到LAION[45]，图像-文本对等公共多模式数据的数量在社区中激增。然而，当涉及到多模态指令遵循数据时，可用的数量是有限的，部分原因是创建此类数据的过程非常耗时，并且在考虑到人类群体搜索时定义不明确。受到最近GPT模型在文本注释任务中的成功[17]的启发，我们提出利用ChatGPT&#x2F;GPT-4进行多模态指令跟随数据收集，基于广泛存在的图像对数据。</p>
<p>对于图像XvXvXv及其相关的标题XcXcXc，创建一组问题XqXqXq是很自然的，目的是指示助手描述图像内容。我们提示GPT-4整理这样一个问题列表(详见附录)。因此，将图像-文本对扩展到其指令后续版本的一个简单方法是Human:XqXv<STOP>Assistant:Xc<STOP>Human: Xq Xv<STOP> Assistant: Xc<STOP>Human:XqXv<STOP>Assistant:Xc<STOP>。虽然构建成本低廉，但这种简单的扩展版本在指令和响应方面缺乏多样性和深度推理。</STOP></STOP></STOP></STOP></STOP></STOP></p>
<p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_1.png)</p>
<p>为了缓解这个问题，我们利用仅语言的GPT-4或ChatGPT作为强大的教师(两者都只接受文本作为输入)，以创建包含视觉内容的指令跟随数据。具体来说，为了将图像编码为其视觉特征以提示纯文本GPT，我们使用两种类型的符号表示:(i)通常从不同角度描述视觉场景的字幕;(ii)边界框通常对场景中的物体进行定位，每个边界框对物体概念及其空间位置进行编码。表14的顶部块显示了一个示例。</p>
<p>这种符号表示允许我们将图像编码为LLM可识别的序列。我们使用COCO图像[31]，生成三种类型的指令跟随数据。表14的底部块显示了每种类型的一个示例。对于每种类型，我们首先手动设计一些示例。它们是我们在数据收集过程中唯一的人工注释，并被用作在上下文学习中查询GPT-4的种子示例。</p>
<ul>
<li>谈话。我们设计了一个对话，在助理和一个询问这张照片问题的人之间进行。回答的语气就好像助理正在看图像并回答问题一样。关于图像的视觉内容有一系列不同的问题，包括对象类型、对象计数、对象动作、对象位置、对象之间的相对位置。只考虑有明确答案的问题。详细提示请参见附录。</li>
<li>详细的描述。为了包含丰富而全面的图像描述，我们创建了一个带有这种意图的问题列表。我们提示GPT-4，然后整理列表(参见附录中详细的提示和整理过程)。对于每个图像，我们从列表中随机抽取一个问题，让GPT-4生成详细的描述。</li>
<li>复杂的推理。以上两种类型关注的是视觉内容本身，在此基础上我们进一步打造深度推理题。答案通常需要遵循严格的逻辑，一步一步地推理。</li>
</ul>
<p>我们总共收集了158K个独特的语言图像指令遵循样本，其中对话58K，详细描述23K，复杂推理77k。我们在早期实验中减少了ChatGPT和GPT-4的使用，发现GPT-4始终如一地提供更高质量的指令跟随数据，例如空间推理。</p>
<h2 id="4-Visual-Instruction-Tuning"><a href="#4-Visual-Instruction-Tuning" class="headerlink" title="4. Visual Instruction Tuning"></a>4. Visual Instruction Tuning</h2><h3 id="4-1-Architecture"><a href="#4-1-Architecture" class="headerlink" title="4.1 Architecture"></a>4.1 Architecture</h3><p>主要目标是有效地利用预训练的LLM和视觉模型的功能。网络架构如图1所示。我们选择Vicuna[9]作为我们的LLM fϕ(·)参数化的ϕ，因为它在公开可用的检查点中具有最佳的语言任务指令跟踪能力[48,9,38]。<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_5.png)</p>
<p>对于输入图像XvXvXv，我们考虑预训练的CLIP视觉编码器ViT-L&#x2F;14[40]，它提供视觉特征Zv&#x3D;g(Xv)Zv &#x3D; g(Xv)Zv&#x3D;g(Xv)。我们的实验考虑了最后一层Transformer前后的网格特征。我们考虑一个简单的线性层来连接图像特征到词嵌入空间。具体来说，我们使用一个可训练的投影矩阵W将ZvZvZv转换为语言嵌入令牌HvHvHv，它与语言模型中的词嵌入空间具有相同的维数:</p>
<p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_3.png)</p>
<p>这样，我们就有了一系列视觉标记HvHvHv。请注意，我们的简单投影方案是轻量级的，它允许我们快速迭代以数据为中心的实验。还可以考虑更复杂的方案来连接图像和语言表征，例如Flamingo中的门控交叉注意[2]和BLIP-2中的Q-former[28]。我们将为LLaVA探索更有效、更复杂的架构设计作为未来的工作。</p>
<h3 id="4-2-Training"><a href="#4-2-Training" class="headerlink" title="4.2 Training"></a>4.2 Training</h3><p>对于每张图像XvXvXv，我们生成多回合对话数据(Xq1,Xa1,…,XqT,XaT)(X_{q}^{1},X_{a}^{1},…,X_{q}^{T},X_{a}^{T})(Xq1​,Xa1​,…,XqT​,XaT​)，其中TTT为总回合数。我们将它们组织成一个序列，将所有的回答视为助手的响应，并将指令XtXtXt指令在第t个转弯处为:<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_4.png)<br>这导致了表2中所示的多模态指令跟随序列的统一格式。我们使用其原始的自回归训练目标对预测令牌执行LLM的指令调优。</p>
<p>具体来说，对于长度为L的序列，我们计算目标答案XaXaXa的概率为:<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_7.png)<br>其中，θ为可训练参数，Xinstruct,&lt;iX_{instruct,&lt;i}Xinstruct,&lt;i​和Xa,&lt;iX_{a,&lt;i}Xa,&lt;i​分别为当前预测令牌xix_{i}xi​之前所有回合的指令令牌和回答令牌。有关预测令牌的说明，请参见表2。对于(3)中的条件，我们显式地添加了XvXvXv，以强调图像是基于所有答案的事实，并且为了更好的可读性，我们省略了Xsystem−messageX_{system-message}Xsystem−message​和所有前面的。对于LLaVA模型训练，我们考虑一个两阶段的指令调优过程。<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_2.png)</p>
<p><strong>阶段1:</strong> 特征对齐的预训练。为了在概念覆盖率和训练效率之间取得平衡，我们将CC3M过滤到595K图像-文本对。有关过滤过程的详情，请参阅附录。使用第3节中描述的朴素展开方法将这些数据对转换为跟随指令的数据。每个样本都可以视为单回合对话。为了构造(2)中的输入x指令，对于图像XvXvXv，随机采样一个问题XqXqXq，这是一个语言指令，要求助手对图像进行简要描述。最基本的预测答案XaXaXa是原始的标题。在训练中，我们保持视觉编码器和LLM权值不变，并最大化(3)的似然值，只有可训练参数θ &#x3D;W(投影矩阵)。这样，图像特征HvHvHv可以与预训练的LLM词嵌入对齐。这个阶段可以理解为为冻结的LLM训练一个兼容的视觉标记器。</p>
<p><strong>阶段2:</strong> 端到端微调。我们始终保持视觉编码器权值不变，并不断更新投影层和LLM的预训练权值;即，可训练的参数是(3)中的θ&#x3D;{W,ϕ}。我们考虑两个特定的用例场景:</p>
<ul>
<li>多通道聊天机器人。我们通过对第3节中的158K语言图像指令跟踪数据进行微调来开发聊天机器人。在这三种类型的响应中，会话是多回合的，而其他两种是单回合的。它们在训练中被统一采样。</li>
<li>科学QA。我们在ScienceQA基准上研究了我们的方法[34]，这是第一个大规模的多模态科学问题数据集，它用详细的讲座和解释注释了答案。每个问题都以自然语言或图像的形式提供上下文。该助手以自然语言提供推理过程，并从多个选项中选择答案。对于(2)中的训练，我们将数据组织为单回合对话，问题和上下文作为XinstructX_{instruct}Xinstruct​，推理和答案作为XaXaXa。</li>
</ul>
<h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><p>我们通过两个主要的实验设置分别评估了LLaVA在指令跟随和视觉推理能力方面的性能:多模态聊天机器人和ScienceQA数据集。我们使用8张A100训练所有模型，遵循Vicuna的超参数[9]。我们在过滤后的CC-595K子集上对模型进行了1 epoch的预训练，学习率为2e-3，批大小为128，并对提出的LLaVA-Instruct-158K数据集进行了3 epoch的微调，学习率为2e-5，批大小为32。更多培训细节见附录。</p>
<h3 id="5-1-Multimodal-Chatbot"><a href="#5-1-Multimodal-Chatbot" class="headerlink" title="5.1 Multimodal Chatbot"></a>5.1 Multimodal Chatbot</h3><p>我们开发了一个聊天机器人演示，以展示LLaVA的图像理解和对话能力，并研究LLaVA在消化视觉输入和展示指令遵循能力方面的能力。我们首先使用原始GPT-4论文[36]中的示例，如表3所示(更多示例见附录)，这些示例需要深入的图像理解。为了比较，我们从他们的论文中引用了多模态GPT-4的提示和响应，并查询了BLIP-2和OpenFlamingo模型检查点来获得它们的响应。</p>
<p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_6.png)</p>
<p>令人惊讶的是，尽管LLaVA是用一个小的多模态指令跟随数据集(~ 80K唯一图像)训练的，但它在这些示例上展示了与多模态GPT-4非常相似的推理结果。请注意，虽然这些图像是LLaVA的域外图像，但LLaVA仍然能够理解场景并按照问题说明提供合理的响应。相比之下，BLIP-2和OpenFlamingo侧重于描述图像，而不是按照用户的指示以适当的方式回答。</p>
<p>定量评价。为了系统地了解LLaVA的性能，我们提出了一个定量度量来衡量模型在多模态数据上的指令跟随能力。受[9]的启发，我们利用GPT-4来衡量生成的响应的质量。具体来说，我们创建了由图像、基本事实文本描述和问题组成的三元组。候选模型(例如，LLaVA)根据问题和图像预测答案。为了提供一个近似的理论上限，我们使用纯文本GPT-4，基于问题和基本事实文本描述创建了一个参考预测。在获得两个模型的响应后，我们将问题、视觉信息(以文本描述的格式)和两个助手生成的响应馈送给裁判(即纯文本GPT-4)。它评估助手回答的有用性、相关性、准确性和详细程度，并给出1到10分的总分，分数越高表明整体表现越好。并要求对评估提供一个全面的解释，以便我们更好地理解模型。我们报告了相对分数，而不是纯文本GPT-4模型，该模型使用纹理基础真值描述作为视觉输入。我们创建了两个基准来评估模型的性能。</p>
<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>本文论证了视觉指令调优的有效性。我们提出了一个自动管道来创建语言图像指令跟随数据，并在此基础上训练多模态模型LLaVA来跟随人类意图完成视觉任务。当在ScienceQA上进行微调时，它达到了新的SoTA精度，当在多模态聊天数据上进行微调时，它具有出色的视觉聊天功能。此外，我们提出了第一个研究多模态指令跟随能力的基准。本文是视觉教学调优的第一步，主要关注现实生活中的任务。关于LLaVA在学术基准上的更多定量结果，请参考使用视觉指令调优的改进基线[32]。我们希望我们的工作能够启发未来建立更有能力的多模态模型的研究。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">MiaoJieF</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/">http://example.com/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">我的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post-share"><div class="social-share" data-image="/img/me.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/09/20/23_%E3%80%90DETR%E3%80%91End-to-End%20Object%20Detection%20with%20Transformers/" title="【论文阅读】【DETR】End-to-End Object Detection with Transformers"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">【论文阅读】【DETR】End-to-End Object Detection with Transformers</div></div><div class="info-2"><div class="info-item-1">End-to-End Object Detection with Transformers2024 总结 针对问题先前的目标检测方法流程复杂，需要anchor和nms操作，不是端到端的![在这里插入图片描述](.&#x2F;2025-09-20-【DETR】End-to-End Object Detection with Transformers&#x2F;img_1.png) 解决方法将检测视为集合预测问题，简化了目标检测的整体流程，不需要anchor和nms操作，做到了真正的end-to-end。编码器部分很常规，关键部分在解码器，设置一系列可学习query，从编码的特征中进行查询，然后通过FFN回归得到box，推理的时候只需要根据置信度对框进行筛选。![在这里插入图片描述](.&#x2F;2025-09-20-【DETR】End-to-End Object Detection with Transformers&#x2F;img_2.png) 注意的问题和细节（1）损失函数计算：通过匈牙利算法与GT进行匹配，没有类别的框则和背景匹配（2）可学习的query的数量：DETR中...</div></div></div></a><a class="pagination-related" href="/2024/07/24/21_%E3%80%90InstructBLIP%E3%80%91Towards%20General-purpose%20Vision-Language%20Models%20with%20Instruction%20Tuning/" title="【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning</div></div><div class="info-2"><div class="info-item-1">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning 总结 针对问题视觉语言预训练已经得到了广泛的研究，但视觉语言指令调优仍未得到充分的探索 解决方法（1）收集了26个公开可用的数据集，涵盖了各种各样的任务和功能，并将它们转换为指令调优格式（2）还引入了一个指令感知的Query Transformer，它可以提取针对给定指令的信息特征 注意问题和细节（1）Query Transformer和BLIP-2中的区别：InstructBLIP中的QFormer增加了instruction，此外LLM的输入增加了instruction（2）亮点：instruction不仅会指导LLM生成文本，同时也会指导image encoder提取不同的视觉特征。这样的好处在于对于同一张图片，根据不同的instruction，可以得到基于instruction偏好更强的视觉特征，同时对于两个不一样的图片，基于instruction内嵌的通用知识，可以使得模型有更好的知识迁移效果。  Ab...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" title="【论文阅读】GWC-Net代码阅读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-30</div><div class="info-item-2">【论文阅读】GWC-Net代码阅读</div></div><div class="info-2"><div class="info-item-1">代码中关于网络定义的部分其实还是挺好懂的，所以本文的重点是cancatenation volume和group-wise volume这部分（建议没有阅读过论文的阅读论文后再看）  1. Cost volume的构建1.1 在stereo matching中，构建cost volume的步骤：1）确定一系列离散的假设视差平面2）将提取的每个视图的 2D 特征warp到假设平面3）最后融合在一起构建 3D 或 4D cost volume 1.2 双目深度中的warp定义：由于左右图像都经过校正，坐标映射由x轴方向的偏移量决定：其中 Cr(d) 是指视差 d 处右视图的变换 x 轴坐标，而Xl是左视图的源 x 轴坐标。简单地就是说将右图沿x轴移动不同的视差d。  2. GWC网络结构看不懂的先看论文  3. Cost volumegroup-wise volume提供了特征向量匹配的相似度，而concat volume提供了语义信息作为补充，两者是互补的 下面结合代码理解concat volume和group-wise volume的具体构建 3.1 Feature volumeF...</div></div></div></a><a class="pagination-related" href="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/" title="【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-16</div><div class="info-item-2">【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision</div></div><div class="info-2"><div class="info-item-1">CLIP 总结：与一些自监督的预训练的方法(如MoCo、MAE)相比，CLIP最大的贡献是实现了文本和图像的对齐。  损失函数和info nce的区别：作用类似，CLIP计算交叉熵，info nce就是计算相似度后计算交叉熵  12345678910111213141516171819202122232425262728import torchimport torch.nn.functional as Ffrom torch import nnimport numpy as npdef get_logits(image_features, text_features, logit_scale):    # 计算image_features @ text_features.T相似度矩阵    logits_per_image = logit_scale * image_features @ text_features.T    logits_per_text = logit_scale * text_features @ image_features.T    return log...</div></div></div></a><a class="pagination-related" href="/2024/07/21/20_%E3%80%90BLIP%E3%80%91Bootstrapping%20Language-Image%20Pre-training/" title="【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-21</div><div class="info-item-2">【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training</div></div><div class="info-2"><div class="info-item-1">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 总结 针对问题（1）基于编码器的模型不太容易直接转移到文本生成任务(例如图像caption)，而编码器-解码器模型尚未成功地用于图像-文本检索任务（2）现有方法使用的网络数据集存在噪声，带来次优结果 解决方法（1）提出编码器-解码器的多模态混合架构，结合检索和生成caption的功能（2）一种新的数据集bootstrap方法：先预训练，然后用精标注数据集微调，通过captioner生成数据，通过filter过滤数据 注意的问题和细节（1）ITM Loss的作用：细粒度上的对齐（2）为什么使用LM Loss而不使用MLM Loss：使用LM loss才能让模型具有生成能力（3）在常规训练时，文本编码器和文本解码器共享除SA层之外的所有参数：原因是编码和解码任务之间的差异最好由SA层捕获，特别是，编码器使用双向自注意来构建当前输入标记的表示，而解码器使用因果自注意来预测下一个标...</div></div></div></a><a class="pagination-related" href="/2024/07/24/21_%E3%80%90InstructBLIP%E3%80%91Towards%20General-purpose%20Vision-Language%20Models%20with%20Instruction%20Tuning/" title="【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-24</div><div class="info-item-2">【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning</div></div><div class="info-2"><div class="info-item-1">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning 总结 针对问题视觉语言预训练已经得到了广泛的研究，但视觉语言指令调优仍未得到充分的探索 解决方法（1）收集了26个公开可用的数据集，涵盖了各种各样的任务和功能，并将它们转换为指令调优格式（2）还引入了一个指令感知的Query Transformer，它可以提取针对给定指令的信息特征 注意问题和细节（1）Query Transformer和BLIP-2中的区别：InstructBLIP中的QFormer增加了instruction，此外LLM的输入增加了instruction（2）亮点：instruction不仅会指导LLM生成文本，同时也会指导image encoder提取不同的视觉特征。这样的好处在于对于同一张图片，根据不同的instruction，可以得到基于instruction偏好更强的视觉特征，同时对于两个不一样的图片，基于instruction内嵌的通用知识，可以使得模型有更好的知识迁移效果。  Ab...</div></div></div></a><a class="pagination-related" href="/2024/09/22/24_%E3%80%90BLIP-2%E3%80%91Bootstrapping%20Language-Image%20Pre-training/" title="【论文阅读】【BLIP-2】Bootstrapping Language-Image Pre-training"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">【论文阅读】【BLIP-2】Bootstrapping Language-Image Pre-training</div></div><div class="info-2"><div class="info-item-1">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models 总结 针对问题由于大规模模型的端到端训练，视觉和语言预训练的成本变得越来越高 解决方法提出了一种通用且高效的预训练策略BLIP-2，通过类似于Adapter的Qformer将图像特征对齐到文本空间，并提出了两阶段的训练策略，第一阶段从固定图像编码器中引导视觉语言表示学习，第二阶段从一个固定的语言模型中引导视觉到语言的生成学习。 注意问题和细节（1）query有多个，选哪个token算ITC loss和ITM Loss：由于Z包含多个输出嵌入(每个query嵌入一个)，首先计算每个query输出与t之间的成对相似度，然后选择最高的一个作为图像-文本相似度；将每个输出查询嵌入到一个两类线性分类器中以获得logit，并将所有查询的logit平均值作为输出匹配分数（2）和BLIP的区别：BLIP的目标是和原始CLIP相同的，让图像和文本特征对齐，BLIP-2则是让图像特征往文本方向对齐...</div></div></div></a><a class="pagination-related" href="/2024/09/22/25_%E3%80%90SAM%E3%80%91Segment%20Anything/" title="【论文阅读】【SAM】Segment Anything"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">【论文阅读】【SAM】Segment Anything</div></div><div class="info-2"><div class="info-item-1">Segment Anything2023 总结 针对问题分割任务上缺乏可以做zero shot的基础模型 解决方法（1）提出了一个新任务：promot segmentation任务，通过promot的方式使得网络可以实现zero shot泛化（2）模型架构：图像编码器用的是ViT，文本编码器用的是CLIP的文本编码器，点和框的嵌入方法是转换成位置编码，掩码的嵌入方式是卷积（3）数据集：人工标注+模型生成 注意问题和细节（1）点和框的输入：首先会进行坐标上的缩放，然后将坐标转成位置编码，再加上可学习参数得到坐标的embedding（2）mask的处理：通过卷积得到embedding![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_2.png)  （3）解码：mask的embedding直接加到图像上，坐标的embedding和iou token、mask token作为query![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_3.pn...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/me.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">MiaoJieF</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MiaoJieF"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">2.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">3.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-Work"><span class="toc-number">4.</span> <span class="toc-text">2. Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-GPT-assisted-Visual-Instruction-Data-Generation"><span class="toc-number">5.</span> <span class="toc-text">3. GPT-assisted Visual Instruction Data Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Visual-Instruction-Tuning"><span class="toc-number">6.</span> <span class="toc-text">4. Visual Instruction Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Architecture"><span class="toc-number">6.1.</span> <span class="toc-text">4.1 Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Training"><span class="toc-number">6.2.</span> <span class="toc-text">4.2 Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiments"><span class="toc-number">7.</span> <span class="toc-text">5. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Multimodal-Chatbot"><span class="toc-number">7.1.</span> <span class="toc-text">5.1 Multimodal Chatbot</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Conclusion"><span class="toc-number">8.</span> <span class="toc-text">6. Conclusion</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/04_LLM_mcp/" title="【LLM学习】【Ollama】四、MCP">【LLM学习】【Ollama】四、MCP</a><time datetime="2025-09-22T15:59:34.000Z" title="发表于 2025-09-22 23:59:34">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/03_LLM_rag/" title="【LLM学习】【Ollama】三、RAG">【LLM学习】【Ollama】三、RAG</a><time datetime="2025-09-22T15:58:40.000Z" title="发表于 2025-09-22 23:58:40">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/22/02_LLM_finetune/" title="【LLM学习】【Ollama】二、微调">【LLM学习】【Ollama】二、微调</a><time datetime="2025-09-22T15:58:34.000Z" title="发表于 2025-09-22 23:58:34">2025-09-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/17/01_LLM_connect/" title="【LLM学习】【Ollama】一、本地运行与API调用">【LLM学习】【Ollama】一、本地运行与API调用</a><time datetime="2025-09-17T07:40:47.000Z" title="发表于 2025-09-17 15:40:47">2025-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/19/10_MySQL_base/" title="【MySQL】MySQL基础教程">【MySQL】MySQL基础教程</a><time datetime="2025-07-18T16:00:00.000Z" title="发表于 2025-07-19 00:00:00">2025-07-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2019 - 2025 By MiaoJieF</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>