<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【LLM学习】【Ollama】四、MCP</title>
      <link href="/2025/09/22/04_LLM_mcp/"/>
      <url>/2025/09/22/04_LLM_mcp/</url>
      
        <content type="html"><![CDATA[<h1 id="在-Ollama-多轮对话中集成-MCP，并将结果嵌入上下文"><a href="#在-Ollama-多轮对话中集成-MCP，并将结果嵌入上下文" class="headerlink" title="在 Ollama 多轮对话中集成 MCP，并将结果嵌入上下文"></a>在 Ollama 多轮对话中集成 MCP，并将结果嵌入上下文</h1><p>本文基于一个最小可用的 CLI 项目，演示如何在 Ollama 的多轮对话中集成 MCP（消息控制协议风格的指令），并把 MCP 的执行结果注入到对话历史里，让大模型在后续轮次中“记得”这些结果并可直接引用。</p><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_mcp">https://github.com/MiaoJieF/LLM_mcp</a></p></blockquote><h2 id="一、目标"><a href="#一、目标" class="headerlink" title="一、目标"></a>一、目标</h2><ul><li>持久化多轮对话上下文</li><li>识别并处理 MCP 指令：<code>/time</code> 和 <code>/bmi 身高 体重</code></li><li>将 MCP 执行结果写回对话历史，供模型后续引用</li><li>通过简单 CLI 进行交互</li></ul><h2 id="二、模块划分"><a href="#二、模块划分" class="headerlink" title="二、模块划分"></a>二、模块划分</h2><ul><li><code>ollama_client.py</code>：与 Ollama HTTP API 交互</li><li><code>mcp_handler.py</code>：解析与执行 MCP 指令（时间查询、BMI 计算）</li><li><code>conversation_manager.py</code>：维护对话上下文，路由 MCP 与普通对话，并把 MCP 结果写入历史</li><li><code>main.py</code>：命令行入口，设定系统提示、读取用户输入</li></ul><h2 id="三、MCP-的识别与执行"><a href="#三、MCP-的识别与执行" class="headerlink" title="三、MCP 的识别与执行"></a>三、MCP 的识别与执行</h2><p>MCP 的识别策略非常直观：所有以 <code>/</code> 开头的输入都作为候选 MCP 指令；具体支持的指令在 <code>mcp_handler.py</code> 中逐一匹配与处理。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def handle_mcp_command(user_text: str) -&gt; MCPResult:</span><br><span class="line">    text = user_text.strip()</span><br><span class="line">    if not text.startswith(&quot;/&quot;):</span><br><span class="line">        return MCPResult(handled=False)</span><br><span class="line"></span><br><span class="line">    if text.lower() == TIME_CMD:</span><br><span class="line">        now = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)</span><br><span class="line">        return MCPResult(handled=True, response=f&quot;当前时间：&#123;now&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    if text.lower().startswith(BMI_CMD):</span><br><span class="line">        try:</span><br><span class="line">            height_m, weight_kg = parse_bmi_args(text)</span><br><span class="line">            bmi = weight_kg / (height_m * height_m)</span><br><span class="line">            category = bmi_category(bmi)</span><br><span class="line">            return MCPResult(</span><br><span class="line">                handled=True,</span><br><span class="line">                response=(</span><br><span class="line">                    f&quot;BMI: &#123;bmi:.2f&#125;，状态：&#123;category&#125;&quot;</span><br><span class="line">                ),</span><br><span class="line">            )</span><br><span class="line">        except Exception as exc:</span><br><span class="line">            return MCPResult(handled=True, response=f&quot;BMI 计算错误：&#123;exc&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    return MCPResult(handled=False)</span><br></pre></td></tr></table></figure><ul><li><code>/time</code>：直接返回当前系统时间。</li><li><code>/bmi</code>：解析参数（米、千克），计算 BMI 并给出健康状态。对格式错误、数值常识做了防御性校验。</li></ul><h2 id="四、将-MCP-结果嵌入对话历史"><a href="#四、将-MCP-结果嵌入对话历史" class="headerlink" title="四、将 MCP 结果嵌入对话历史"></a>四、将 MCP 结果嵌入对话历史</h2><p>关键在 <code>ConversationManager.process_input</code>。当检测到 MCP 命令被处理后，我们不仅把指令作为用户消息写入历史，还把“结构化的 MCP 结果”作为一次助手回复写回历史，带上统一的可识别前缀“（来自MCP）”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mcp: MCPResult = handle_mcp_command(user_text)</span><br><span class="line">if mcp.handled:</span><br><span class="line">    # 记录指令与结果到历史，便于模型后续引用</span><br><span class="line">    self.add_user_message(user_text)</span><br><span class="line">    mcp_response_text = mcp.response or &quot;&quot;</span><br><span class="line">    history_note = (</span><br><span class="line">        &quot;（来自MCP）\n&quot;</span><br><span class="line">        f&quot;指令：&#123;user_text&#125;\n&quot;</span><br><span class="line">        f&quot;结果：&#123;mcp_response_text&#125;&quot;</span><br><span class="line">    )</span><br><span class="line">    self.add_assistant_message(history_note)</span><br><span class="line">    return mcp_response_text</span><br></pre></td></tr></table></figure><p>为什么要以“助手消息”写回？</p><ul><li>因为多数 Chat 模型会把 <code>assistant</code> 角色视为“可被引用的已有事实”。</li><li>通过统一前缀“（来自MCP）”，我们还能在系统提示中明确告诉模型：这些是可信的工具结果。</li></ul><h2 id="五、系统提示（System-Prompt）的配合"><a href="#五、系统提示（System-Prompt）的配合" class="headerlink" title="五、系统提示（System Prompt）的配合"></a>五、系统提示（System Prompt）的配合</h2><p>系统提示负责为模型注入“如何看待 MCP 结果”的规则。在 <code>main.py</code> 中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">convo.add_system_message(</span><br><span class="line">    &quot;你是一个乐于助人的助手。回答时简洁、准确。\n&quot;</span><br><span class="line">    &quot;如果对话中出现以‘（来自MCP）’开头的内容，视为已执行的工具结果，&quot;</span><br><span class="line">    &quot;可直接引用其中的事实进行回答，不必重复计算或质疑其有效性。&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这样，当用户后续追问“刚才的时间是多少？”、“我属于什么 BMI 状态？”时，模型会优先引用历史里标注为“（来自MCP）”的那条助手消息，得到稳定、一致的回答。</p><h2 id="六、与-Ollama-的集成"><a href="#六、与-Ollama-的集成" class="headerlink" title="六、与 Ollama 的集成"></a>六、与 Ollama 的集成</h2><p>与 Ollama 的交互通过 HTTP Chat API 完成：</p><ul><li>封装在 <code>OllamaClient.chat</code> 中，传入 <code>messages</code>（包含 system、user、assistant 历史）。</li><li>发生异常时统一抛出 <code>RuntimeError</code>，便于 CLI 层捕获与提示。</li></ul><h2 id="七、交互体验与容错"><a href="#七、交互体验与容错" class="headerlink" title="七、交互体验与容错"></a>七、交互体验与容错</h2><ul><li>CLI 会给出指令提示：<code>/time</code>、<code>/bmi 身高 体重</code>、<code>/exit</code>。</li><li>对 <code>/bmi</code> 的输入做了格式与范围校验，错误会以“BMI 计算错误：…”形式返回，同时仍会写入“来自MCP”的历史记录，方便追溯。</li></ul><h2 id="八、常见扩展点"><a href="#八、常见扩展点" class="headerlink" title="八、常见扩展点"></a>八、常见扩展点</h2><ul><li>更多 MCP 指令：如 <code>/weather 城市</code>、<code>/stock 代码</code>，只需在 <code>mcp_handler.py</code> 增加解析与处理，并复用现有的写回历史逻辑。</li><li>Streaming 输出：<code>OllamaClient.chat</code> 可切换 <code>stream=True</code>，并按块回显，同时仍把完整结果写回历史。</li><li>结构化结果：将 MCP 响应用 JSON 序列化写入历史，并在 system prompt 中说明“JSON 字段的语义”，提升模型引用的准确性。</li><li>只记录不回显：如果你不希望在屏幕上重复显示 MCP 结果，可修改 <code>process_input</code> 只写历史、不回显，或提供开关。</li></ul><h2 id="九、小结"><a href="#九、小结" class="headerlink" title="九、小结"></a>九、小结</h2><p>本文展示了一个简单而实用的模式：</p><ul><li>先在输入侧识别 MCP 指令</li><li>执行后把“带前缀的工具结果”作为助手消息写入历史</li><li>在系统提示中给予模型明确的“引用规则”</li></ul><p>通过这三步，模型不仅能在当下给出正确答复，更能在后续轮次中记住并稳定引用同一事实，显著提升混合“对话 + 工具”场景下的可用性与一致性。</p><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_mcp">https://github.com/MiaoJieF/LLM_mcp</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【LLM学习】【Ollama】三、RAG</title>
      <link href="/2025/09/22/03_LLM_rag/"/>
      <url>/2025/09/22/03_LLM_rag/</url>
      
        <content type="html"><![CDATA[<p>在大语言模型（LLM）应用中，检索增强生成（RAG）技术因其能让模型结合外部知识回答问题而备受关注。本文将结合一个简洁的开源项目，带你从零理解 RAG 的核心原理与实现方式，即使没有复杂的工程经验也能快速上手。</p><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_rag">https://github.com/MiaoJieF/LLM_rag</a></p></blockquote><h1 id="一、什么是-RAG？为什么需要它？​"><a href="#一、什么是-RAG？为什么需要它？​" class="headerlink" title="一、什么是 RAG？为什么需要它？​"></a>一、什么是 RAG？为什么需要它？​</h1><p>RAG（Retrieval-Augmented Generation，检索增强生成）是一种将 “检索外部知识” 与 “LLM 生成” 结合的技术。它解决了传统 LLM 的两大痛点：​</p><ul><li>知识时效性问题：LLM 训练数据有截止日期，无法回答最新信息​</li><li>知识准确性问题：避免模型 “一本正经地胡说八道”，答案可追溯到来源文档​</li></ul><p>简单来说，RAG 的工作流程可以概括为四步：​</p><ul><li>分块：将长文档切割成有重叠的小片段（保持语义完整）​</li><li>向量化：用嵌入模型将文本片段转换为向量（数字表示）​</li><li>检索：将用户问题转为向量，与文本向量做相似度匹配，找到最相关的片段​</li><li>生成：将问题和检索到的片段一起传给 LLM，生成基于参考文档的答案</li></ul><h1 id="二、一个极简的-RAG-实现：简易版-localGPT​"><a href="#二、一个极简的-RAG-实现：简易版-localGPT​" class="headerlink" title="二、一个极简的 RAG 实现：简易版 localGPT​"></a>二、一个极简的 RAG 实现：简易版 localGPT​</h1><p>今天要介绍的这个开源项目（项目结构）专为学习 RAG 原理设计，代码简洁易懂，无需复杂依赖即可本地运行。它的核心特点是：​</p><ul><li>纯离线运行：支持本地嵌入模型和本地 LLM（无需联网调用 API）​</li><li>轻量易上手：代码量少，模块划分清晰，适合初学者阅读​</li><li>功能完整：包含文档加载、分块、嵌入、检索、生成全流程</li></ul><h2 id="1-项目核心结构​"><a href="#1-项目核心结构​" class="headerlink" title="1. 项目核心结构​"></a>1. 项目核心结构​</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├─ data/               <span class="comment"># TXT 文档（示例：sample.txt）</span></span><br><span class="line">├─ src/</span><br><span class="line">│  ├─ loader.py        <span class="comment"># 加载 .txt 文件</span></span><br><span class="line">│  ├─ chunker.py       <span class="comment"># 文本分块（定长+重叠）</span></span><br><span class="line">│  ├─ embeddings.py    <span class="comment"># Sentence-Transformers 嵌入（支持本地目录）</span></span><br><span class="line">│  ├─ vector_store.py  <span class="comment"># 内存向量库 + 余弦相似度检索 + 保存/加载</span></span><br><span class="line">│  ├─ retriever.py     <span class="comment"># 检索器（整合分块、嵌入、搜索）</span></span><br><span class="line">│  ├─ llm.py           <span class="comment"># 本地 LLM（llama.cpp 可选）+ Ollama 客户端（支持流式&amp;中文）</span></span><br><span class="line">│  └─ cli.py           <span class="comment"># 命令行入口（index / ask / chat）</span></span><br><span class="line">├─ tests/              <span class="comment"># 基础测试</span></span><br><span class="line">├─ requirements.txt    <span class="comment"># 依赖</span></span><br><span class="line">└─ README.md</span><br></pre></td></tr></table></figure><p>每个模块各司其职，通过简单的调用关系构成完整的 RAG 流程，非常适合逐模块学习。​</p><h2 id="2-核心功能解析​"><a href="#2-核心功能解析​" class="headerlink" title="2. 核心功能解析​"></a>2. 核心功能解析​</h2><h3 id="（1）文本分块：保持语义的关键​"><a href="#（1）文本分块：保持语义的关键​" class="headerlink" title="（1）文本分块：保持语义的关键​"></a>（1）文本分块：保持语义的关键​</h3><p>长文本直接处理会丢失上下文，项目通过chunker.py实现定长 + 重叠分块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心分块逻辑（src/chunker.py）</span></span><br><span class="line">def simple_chunk(text: str, chunk_size: int = 500, chunk_overlap: int = 100) -&gt; List[str]:</span><br><span class="line">    chunks = []</span><br><span class="line">    start_index = 0</span><br><span class="line">    step = chunk_size - chunk_overlap  <span class="comment"># 实际步长 = 块大小 - 重叠部分</span></span><br><span class="line">    <span class="keyword">while</span> start_index &lt; len(text):</span><br><span class="line">        end_index = min(start_index + chunk_size, len(text))</span><br><span class="line">        chunk = text[start_index:end_index].strip()</span><br><span class="line">        <span class="keyword">if</span> chunk:</span><br><span class="line">            chunks.append(chunk)</span><br><span class="line">        start_index += step</span><br><span class="line">    <span class="built_in">return</span> chunks</span><br></pre></td></tr></table></figure><p>分块参数建议：​</p><ul><li>chunk_size：300~800 字符（根据文档密度调整）​</li><li>chunk_overlap：50~200 字符（确保上下文连贯）​</li></ul><h3 id="（2）向量化：文本转向量的魔法​"><a href="#（2）向量化：文本转向量的魔法​" class="headerlink" title="（2）向量化：文本转向量的魔法​"></a>（2）向量化：文本转向量的魔法​</h3><p>项目使用 Sentence-Transformers 的all-MiniLM-L6-v2模型（轻量且效果好），通过embeddings.py实现文本向量化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量生成示例（src/embeddings.py）</span></span><br><span class="line">class EmbeddingModel:</span><br><span class="line">    def __init__(self, model_name: str = <span class="string">&quot;all-MiniLM-L6-v2&quot;</span>):</span><br><span class="line">        self.model = SentenceTransformer(model_name)</span><br><span class="line">    </span><br><span class="line">    def embed_documents(self, texts: List[str]) -&gt; np.ndarray:</span><br><span class="line">        <span class="comment"># 将文本列表转为向量矩阵</span></span><br><span class="line">        <span class="built_in">return</span> self.model.encode(texts, convert_to_numpy=True)</span><br><span class="line">    </span><br><span class="line">    def embed_query(self, text: str) -&gt; np.ndarray:</span><br><span class="line">        <span class="comment"># 将单个问题转为向量</span></span><br><span class="line">        <span class="built_in">return</span> self.model.encode([text], convert_to_numpy=True)[0]</span><br></pre></td></tr></table></figure><p>支持本地离线加载模型：只需下载模型文件到本地，通过路径指定即可避免联网。​</p><h3 id="（3）向量检索：快速找到相关片段​"><a href="#（3）向量检索：快速找到相关片段​" class="headerlink" title="（3）向量检索：快速找到相关片段​"></a>（3）向量检索：快速找到相关片段​</h3><p>向量存储在vector_store.py中实现，核心是余弦相似度计算：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 余弦相似度检索（src/vector_store.py）</span></span><br><span class="line">def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:</span><br><span class="line">    <span class="built_in">return</span> a @ b.T  <span class="comment"># 矩阵乘法等价于余弦相似度（已归一化向量）</span></span><br><span class="line"></span><br><span class="line">class InMemoryVectorStore:</span><br><span class="line">    def search(self, query_vector: np.ndarray, top_k: int = 4) -&gt; List[Tuple[str, <span class="built_in">float</span>]]:</span><br><span class="line">        scores = cosine_similarity(self.embeddings, query_vector.reshape(1, -1)).reshape(-1)</span><br><span class="line">        idx = np.argsort(-scores)[:top_k]  <span class="comment"># 取分数最高的前k个</span></span><br><span class="line">        <span class="built_in">return</span> [(self.texts[i], <span class="built_in">float</span>(scores[i])) <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br></pre></td></tr></table></figure><h3 id="（4）答案生成：结合知识的智能回答​"><a href="#（4）答案生成：结合知识的智能回答​" class="headerlink" title="（4）答案生成：结合知识的智能回答​"></a>（4）答案生成：结合知识的智能回答​</h3><p>生成模块llm.py支持三种模式：​</p><ul><li>启发式回答：无 LLM 时可用，直接从检索结果中提取相关句子​</li><li>llama.cpp 本地模型：支持.gguf格式模型，完全离线运行​</li><li>Ollama 客户端：支持多轮对话和流式输出，对中文友好</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提示词构建（src/llm.py）</span></span><br><span class="line">def build_prompt(question: str, contexts: List[str]) -&gt; str:</span><br><span class="line">    context_block = <span class="string">&quot;\n\n&quot;</span>.<span class="built_in">join</span>(f<span class="string">&quot;[Document &#123;i+1&#125;]\n&#123;c&#125;&quot;</span> <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(contexts))</span><br><span class="line">    <span class="built_in">return</span> f<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    请根据以下上下文回答问题，若答案不在上下文中请说明不知道。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    上下文：</span></span><br><span class="line"><span class="string">    &#123;context_block&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    问题：&#123;question&#125;</span></span><br><span class="line"><span class="string">    答案：</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><h1 id="三、快速上手：3-步实现本地文档问答"><a href="#三、快速上手：3-步实现本地文档问答" class="headerlink" title="三、快速上手：3 步实现本地文档问答"></a>三、快速上手：3 步实现本地文档问答</h1><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆项目（假设已获取仓库地址）</span></span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/MiaoJieF/LLM_rag.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> LLM_rag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>依赖说明：</p><ul><li><p><code>sentence-transformers</code>：用于生成文本向量</p></li><li><p><code>numpy</code>：向量计算</p></li><li><p><code>llama-cpp-python</code>：可选，用于本地 LLM 推理</p></li><li><p><code>requests</code>：Ollama 客户端依赖</p></li></ul><h2 id="2-构建索引（分块-向量化-存储）"><a href="#2-构建索引（分块-向量化-存储）" class="headerlink" title="2. 构建索引（分块 + 向量化 + 存储）"></a>2. 构建索引（分块 + 向量化 + 存储）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将data目录下的TXT文档构建索引</span></span><br><span class="line"></span><br><span class="line">python -m src.cli index --data_dir data --index_path index.pkl --chunk_size 500 --chunk_overlap 100</span><br></pre></td></tr></table></figure><p>执行后会生成<code>index.pkl</code>文件，包含所有文档的向量和文本片段。</p><h2 id="3-开始问答"><a href="#3-开始问答" class="headerlink" title="3. 开始问答"></a>3. 开始问答</h2><h3 id="单轮问答："><a href="#单轮问答：" class="headerlink" title="单轮问答："></a>单轮问答：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m src.cli ask --index_path index.pkl --question <span class="string">&quot;贵宾客户有哪些权益？&quot;</span> --top_k 4 --show_context</span><br></pre></td></tr></table></figure><h3 id="多轮对话："><a href="#多轮对话：" class="headerlink" title="多轮对话："></a>多轮对话：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先安装Ollama并拉取模型：ollama pull gemma3:1b</span></span><br><span class="line">python -m src.cli chat --index_path index.pkl --top_k 4 --show_context --ollama_model gemma3:1b --ollama_base_url http://127.0.0.1:11434</span><br></pre></td></tr></table></figure><h1 id="四、常见问题与进阶技巧"><a href="#四、常见问题与进阶技巧" class="headerlink" title="四、常见问题与进阶技巧"></a>四、常见问题与进阶技巧</h1><ol><li><p><strong>中文乱码问题</strong>：Windows 下可先执行<code>chcp 65001</code>切换编码，或使用 VSCode 终端</p></li><li><p><strong>本地模型路径设置</strong>：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌入模型本地路径</span></span><br><span class="line"><span class="built_in">export</span> EMBEDDING\_MODEL=<span class="string">&quot;E:/models/all-MiniLM-L6-v2&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># llama.cpp模型路径</span></span><br><span class="line"><span class="built_in">export</span> LLAMA\_CPP\_MODEL=<span class="string">&quot;E:/models/your\_model.gguf&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li><strong>分块参数调优</strong>：</li></ol><ul><li>长文档（如论文）：chunk_size&#x3D;800，overlap&#x3D;200</li><li>短文本（如问答库）：chunk_size&#x3D;300，overlap&#x3D;50</li></ul><ol start="4"><li><strong>检索效果优化</strong>：</li></ol><ul><li>增加<code>top_k</code>值（如 6）可获取更多上下文</li><li>尝试不同的嵌入模型（如<code>paraphrase-multilingual-MiniLM-L12-v2</code>支持多语言）</li></ul><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>这个简易版 localGPT 项目以 “最小可行” 的方式展示了 RAG 的核心流程，非常适合初学者理解检索增强生成的工作原理。通过逐模块阅读代码，你可以掌握：</p><ul><li><p>如何将文本转换为可计算的向量</p></li><li><p>如何通过向量相似度找到相关知识</p></li><li><p>如何让 LLM 基于参考文档生成可靠答案</p></li></ul><p>RAG 技术的应用远不止文档问答，还可以扩展到知识库构建、智能客服、代码助手等场景。动手实践这个项目，相信能为你深入学习 LLM 应用打下坚实基础。</p><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_rag">https://github.com/MiaoJieF/LLM_rag</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【LLM学习】【Ollama】二、微调</title>
      <link href="/2025/09/22/02_LLM_finetune/"/>
      <url>/2025/09/22/02_LLM_finetune/</url>
      
        <content type="html"><![CDATA[<p>在大语言模型（LLM）应用中，如何在有限资源下实现模型的领域适配是一个关键问题。LoRA（Low-Rank Adaptation）技术作为参数高效微调的代表，通过冻结预训练模型权重并注入可训练的低秩矩阵，在大幅减少参数量的同时保持良好性能。本文基于开源项目<a href="https://github.com/MiaoJieF/LLM_finetune">LLM_finetune</a>，详细介绍如何使用 LoRA 技术对 Gemma 模型进行银行领域微调，包含完整流程与实操指南。</p><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_finetune">https://github.com/MiaoJieF/LLM_finetune</a></p></blockquote><h1 id="一、项目概述"><a href="#一、项目概述" class="headerlink" title="一、项目概述"></a>一、项目概述</h1><p>LLM_finetune是一个专注于 Gemma 模型 LoRA 微调的完整工具链，专为银行领域场景优化。项目提供从数据集构建到模型部署的全流程支持，核心特点包括：</p><ul><li>轻量级：采用 LoRA 技术，仅训练少量参数（相比全量微调减少 90% 以上参数量）</li><li>专业性：内置银行领域数据集与评估体系</li><li>易用性：提供一键式脚本与详细配置说明</li><li>完整性：涵盖数据处理、训练、评估、对话交互全流程</li></ul><p>项目文件结构清晰，核心组件包括：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── banking_dataset.py      <span class="comment"># 银行领域数据集生成</span></span><br><span class="line">├── lora_finetune.py       <span class="comment"># LoRA微调核心脚本</span></span><br><span class="line">├── chat_with_model.py      <span class="comment"># 交互式对话工具</span></span><br><span class="line">├── compare_models.py       <span class="comment"># 微调前后效果对比</span></span><br><span class="line">└── quick_start.py         <span class="comment"># 一键式流程脚本</span></span><br></pre></td></tr></table></figure><h1 id="二、LoRA-微调核心技术解析"><a href="#二、LoRA-微调核心技术解析" class="headerlink" title="二、LoRA 微调核心技术解析"></a>二、LoRA 微调核心技术解析</h1><h2 id="1-LoRA-原理与优势"><a href="#1-LoRA-原理与优势" class="headerlink" title="1. LoRA 原理与优势"></a>1. LoRA 原理与优势</h2><p>LoRA 通过在模型关键层（如注意力模块）插入低秩矩阵分解参数，实现参数高效微调：</p><ul><li>冻结预训练模型权重，避免灾难性遗忘</li><li>仅训练低秩矩阵参数（通常数十万级），大幅降低计算资源需求</li><li>训练完成后可将低秩矩阵合并回原模型，不影响推理效率</li><li>在银行领域场景中，这种方式既能保留 Gemma 模型的通用能力，又能高效学习领域知识。</li></ul><h2 id="2-核心配置参数"><a href="#2-核心配置参数" class="headerlink" title="2. 核心配置参数"></a>2. 核心配置参数</h2><p>项目中 LoRA 的关键配置如下（定义于lora_finetune.py）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lora_config = &#123;</span><br><span class="line">    <span class="string">&quot;r&quot;</span>: <span class="number">8</span>,                    <span class="comment"># 低秩矩阵的秩，控制参数量与表达能力</span></span><br><span class="line">    <span class="string">&quot;lora_alpha&quot;</span>: <span class="number">32</span>,          <span class="comment"># 缩放参数，通常设为r的2-4倍</span></span><br><span class="line">    <span class="string">&quot;target_modules&quot;</span>: [        <span class="comment"># 目标微调模块（Gemma注意力关键组件）</span></span><br><span class="line">        <span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;lora_dropout&quot;</span>: <span class="number">0.1</span>,        <span class="comment"># Dropout防止过拟合</span></span><br><span class="line">    <span class="string">&quot;bias&quot;</span>: <span class="string">&quot;none&quot;</span>,           <span class="comment"># 不微调偏置参数</span></span><br><span class="line">    <span class="string">&quot;task_type&quot;</span>: <span class="string">&quot;CAUSAL_LM&quot;</span>   <span class="comment"># 因果语言建模任务</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>训练参数配置：</p><ul><li>训练轮数（epochs）：建议 3-20（银行领域小数据集推荐 5-10）</li><li>批次大小（batch_size）：根据 GPU 显存调整（4-16）</li><li>学习率：2e-4（LoRA 通常使用比全量微调更高的学习率）</li><li>最大序列长度：512 tokens（覆盖银行问答场景需求）</li></ul><h1 id="三、完整实操流程"><a href="#三、完整实操流程" class="headerlink" title="三、完整实操流程"></a>三、完整实操流程</h1><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><p><strong>前置条件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8+</span><br><span class="line">建议 GPU 显存 6GB 以上（支持 CUDA 11.8+）</span><br><span class="line">16GB 以上内存，10GB 以上存储空间</span><br></pre></td></tr></table></figure><p><strong>依赖安装</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>核心依赖包括：PyTorch、Transformers、PEFT、Datasets 等。</p><p><strong>模型准备</strong><br>在<a href="https://huggingface.co/google/gemma-3-1b-pt/tree/main">Hugging Face</a>下载模型文件，将 Gemma-1B 模型文件放置于models&#x2F;gemma3-1b&#x2F;目录，需包含：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config.json：模型配置文件</span><br><span class="line">tokenizer.json：分词器文件</span><br><span class="line">model.safetensors：模型权重文件</span><br></pre></td></tr></table></figure><h2 id="2-一键式微调流程"><a href="#2-一键式微调流程" class="headerlink" title="2. 一键式微调流程"></a>2. 一键式微调流程</h2><p>对于新手用户，推荐使用quick_start.py脚本完成全流程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python quick_start.py</span><br></pre></td></tr></table></figure><p>该脚本自动执行以下步骤：</p><ol><li>生成银行领域数据集（16 组专业问答对）</li><li>执行 LoRA 微调（输出至outputs&#x2F;lora_banking_时间戳&#x2F;）</li><li>对比基础模型与微调模型效果</li><li>生成评估报告（comparison_results_*.json）</li></ol><h2 id="3-分步执行"><a href="#3-分步执行" class="headerlink" title="3. 分步执行"></a>3. 分步执行</h2><p><strong>步骤 1：创建数据集</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python banking_dataset.py</span><br></pre></td></tr></table></figure><p>生成的数据集包含银行常见场景问答，如账户开户、转账流程、理财产品等，自动划分为训练集（80%）与验证集（20%）。</p><p><strong>步骤 2：执行 LoRA 微调</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python lora_finetune.py \</span><br><span class="line">  --model-path models/gemma3-1b \</span><br><span class="line">  --output-dir outputs/lora_banking \</span><br><span class="line">  --epochs 10 \</span><br><span class="line">  --batch-size 8 \</span><br><span class="line">  --learning-rate 2e-4</span><br></pre></td></tr></table></figure><p>训练过程中会自动保存：</p><ul><li>LoRA 适配器参数</li><li>分词器配置</li><li>训练日志与评估指标</li></ul><p><strong>步骤 3：模型对比测试</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python compare_models.py \</span><br><span class="line">  --peft-model-path outputs/lora_banking \</span><br><span class="line">  --base-model-path models/gemma3-1b \</span><br><span class="line">  --output-file comparison_results.json</span><br></pre></td></tr></table></figure><p>对比指标包括：</p><ul><li>关键词匹配率（专业术语准确率）</li><li>回答完整性评分</li><li>响应时间</li><li>综合效果评分</li></ul><p><strong>步骤 4：交互式对话</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat_with_model.py --model-path outputs/lora_banking</span><br></pre></td></tr></table></figure><p>支持流式输出，可直接测试微调后模型在银行场景的问答效果。</p><h1 id="四、微调效果"><a href="#四、微调效果" class="headerlink" title="四、微调效果"></a>四、微调效果</h1><p><img src="/images/02_1.png" alt="在这里插入图片描述"><br><img src="/images/02_2.png" alt="在这里插入图片描述"></p><h1 id="五、TODO"><a href="#五、TODO" class="headerlink" title="五、TODO"></a>五、TODO</h1><ul><li>将微调得到的LORA模型参数和Ollama模型结合创建新模型，通过Ollama直接调用</li></ul><blockquote><p>项目地址：<a href="https://github.com/MiaoJieF/LLM_finetune">https://github.com/MiaoJieF/LLM_finetune</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【LLM学习】【Ollama】一、本地运行与API调用</title>
      <link href="/2025/09/17/01_LLM_connect/"/>
      <url>/2025/09/17/01_LLM_connect/</url>
      
        <content type="html"><![CDATA[<h1 id="一、LLM-核心认知：从技术到应用"><a href="#一、LLM-核心认知：从技术到应用" class="headerlink" title="一、LLM 核心认知：从技术到应用"></a>一、LLM 核心认知：从技术到应用</h1><h2 id="1-技术原理：LLM-的-“底层骨架”-与-“训练密码”​"><a href="#1-技术原理：LLM-的-“底层骨架”-与-“训练密码”​" class="headerlink" title="1. 技术原理：LLM 的 “底层骨架” 与 “训练密码”​"></a>1. 技术原理：LLM 的 “底层骨架” 与 “训练密码”​</h2><p>LLM 的能力源于「架构设计」与「训练方法」的双重革新，核心目标是平衡 “模型规模” 与 “运行效率”：</p><h4 id="（1）核心架构：突破性能瓶颈的-3-大技术"><a href="#（1）核心架构：突破性能瓶颈的-3-大技术" class="headerlink" title="（1）核心架构：突破性能瓶颈的 3 大技术"></a>（1）核心架构：突破性能瓶颈的 3 大技术</h4><ul><li><p><strong>Transformer 架构：LLM 的 “地基”</strong><br>传统循环神经网络（RNN）需按文本顺序逐字处理，如同 “读句子只能从头读到尾”；而 Transformer 通过「自注意力机制」，可同时关注文本中所有词的关联（比如 “猫” 和 “抓老鼠” 的逻辑关系），实现 “全局上下文理解”，且支持并行计算（多 GPU 同时训练），为大规模模型奠定基础。</p></li><li><p><strong>超长上下文窗口：处理 “长篇文档” 的关键</strong><br>2025 年英伟达的 4M token 突破（约 300 万字，相当于 2 本《红楼梦》），核心依赖「YaRN-based RoPE 缩放技术」：</p><ul><li><p>先通过 “持续预训练” 让模型适应长文本；</p></li><li><p>再通过 “指令微调” 强化推理能力（避免 “读长文忘前文”）。</p><ul><li>对比：2023 年主流模型仅支持 128K token（约 9.6 万字），4M 突破让法律合同、科研论文的 “全文分析” 成为可能。</li></ul></li></ul></li><li><p><strong>混合专家（MoE）架构：效率革命的核心</strong><br>传统模型训练 &#x2F; 推理需激活全部参数（如 1750 亿参数模型每次都要调用 1750 亿），而 MoE 架构如同 “医院分科”—— 总参数庞大（如 Llama 4 的 2 万亿），但每次仅激活 “对应任务的专家模块”（如文本生成激活 2880 亿参数），通过「动态路由机制」（判断任务类型，分配对应 “专家”）降低计算成本。<br>国产案例：腾讯混元通过 “共享专家” 机制，800 亿总参数仅激活 130 亿，同时支持 25.6 万 token 上下文。</p></li></ul><h4 id="（2）训练流程：“预训练-微调”-的两段式逻辑"><a href="#（2）训练流程：“预训练-微调”-的两段式逻辑" class="headerlink" title="（2）训练流程：“预训练 + 微调” 的两段式逻辑"></a>（2）训练流程：“预训练 + 微调” 的两段式逻辑</h4><table><thead><tr><th>阶段</th><th>目标</th><th>数据特点</th><th>2025 年优化方向</th></tr></thead><tbody><tr><td>预训练</td><td>学习通用语言规律</td><td>海量无标注文本（万亿级）</td><td>提升数据质量（过滤低质内容）</td></tr><tr><td>微调</td><td>优化特定任务性能</td><td>高质量标注数据（百万级）</td><td>缩短训练时间（英伟达 H100 集群 5 小时训 1M 上下文模型）</td></tr></tbody></table><p><strong>节能技术补充</strong>：动态早期退出（Early Exiting）—— 模型生成简单内容（如 “今天天气如何”）时，仅用前几层网络计算；生成复杂内容（如代码）时再用完整网络，推理能耗降低 30%+。</p><h2 id="2-发展历程：从-“参数竞赛”-到-“生态分化”"><a href="#2-发展历程：从-“参数竞赛”-到-“生态分化”" class="headerlink" title="2. 发展历程：从 “参数竞赛” 到 “生态分化”"></a>2. 发展历程：从 “参数竞赛” 到 “生态分化”</h2><table><thead><tr><th>阶段</th><th>时间范围</th><th>核心特征</th><th>代表模型</th><th>关键突破</th></tr></thead><tbody><tr><td>萌芽期</td><td>2019-2022</td><td>参数规模快速扩张</td><td>GPT-2（16.1 亿）、GPT-3（1750 亿）</td><td>首次展现 “涌现能力”（参数超一定规模后能力质变）</td></tr><tr><td>开源爆发期</td><td>2023-2024</td><td>打破闭源壁垒</td><td>LLaMA 2（70 亿）、Mistral（7B）</td><td>平民化部署（消费级 GPU 可运行 7B 模型）</td></tr><tr><td>多元化竞争期</td><td>2025</td><td>效率优化 + 区域特色</td><td>Llama 3.3、ERNIE 4.5（4240 亿）、混元（800 亿）</td><td>4M 上下文、多模态融合、国产模型崛起</td></tr></tbody></table><p>LLM 的能力源于「架构设计」与「训练方法」的双重革新，核心目标是平衡 “模型规模” 与 “运行效率”：</p><h2 id="3-关键挑战与未来趋势"><a href="#3-关键挑战与未来趋势" class="headerlink" title="3. 关键挑战与未来趋势"></a>3. 关键挑战与未来趋势</h2><h4 id="（1）未解决的核心痛点（加示例更易理解）"><a href="#（1）未解决的核心痛点（加示例更易理解）" class="headerlink" title="（1）未解决的核心痛点（加示例更易理解）"></a>（1）未解决的核心痛点（加示例更易理解）</h4><ul><li><p><strong>幻觉问题</strong>：生成 “看似合理但错误” 的内容，如医疗模型将 “布洛芬用量” 错写为 “每日 5 次”（实际建议 3 次），在高精度领域（法律 &#x2F; 医疗）风险极高；</p></li><li><p><strong>监管碎片化</strong>：欧盟要求公开训练数据来源（推高成本），美国侧重自律，中国需政府备案，跨国企业合规成本增加；</p></li><li><p><strong>能耗与垄断</strong>：ChatGPT 每日推理能耗 564 兆瓦时（相当于 44 个 GPT-3 预训练能耗），且高性能模型研发需千亿级算力，中小团队难以参与。</p></li></ul><h4 id="（2）2025-未来方向"><a href="#（2）2025-未来方向" class="headerlink" title="（2）2025 + 未来方向"></a>（2）2025 + 未来方向</h4><ul><li><p><strong>技术突破</strong>：上下文窗口向 10M token 迈进（支持完整学术专著分析），边缘部署（手机 &#x2F; 平板运行 7B 模型）；</p></li><li><p><strong>生态分化</strong>：通用模型（如 Llama 3.3）做基础工具，垂直模型（如 “医疗 LLM”“法律 LLM”）做专业深度；</p></li><li><p><strong>治理落地</strong>：区域性认证体系成型（如欧盟 AI 认证），平衡 “创新” 与 “安全”。</p></li></ul><h1 id="二、Ollama简介"><a href="#二、Ollama简介" class="headerlink" title="二、Ollama简介"></a>二、Ollama简介</h1><p>Ollama 是一款开源的跨平台大语言模型部署工具，致力于让开发者和研究者在本地设备便捷运行大型语言模型（LLM），其核心使命是通过技术创新打破算力垄断，实现 AI 能力的民主化。以下从核心功能、技术优势、应用场景等维度展开介绍：</p><h2 id="1-极简部署体验"><a href="#1-极简部署体验" class="headerlink" title="1. 极简部署体验"></a>1. 极简部署体验</h2><p>采用类似 Docker 的容器化理念，用户通过简单命令（如ollama run llama3）即可一键下载并运行模型，自动处理环境配置与依赖安装，无需手动管理模型权重或复杂参数。支持 Llama、Mistral、Qwen 等 30 + 主流开源模型，覆盖从 7B 到 405B 参数规模，适配消费级显卡（如 7B 模型可在普通 GPU 流畅运行）。</p><h2 id="2-多模态与高性能优化"><a href="#2-多模态与高性能优化" class="headerlink" title="2. 多模态与高性能优化"></a>2. 多模态与高性能优化</h2><ul><li>技术架构：融合容器化部署与 AI 模型优化技术，独创分层量化技术（如 4-bit 量化）显著降低显存需求，增量加载机制减少内存占用，13B 模型仅需 16GB 内存即可启动。</li><li>多模态支持：兼容文本、代码、图像混合输入，支持长文本处理（单次对话 8k token 上下文），满足复杂场景需求。</li><li>性能升级：最新版本 v0.11.8 默认启用 Flash Attention，提升长序列推理效率；优化 GGUF 模型加载速度，减少内存碎片，并支持 AMD 显卡加速。</li></ul><h2 id="3-开发者友好工具链"><a href="#3-开发者友好工具链" class="headerlink" title="3. 开发者友好工具链"></a>3. 开发者友好工具链</h2><ul><li>API 与 SDK：提供 OpenAI 兼容的 REST API 及 Python&#x2F;JavaScript SDK，无缝对接 - LangChain 等框架，方便集成至企业级应用。</li><li>安全与版本控制：内置安全沙箱隔离运行环境，支持模型版本回溯与差异更新，保障数据隐私。</li></ul><h1 id="三、Ollama安装"><a href="#三、Ollama安装" class="headerlink" title="三、Ollama安装"></a>三、Ollama安装</h1><ul><li>官网下载：<a href="https://ollama.com/">https://ollama.com/</a></li><li>安装好后可在客户端中打开运行，也可在cmd中查看</li><li>打开cmd，输入</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama <span class="literal">--version</span></span><br></pre></td></tr></table></figure><p><img src="/images/01_1.png" alt="在这里插入图片描述"></p><ul><li>通过<code>ollama run &lt;model&gt;</code>运行大模型</li></ul><p><img src="/images/01_2.png" alt="在这里插入图片描述"></p><h1 id="四、Ollama常用命令"><a href="#四、Ollama常用命令" class="headerlink" title="四、Ollama常用命令"></a>四、Ollama常用命令</h1><ol><li>搜索远程仓库中支持的模型</li></ol><p>官网地址：<a href="https://ollama.com/library">https://ollama.com/library</a></p><ol start="3"><li>查看本地已下载的模型</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure><ol start="4"><li>下载并运行模型</li></ol><p>自动下载（若本地没有）并启动模型交互：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run &lt;模型名[:标签]&gt;</span><br></pre></td></tr></table></figure><ol start="4"><li>删除本地模型</li></ol><p>清理不需要的模型以释放空间：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama <span class="built_in">rm</span> &lt;模型名[:标签]&gt;</span><br></pre></td></tr></table></figure><ol start="5"><li>查看模型详细配置</li></ol><p>显示模型的参数、系统提示等信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama show &lt;模型名&gt;</span><br></pre></td></tr></table></figure><ol start="6"><li>非交互式生成内容</li></ol><p>直接输入提示词，让模型生成结果后退出（适合脚本或批量处理）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama generate &lt;模型名&gt; <span class="string">&quot;你的提示词&quot;</span></span><br></pre></td></tr></table></figure><ol start="7"><li>结构化对话（JSON 输出）</li></ol><p>以 JSON 格式输出对话内容，方便程序解析：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama chat &lt;模型名&gt; --format json</span><br></pre></td></tr></table></figure><ol start="8"><li>设置推理参数</li></ol><p>临时调整模型生成参数（如温度、上下文长度等）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run &lt;模型名&gt; -- &lt;参数名&gt; &lt;值&gt;</span><br></pre></td></tr></table></figure><p>示例：ollama run llama3 – temperature 0.3（降低随机性，使输出更确定）。</p><ol start="9"><li>启动 &#x2F; 停止 Ollama 服务</li></ol><p>部分系统（如 Linux）需手动控制后台服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">ollama serve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止服务（在另一个终端执行，或用kill命令）</span></span><br><span class="line">pkill ollama</span><br></pre></td></tr></table></figure><ol start="10"><li>查看服务状态<br>检查 Ollama 服务是否在运行：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linux/macOS</span></span><br><span class="line">ps aux | grep ollama</span><br><span class="line"></span><br><span class="line"><span class="comment"># Windows（PowerShell）</span></span><br><span class="line">Get-Process ollama</span><br></pre></td></tr></table></figure><ol start="11"><li>获取命令帮助</li></ol><p>查看所有命令或单个命令的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有命令</span></span><br><span class="line">ollama <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某个命令的详细说明（如run）</span></span><br><span class="line">ollama <span class="built_in">help</span> run</span><br></pre></td></tr></table></figure><ol start="12"><li>导出 &#x2F; 导入模型（备份）</li></ol><p>将本地模型导出为文件（用于迁移或备份）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导出</span></span><br><span class="line">ollama <span class="built_in">export</span> &lt;模型名&gt; &lt;导出路径&gt;</span><br><span class="line"><span class="comment"># 示例：ollama export llama3 ./llama3-backup</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入</span></span><br><span class="line">ollama import &lt;新模型名&gt; &lt;导出文件路径&gt;</span><br></pre></td></tr></table></figure><p>示例：ollama import my-llama .&#x2F;llama3-backup</p><h1 id="五、API-调用"><a href="#五、API-调用" class="headerlink" title="五、API 调用"></a>五、API 调用</h1><ul><li><p>Ollama 提供了 HTTP API，可以通过编程方式调用模型。默认 API 地址为<a href="http://localhost:11434。">http://localhost:11434。</a></p></li><li><p>设置模型路径</p></li></ul><p>如果在客户端修改了模型保存路径，需要先在cmd中修改路径（仅在本次运行中生效）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMD 中</span></span><br><span class="line"><span class="built_in">set</span> OLLAMA_MODELS=E:\Ollama_models</span><br></pre></td></tr></table></figure><ul><li>启动服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">ollama serve</span><br></pre></td></tr></table></figure><h2 id="1-Python调用实例"><a href="#1-Python调用实例" class="headerlink" title="1. Python调用实例"></a>1. Python调用实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_ollama</span>(<span class="params">prompt, model=<span class="string">&quot;gemma3:1b&quot;</span></span>):</span><br><span class="line">    url = <span class="string">&quot;http://localhost:11434/api/generate&quot;</span></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">&quot;model&quot;</span>: model,</span><br><span class="line">        <span class="string">&quot;prompt&quot;</span>: prompt,</span><br><span class="line">        <span class="string">&quot;stream&quot;</span>: <span class="literal">False</span>  <span class="comment"># 非流式输出</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    response = requests.post(url, json=payload)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        <span class="keyword">return</span> result[<span class="string">&quot;response&quot;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;请求失败: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    prompt = <span class="string">&quot;请简要介绍一下机器学习的基本概念&quot;</span></span><br><span class="line">    response = call_ollama(prompt)</span><br><span class="line">    <span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><h2 id="2-流式输出示例："><a href="#2-流式输出示例：" class="headerlink" title="2. 流式输出示例："></a>2. 流式输出示例：</h2><p>如果需要实时获取模型输出（如聊天应用），可以使用流式输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stream_ollama</span>(<span class="params">prompt, model=<span class="string">&quot;gemma3:1b&quot;</span></span>):</span><br><span class="line">    url = <span class="string">&quot;http://localhost:11434/api/generate&quot;</span></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">&quot;model&quot;</span>: model,</span><br><span class="line">        <span class="string">&quot;prompt&quot;</span>: prompt,</span><br><span class="line">        <span class="string">&quot;stream&quot;</span>: <span class="literal">True</span>  <span class="comment"># 启用流式输出</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    response = requests.post(url, json=payload, stream=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.iter_lines():</span><br><span class="line">        <span class="keyword">if</span> line:</span><br><span class="line">            data = json.loads(line)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;response&quot;</span> <span class="keyword">in</span> data:</span><br><span class="line">                <span class="built_in">print</span>(data[<span class="string">&quot;response&quot;</span>], end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">if</span> data.get(<span class="string">&quot;done&quot;</span>, <span class="literal">False</span>):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>()  <span class="comment"># 换行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    prompt = <span class="string">&quot;请详细解释什么是深度学习，以及它与传统机器学习的区别&quot;</span></span><br><span class="line">    stream_ollama(prompt)</span><br></pre></td></tr></table></figure><h2 id="3-多轮交流示例"><a href="#3-多轮交流示例" class="headerlink" title="3. 多轮交流示例"></a>3. 多轮交流示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stream_ollama_chat</span>(<span class="params">messages, model=<span class="string">&quot;gemma3:1b&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    支持多轮对话的Ollama聊天函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        messages: 对话历史列表，每个元素包含role和content</span></span><br><span class="line"><span class="string">        model: 使用的模型名称</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    url = <span class="string">&quot;http://localhost:11434/api/chat&quot;</span></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">&quot;model&quot;</span>: model,</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: messages,</span><br><span class="line">        <span class="string">&quot;stream&quot;</span>: <span class="literal">True</span>  <span class="comment"># 启用流式输出</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 常用参数配置示例</span></span><br><span class="line">    options = &#123;</span><br><span class="line">        <span class="string">&quot;temperature&quot;</span>: <span class="number">0.7</span>,  <span class="comment"># 控制输出的随机性，值越高越随机</span></span><br><span class="line">        <span class="string">&quot;top_p&quot;</span>: <span class="number">0.9</span>,  <span class="comment"># nucleus sampling参数</span></span><br><span class="line">        <span class="string">&quot;top_k&quot;</span>: <span class="number">40</span>,  <span class="comment"># top-k采样参数</span></span><br><span class="line">        <span class="string">&quot;num_ctx&quot;</span>: <span class="number">2048</span>,  <span class="comment"># 上下文窗口大小</span></span><br><span class="line">        <span class="string">&quot;num_predict&quot;</span>: <span class="number">512</span>,  <span class="comment"># 最大生成token数</span></span><br><span class="line">        <span class="string">&quot;repeat_penalty&quot;</span>: <span class="number">1.1</span>,  <span class="comment"># 重复惩罚</span></span><br><span class="line">        <span class="string">&quot;seed&quot;</span>: <span class="number">42</span>  <span class="comment"># 随机种子，设置固定值可获得可重复结果</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果提供了options参数，则添加到请求中</span></span><br><span class="line">    <span class="keyword">if</span> options:</span><br><span class="line">        payload[<span class="string">&quot;options&quot;</span>] = options</span><br><span class="line"></span><br><span class="line">    response = requests.post(url, json=payload, stream=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    full_response = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.iter_lines():</span><br><span class="line">        <span class="keyword">if</span> line:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                data = json.loads(line)</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;message&quot;</span> <span class="keyword">in</span> data <span class="keyword">and</span> data[<span class="string">&quot;message&quot;</span>][<span class="string">&quot;role&quot;</span>] == <span class="string">&quot;assistant&quot;</span>:</span><br><span class="line">                    content = data[<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">                    <span class="built_in">print</span>(content, end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">                    full_response += content</span><br><span class="line">                <span class="keyword">if</span> data.get(<span class="string">&quot;done&quot;</span>, <span class="literal">False</span>):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> json.JSONDecodeError:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>()  <span class="comment"># 换行</span></span><br><span class="line">    <span class="keyword">return</span> full_response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_with_ollama</span>(<span class="params">model=<span class="string">&quot;gemma3:1b&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    多轮对话交互函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    messages = []</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;开始多轮对话（输入 &#x27;quit&#x27; 或 &#x27;exit&#x27; 退出）:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;\n你: &quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> user_input.lower() <span class="keyword">in</span> [<span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;exit&#x27;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;对话结束&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加用户消息到对话历史</span></span><br><span class="line">        messages.append(&#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: user_input</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;AI: &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="comment"># 获取AI回复</span></span><br><span class="line">        ai_response = stream_ollama_chat(messages, model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加AI回复到对话历史</span></span><br><span class="line">        messages.append(&#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: ai_response</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 单次对话示例</span></span><br><span class="line">    <span class="comment"># messages = [</span></span><br><span class="line">    <span class="comment">#     &#123;</span></span><br><span class="line">    <span class="comment">#         &quot;role&quot;: &quot;user&quot;,</span></span><br><span class="line">    <span class="comment">#         &quot;content&quot;: &quot;请详细解释什么是深度学习，以及它与传统机器学习的区别&quot;</span></span><br><span class="line">    <span class="comment">#     &#125;</span></span><br><span class="line">    <span class="comment"># ]</span></span><br><span class="line">    <span class="comment"># print(&quot;单次对话示例:&quot;)</span></span><br><span class="line">    <span class="comment"># stream_ollama_chat(messages)</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span> + <span class="string">&quot;=&quot;</span> * <span class="number">50</span> + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多轮对话示例</span></span><br><span class="line">    chat_with_ollama()</span><br></pre></td></tr></table></figure><h1 id="附录：通过Huggingface下载大模型并本地运行"><a href="#附录：通过Huggingface下载大模型并本地运行" class="headerlink" title="附录：通过Huggingface下载大模型并本地运行"></a>附录：通过Huggingface下载大模型并本地运行</h1><ol><li>Huggingface<br>(1) 代码调用直接下载（均因网络问题失败，打开vpn也不行）<br>(2) 手动下载<br>在<a href="https://huggingface.co/">https://huggingface.co/</a>找到相应模型下载目录下所有文件<br><img src="/images/01_3.png" alt="在这里插入图片描述"></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">from transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"># 模型名称和本地保存路径</span><br><span class="line">model_name = <span class="string">&quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;</span></span><br><span class="line">model_save_path = <span class="string">&quot;./DeepSeek-R1-Distill-Qwen-1.5B&quot;</span>  # 自定义本地保存路径</span><br><span class="line"></span><br><span class="line"># 创建保存目录（如果不存在）</span><br><span class="line">os.makedirs(model_save_path, exist_ok=True)</span><br><span class="line"></span><br><span class="line"># 加载并保存tokenizer</span><br><span class="line"># tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"># tokenizer.save_pretrained(model_save_path)</span><br><span class="line"># print(f<span class="string">&quot;Tokenizer已保存至: &#123;model_save_path&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 加载并保存模型</span><br><span class="line"># model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">#     model_name,</span><br><span class="line">#     device_map=<span class="string">&quot;auto&quot;</span>  # 自动分配设备（优先GPU）</span><br><span class="line"># )</span><br><span class="line"># model.save_pretrained(model_save_path)</span><br><span class="line"># print(f<span class="string">&quot;模型已保存至: &#123;model_save_path&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 配置量化参数</span><br><span class="line">quantization_config = BitsAndBytesConfig(load_in_8bit=True)</span><br><span class="line"></span><br><span class="line"># 后续可直接从本地加载（取消注释即可使用）</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_save_path)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_save_path,</span><br><span class="line">                                             quantization_config=quantization_config,</span><br><span class="line">                                             device_map=<span class="string">&quot;auto&quot;</span>)</span><br><span class="line"></span><br><span class="line">print(f<span class="string">&quot;模型已加载至: &#123;model.device&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 对话示例</span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;解释一下勾股定理&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line">inputs = tokenizer.apply_chat_template(messages,</span><br><span class="line">                                       add_generation_prompt=True,</span><br><span class="line">                                       tokenize=True,</span><br><span class="line">                                       return_dict=True,</span><br><span class="line">                                       return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">50000</span>)</span><br><span class="line"># 解码并打印生成的回复（排除输入部分）</span><br><span class="line">print(tokenizer.decode(outputs[<span class="number">0</span>][inputs[<span class="string">&quot;input_ids&quot;</span>].shape[-<span class="number">1</span>]:], skip_special_tokens=True))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【MySQL】MySQL基础教程</title>
      <link href="/2025/07/19/10_MySQL_base/"/>
      <url>/2025/07/19/10_MySQL_base/</url>
      
        <content type="html"><![CDATA[<h2 id="一、安装与登录"><a href="#一、安装与登录" class="headerlink" title="一、安装与登录"></a>一、安装与登录</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>windows下载链接：<a href="https://dev.mysql.com/downloads/mysql/">https://dev.mysql.com/downloads/mysql/</a>，默认安装</p><h3 id="2-登录"><a href="#2-登录" class="headerlink" title="2. 登录"></a>2. 登录</h3><p>打开cmd，cd进入到安装路径，然后运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p>输入密码后进入 MySQL 命令行。</p><h2 id="二、基本操作"><a href="#二、基本操作" class="headerlink" title="二、基本操作"></a>二、基本操作</h2><h3 id="1-创建数据库"><a href="#1-创建数据库" class="headerlink" title="1. 创建数据库"></a>1. 创建数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE mydatabase;</span><br></pre></td></tr></table></figure><h3 id="2-查询已创建数据库"><a href="#2-查询已创建数据库" class="headerlink" title="2. 查询已创建数据库"></a>2. 查询已创建数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW DATABASES;</span><br></pre></td></tr></table></figure><h3 id="3-选择数据库"><a href="#3-选择数据库" class="headerlink" title="3. 选择数据库"></a>3. 选择数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE mydatabase;</span><br></pre></td></tr></table></figure><h3 id="4-查看当前使用数据库"><a href="#4-查看当前使用数据库" class="headerlink" title="4. 查看当前使用数据库"></a>4. 查看当前使用数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT DATABASE();</span><br></pre></td></tr></table></figure><h3 id="5-创建表"><a href="#5-创建表" class="headerlink" title="5. 创建表"></a>5. 创建表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">-- 用户表</span><br><span class="line">CREATE TABLE users (</span><br><span class="line">  id INT PRIMARY KEY AUTO_INCREMENT,</span><br><span class="line">  name VARCHAR(50) NOT NULL,</span><br><span class="line">  email VARCHAR(100) UNIQUE,</span><br><span class="line">  age INT CHECK (age &gt;= 18),</span><br><span class="line">  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 订单表</span><br><span class="line">CREATE TABLE orders (</span><br><span class="line">  order_id INT PRIMARY KEY AUTO_INCREMENT,</span><br><span class="line">  user_id INT,</span><br><span class="line">  product_name VARCHAR(100),</span><br><span class="line">  amount DECIMAL(10, 2),</span><br><span class="line">  order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</span><br><span class="line">  FOREIGN KEY (user_id) REFERENCES users(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="6-查询所有表"><a href="#6-查询所有表" class="headerlink" title="6. 查询所有表"></a>6. 查询所有表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW TABLES;</span><br></pre></td></tr></table></figure><h3 id="7-插入数据"><a href="#7-插入数据" class="headerlink" title="7. 插入数据"></a>7. 插入数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-- 向 users 表插入数据</span><br><span class="line">INSERT INTO users (name, email, age)</span><br><span class="line">VALUES </span><br><span class="line">  (&#x27;John Doe&#x27;, &#x27;john@example.com&#x27;, 25),</span><br><span class="line">  (&#x27;Jane Smith&#x27;, &#x27;jane@example.com&#x27;, 30);</span><br><span class="line"></span><br><span class="line">-- 向 orders 表插入数据</span><br><span class="line">INSERT INTO orders (user_id, product_name, amount)</span><br><span class="line">VALUES </span><br><span class="line">  (1, &#x27;Laptop&#x27;, 999.99),</span><br><span class="line">  (1, &#x27;Mouse&#x27;, 29.99),</span><br><span class="line">  (2, &#x27;Keyboard&#x27;, 59.99);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_17.png" alt="在这里插入图片描述"><br> <img src="/2025/07/19/10_MySQL_base/img_13.png" alt="在这里插入图片描述"></p><h3 id="8-查询数据"><a href="#8-查询数据" class="headerlink" title="8. 查询数据"></a>8. 查询数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 查询所有用户</span><br><span class="line">SELECT * FROM users;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 查询 John 的订单</span><br><span class="line">SELECT * </span><br><span class="line">FROM orders </span><br><span class="line">WHERE user_id = (SELECT id FROM users WHERE name = &#x27;John Doe&#x27;);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_1.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 联合查询用户及其订单</span><br><span class="line">SELECT users.name, orders.product_name, orders.amount</span><br><span class="line">FROM users</span><br><span class="line">JOIN orders ON users.id = orders.user_id;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_6.png" alt="在这里插入图片描述"></p><h3 id="9-更新数据"><a href="#9-更新数据" class="headerlink" title="9. 更新数据"></a>9. 更新数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 将 John 的年龄更新为 26</span><br><span class="line">UPDATE users </span><br><span class="line">SET age = 26 </span><br><span class="line">WHERE name = &#x27;John Doe&#x27;;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_15.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- 将所有订单金额增加 10%</span><br><span class="line">UPDATE orders </span><br><span class="line">SET amount = amount * 1.1;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_2.png" alt="在这里插入图片描述"></p><h3 id="10-删除数据"><a href="#10-删除数据" class="headerlink" title="10. 删除数据"></a>10. 删除数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-- 删除 Jane 的订单</span><br><span class="line">DELETE FROM orders </span><br><span class="line">WHERE user_id = (SELECT id FROM users WHERE name = &#x27;Jane Smith&#x27;);</span><br><span class="line"></span><br><span class="line">-- 删除 Jane 用户</span><br><span class="line">DELETE FROM users </span><br><span class="line">WHERE name = &#x27;Jane Smith&#x27;;</span><br></pre></td></tr></table></figure><h2 id="三、数据类型与约束"><a href="#三、数据类型与约束" class="headerlink" title="三、数据类型与约束"></a>三、数据类型与约束</h2><h3 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h3><p>MySQL 支持多种数据类型，常见的有：</p><ul><li>数值类型: <code>INT</code>, <code>FLOAT</code>, <code>DOUBLE</code>, <code>DECIMAL</code></li><li>字符串类型: <code>VARCHAR</code>, <code>CHAR</code>, <code>TEXT</code></li><li>日期时间类型: <code>DATE</code>, <code>TIME</code>, <code>DATETIME</code>, <code>TIMESTAMP</code></li><li>布尔类型: <code>TINYINT(1)</code>（0 表示FALSE，1 表示TRUE）</li></ul><h3 id="2-约束"><a href="#2-约束" class="headerlink" title="2. 约束"></a>2. 约束</h3><p>约束用于确保数据的完整性，常见约束有：</p><ul><li><code>PRIMARY KEY</code>: 唯一标识表中每条记录。</li><li><code>UNIQUE</code>: 确保字段值唯一。</li><li><code>NOT NULL</code>: 字段不能为空。</li><li><code>CHECK</code>: 限制字段值的范围。</li><li><code>FOREIGN KEY</code>: 建立表间关联。</li></ul><h2 id="四、表间关系示例"><a href="#四、表间关系示例" class="headerlink" title="四、表间关系示例"></a>四、表间关系示例</h2><h3 id="1-一对多关系"><a href="#1-一对多关系" class="headerlink" title="1. 一对多关系"></a>1. 一对多关系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">-- 部门表（主表）</span><br><span class="line">CREATE TABLE departments (</span><br><span class="line">  id INT PRIMARY KEY,</span><br><span class="line">  name VARCHAR(50)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 员工表（从表）</span><br><span class="line">CREATE TABLE employees (</span><br><span class="line">  id INT PRIMARY KEY,</span><br><span class="line">  name VARCHAR(50),</span><br><span class="line">  department_id INT,</span><br><span class="line">  salary DECIMAL(10, 2),</span><br><span class="line">  FOREIGN KEY (department_id) REFERENCES departments(id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 插入示例数据</span><br><span class="line">INSERT INTO departments (id, name) VALUES (1, &#x27;IT&#x27;), (2, &#x27;HR&#x27;);</span><br><span class="line">INSERT INTO employees (id, name, department_id, salary) </span><br><span class="line">VALUES </span><br><span class="line">  (101, &#x27;Alice&#x27;, 1, 8000),</span><br><span class="line">  (102, &#x27;Bob&#x27;, 1, 7500),</span><br><span class="line">  (103, &#x27;Charlie&#x27;, 2, 6000);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_7.png" alt="在这里插入图片描述"><br> <img src="/2025/07/19/10_MySQL_base/img_16.png" alt="在这里插入图片描述"></p><h3 id="2-多对多关系"><a href="#2-多对多关系" class="headerlink" title="2. 多对多关系"></a>2. 多对多关系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">-- 学生表</span><br><span class="line">CREATE TABLE students (</span><br><span class="line">  student_id INT PRIMARY KEY,</span><br><span class="line">  name VARCHAR(50)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 课程表</span><br><span class="line">CREATE TABLE courses (</span><br><span class="line">  course_id INT PRIMARY KEY,</span><br><span class="line">  course_name VARCHAR(50)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 中间表（学生-课程关联）</span><br><span class="line">CREATE TABLE student_courses (</span><br><span class="line">  student_id INT,</span><br><span class="line">  course_id INT,</span><br><span class="line">  PRIMARY KEY (student_id, course_id),</span><br><span class="line">  FOREIGN KEY (student_id) REFERENCES students(student_id),</span><br><span class="line">  FOREIGN KEY (course_id) REFERENCES courses(course_id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 插入示例数据</span><br><span class="line">INSERT INTO students (student_id, name) VALUES (1, &#x27;David&#x27;), (2, &#x27;Emma&#x27;);</span><br><span class="line">INSERT INTO courses (course_id, course_name) VALUES (1001, &#x27;Math&#x27;), (1002, &#x27;Physics&#x27;);</span><br><span class="line">INSERT INTO student_courses (student_id, course_id) VALUES (1, 1001), (1, 1002), (2, 1001);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_11.png" alt="在这里插入图片描述"><br> <img src="/2025/07/19/10_MySQL_base/img_8.png" alt="在这里插入图片描述"><br> <img src="/2025/07/19/10_MySQL_base/img_12.png" alt="在这里插入图片描述"></p><h2 id="五、高级查询示例"><a href="#五、高级查询示例" class="headerlink" title="五、高级查询示例"></a>五、高级查询示例</h2><h3 id="1-连接查询"><a href="#1-连接查询" class="headerlink" title="1. 连接查询"></a>1. 连接查询</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 查询每个部门的员工数量</span><br><span class="line">SELECT departments.name, COUNT(employees.id) AS employee_count</span><br><span class="line">FROM departments</span><br><span class="line">LEFT JOIN employees ON departments.id = employees.department_id</span><br><span class="line">GROUP BY departments.name;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_5.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 查询选修了 Math 课程的学生</span><br><span class="line">SELECT students.name</span><br><span class="line">FROM students</span><br><span class="line">JOIN student_courses ON students.student_id = student_courses.student_id</span><br><span class="line">JOIN courses ON student_courses.course_id = courses.course_id</span><br><span class="line">WHERE courses.course_name = &#x27;Math&#x27;;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_14.png" alt="在这里插入图片描述"></p><h3 id="2-子查询"><a href="#2-子查询" class="headerlink" title="2. 子查询"></a>2. 子查询</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 查询工资高于平均工资的员工</span><br><span class="line">SELECT name, salary</span><br><span class="line">FROM employees</span><br><span class="line">WHERE salary &gt; (SELECT AVG(salary) FROM employees);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_3.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 查询没有订单的用户</span><br><span class="line">SELECT name</span><br><span class="line">FROM users</span><br><span class="line">WHERE id NOT IN (SELECT DISTINCT user_id FROM orders);</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_10.png" alt="在这里插入图片描述"></p><h3 id="3-聚合函数"><a href="#3-聚合函数" class="headerlink" title="3. 聚合函数"></a>3. 聚合函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 统计每个部门的总工资和平均工资</span><br><span class="line">SELECT </span><br><span class="line">  department_id,</span><br><span class="line">  SUM(salary) AS total_salary,</span><br><span class="line">  AVG(salary) AS average_salary,</span><br><span class="line">  COUNT(*) AS employee_count</span><br><span class="line">FROM employees</span><br><span class="line">GROUP BY department_id;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_4.png" alt="在这里插入图片描述"></p><h3 id="4-分组与过滤"><a href="#4-分组与过滤" class="headerlink" title="4. 分组与过滤"></a>4. 分组与过滤</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 查询订单总金额超过 1000 的用户</span><br><span class="line">SELECT </span><br><span class="line">  users.name,</span><br><span class="line">  SUM(orders.amount) AS total_spent</span><br><span class="line">FROM users</span><br><span class="line">JOIN orders ON users.id = orders.user_id</span><br><span class="line">GROUP BY users.id</span><br><span class="line">HAVING SUM(orders.amount) &gt; 1000;</span><br></pre></td></tr></table></figure><p><img src="/2025/07/19/10_MySQL_base/img_9.png" alt="在这里插入图片描述"></p><h2 id="六、用户权限管理"><a href="#六、用户权限管理" class="headerlink" title="六、用户权限管理"></a>六、用户权限管理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-- 创建只读用户</span><br><span class="line">CREATE USER &#x27;readonly_user&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;password&#x27;;</span><br><span class="line">GRANT SELECT ON mydatabase.* TO &#x27;readonly_user&#x27;@&#x27;localhost&#x27;;</span><br><span class="line"></span><br><span class="line">-- 创建读写用户（仅限 users 表）</span><br><span class="line">CREATE USER &#x27;user_manager&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;password&#x27;;</span><br><span class="line">GRANT SELECT, INSERT, UPDATE, DELETE ON mydatabase.users TO &#x27;user_manager&#x27;@&#x27;localhost&#x27;;</span><br></pre></td></tr></table></figure><h2 id="七、备份与恢复"><a href="#七、备份与恢复" class="headerlink" title="七、备份与恢复"></a>七、备份与恢复</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 备份单个表</span><br><span class="line">mysqldump -u root -p mydatabase users &gt; users_backup.sql</span><br><span class="line"></span><br><span class="line"># 备份多个表</span><br><span class="line">mysqldump -u root -p mydatabase users orders &gt; data_backup.sql</span><br><span class="line"></span><br><span class="line"># 恢复表（需先创建数据库）</span><br><span class="line">mysql -u root -p mydatabase &lt; data_backup.sql</span><br></pre></td></tr></table></figure><p>命令拆解：</p><ul><li><code>mysqldump</code>：备份工具主命令</li><li><code>-u root</code>：指定连接数据库的用户（这里是 root）</li><li><code>-p</code>：提示输入用户密码（执行后会要求输入 root 的密码）</li><li><code>mydatabase</code>：指定要备份的数据库名</li><li><code>users</code>：指定要备份的表名（仅备份该表）</li><li><code>users_backup.sql</code>：将备份内容输出到 users_backup.sql 文件（重定向输出）</li></ul><p>备份原理：</p><ul><li>mysqldump 通过 root 用户连接 MySQL 服务器，验证密码后访问 mydatabase 数据库；</li><li>读取 users 表的元信息（结构）：生成 CREATE TABLE users (…) 语句（包含字段、类型、约束等）；</li><li>读取 users 表的所有数据：生成 INSERT INTO users (…) VALUES (…) 语句（一行数据对应一条或批量 INSERT）；</li><li>为了避免恢复时的冲突，会自动添加 DROP TABLE IF EXISTS users;（如果表已存在则先删除）；</li><li>将上述所有 SQL 语句通过 &gt; 重定向到 users_backup.sql 文件，完成备份。</li></ul>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Java】ArrayList 和 LinkedList 详解</title>
      <link href="/2025/06/15/08_Java_arraylist_linkedlist/"/>
      <url>/2025/06/15/08_Java_arraylist_linkedlist/</url>
      
        <content type="html"><![CDATA[<h1 id="一、数据结构与核心特性"><a href="#一、数据结构与核心特性" class="headerlink" title="一、数据结构与核心特性"></a>一、数据结构与核心特性</h1><h2 id="1-ArrayList"><a href="#1-ArrayList" class="headerlink" title="1. ArrayList"></a>1. ArrayList</h2><ul><li>数据结构：基于动态数组实现，元素存储在连续内存中。</li><li>核心特性：<br>– 通过索引随机访问元素，时间复杂度 O (1)。<br>– 扩容机制：容量不足时新建数组（原容量的 1.5 倍）并复制元素。<br>– 尾部添加 &#x2F; 删除效率高（O (1)），中间操作需移动元素（O (n)）。<br>– 不直接支持 poll&#x2F;offer，需借助 ArrayDeque 或 LinkedList 包装。</li></ul><h2 id="2-LinkedList"><a href="#2-LinkedList" class="headerlink" title="2. LinkedList"></a>2. LinkedList</h2><ul><li>数据结构：双向链表，每个节点包含前驱、后继指针和元素值。</li><li>核心特性：<br>– 任意位置插入 &#x2F; 删除仅需修改指针（O (1)），但随机访问需遍历（O (n)）。<br>– 实现 Deque 接口，原生支持 poll&#x2F;offer 等队列操作。<br>– 无容量限制，节点动态分配内存，内存占用高于 ArrayList。</li></ul><h2 id="二、核心方法对比"><a href="#二、核心方法对比" class="headerlink" title="二、核心方法对比"></a>二、核心方法对比</h2><table><thead><tr><th>操作类型</th><th>ArrayList</th><th>LinkedList</th></tr></thead><tbody><tr><td>构造</td><td><code>new ArrayList&lt;&gt;()</code>（初始容量 10）</td><td><code>new LinkedList&lt;&gt;()</code></td></tr><tr><td>添加元素</td><td><code>add(E e)</code>（尾部，O(1)）<br><code>add(index, e)</code>（中间，O(n)）</td><td><code>add(e)</code>（尾部，O(1)）<br><code>addFirst(e)</code> &#x2F; <code>addLast(e)</code>（O(1)）</td></tr><tr><td>删除元素</td><td><code>remove(index)</code>（O(n)）</td><td><code>removeFirst()</code> &#x2F; <code>removeLast()</code>（O(1)）</td></tr><tr><td>随机访问</td><td><code>get(index)</code>（O(1)）</td><td><code>get(index)</code>（O(n)，需遍历链表）</td></tr><tr><td>队列操作（poll）</td><td>需包装为 <code>ArrayDeque</code>，本质数组操作</td><td><code>poll()</code>（取头部，O(1)）<br><code>pollFirst()</code> &#x2F; <code>pollLast()</code>（O(1)）</td></tr><tr><td>队列操作（offer）</td><td>需包装为 <code>ArrayDeque</code>，尾部添加 O(1)</td><td><code>offer(e)</code>（尾部，O(1)）<br><code>offerFirst(e)</code> &#x2F; <code>offerLast(e)</code>（O(1)）</td></tr></tbody></table><h2 id="三、poll-和-offer-方法详解"><a href="#三、poll-和-offer-方法详解" class="headerlink" title="三、poll 和 offer 方法详解"></a>三、poll 和 offer 方法详解</h2><h3 id="1-方法定义与功能"><a href="#1-方法定义与功能" class="headerlink" title="1. 方法定义与功能"></a>1. 方法定义与功能</h3><ul><li>poll()：获取并移除集合头部元素，空集合返回 null（区别于 remove() 的异常）。</li><li>offer(E e)：添加元素到集合，成功返回 true（LinkedList 无容量限制，始终成功）。</li></ul><h3 id="2-LinkedList-中的队列操作"><a href="#2-LinkedList-中的队列操作" class="headerlink" title="2. LinkedList 中的队列操作"></a>2. LinkedList 中的队列操作</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">LinkedList&lt;String&gt; deque = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line"><span class="comment">// 添加元素</span></span><br><span class="line">deque.offer(<span class="string">&quot;A&quot;</span>);       <span class="comment">// 尾部添加，等价于 addLast()</span></span><br><span class="line">deque.offerFirst(<span class="string">&quot;B&quot;</span>);  <span class="comment">// 头部添加</span></span><br><span class="line">deque.offerLast(<span class="string">&quot;C&quot;</span>);   <span class="comment">// 尾部添加，等价于 offer()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取并移除元素</span></span><br><span class="line"><span class="type">String</span> <span class="variable">head</span> <span class="operator">=</span> deque.poll();      <span class="comment">// 取头部（&quot;B&quot;），队列变为 [&quot;A&quot;, &quot;C&quot;]</span></span><br><span class="line"><span class="type">String</span> <span class="variable">tail</span> <span class="operator">=</span> deque.pollLast();  <span class="comment">// 取尾部（&quot;C&quot;），队列变为 [&quot;A&quot;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 空集合处理</span></span><br><span class="line"><span class="type">String</span> <span class="variable">empty</span> <span class="operator">=</span> deque.poll(); <span class="comment">// 返回 null，不抛异常</span></span><br></pre></td></tr></table></figure><h3 id="3-ArrayList-实现队列操作（需适配器）"><a href="#3-ArrayList-实现队列操作（需适配器）" class="headerlink" title="3. ArrayList 实现队列操作（需适配器）"></a>3. ArrayList 实现队列操作（需适配器）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 推荐方式：使用 ArrayDeque 包装 ArrayList</span></span><br><span class="line">Deque&lt;Integer&gt; queue = <span class="keyword">new</span> <span class="title class_">ArrayDeque</span>&lt;&gt;(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>));</span><br><span class="line">queue.offer(<span class="number">4</span>);   <span class="comment">// 尾部添加，队列变为 [1,2,3,4]</span></span><br><span class="line"><span class="type">int</span> <span class="variable">first</span> <span class="operator">=</span> queue.poll(); <span class="comment">// 取头部（1），队列变为 [2,3,4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 不推荐方式：用 LinkedList 包装 ArrayList（性能差）</span></span><br><span class="line">Queue&lt;Integer&gt; arrayListQueue = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;());</span><br><span class="line">arrayListQueue.offer(<span class="number">5</span>); <span class="comment">// 内部通过链表添加，非数组直接操作</span></span><br></pre></td></tr></table></figure><h2 id="四、性能与适用场景"><a href="#四、性能与适用场景" class="headerlink" title="四、性能与适用场景"></a>四、性能与适用场景</h2><h3 id="1-性能对比"><a href="#1-性能对比" class="headerlink" title="1. 性能对比"></a>1. 性能对比</h3><table><thead><tr><th>操作</th><th>ArrayList</th><th>LinkedList</th></tr></thead><tbody><tr><td>尾部添加</td><td>O(1)</td><td>O(1)</td></tr><tr><td>中间插入</td><td>O(n) (移动元素)</td><td>O(1) (修改指针)</td></tr><tr><td>随机访问</td><td>O(1)</td><td>O(n) (遍历链表)</td></tr><tr><td>poll&#x2F;offer 头部</td><td>需适配器，O(1)</td><td>O(1)</td></tr></tbody></table><h3 id="2-适用场景"><a href="#2-适用场景" class="headerlink" title="2. 适用场景"></a>2. 适用场景</h3><ul><li>选 ArrayList 的场景：<ul><li>频繁随机访问（如 get(index)）。</li><li>尾部添加 &#x2F; 删除为主（如日志记录）。</li><li>数据量可预测，避免频繁扩容。</li></ul></li><li>选 LinkedList 的场景：<ul><li>频繁在头部 &#x2F; 中间插入 &#x2F; 删除（如任务队列）。</li><li>需要使用 poll&#x2F;offer 等双端队列操作。</li><li>数据量不确定，不希望有扩容开销。</li></ul></li></ul><h2 id="五、代码示例：综合应用"><a href="#五、代码示例：综合应用" class="headerlink" title="五、代码示例：综合应用"></a>五、代码示例：综合应用</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Deque;</span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">import</span> java.util.Queue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ListComparison</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// === ArrayList 作为普通列表 ===</span></span><br><span class="line">        ArrayList&lt;String&gt; arrayList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        arrayList.add(<span class="string">&quot;苹果&quot;</span>);</span><br><span class="line">        arrayList.add(<span class="string">&quot;香蕉&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;ArrayList 随机访问：&quot;</span> + arrayList.get(<span class="number">1</span>)); <span class="comment">// 输出：香蕉</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// === LinkedList 作为队列 ===</span></span><br><span class="line">        Queue&lt;String&gt; taskQueue = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        taskQueue.offer(<span class="string">&quot;数据采集&quot;</span>);</span><br><span class="line">        taskQueue.offer(<span class="string">&quot;模型训练&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;队列处理顺序：&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (!taskQueue.isEmpty()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;- &quot;</span> + taskQueue.poll()); <span class="comment">// 按添加顺序输出</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// === LinkedList 作为栈 ===</span></span><br><span class="line">        Deque&lt;String&gt; stack = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        stack.offerFirst(<span class="string">&quot;HTML&quot;</span>);</span><br><span class="line">        stack.offerFirst(<span class="string">&quot;CSS&quot;</span>);</span><br><span class="line">        stack.offerFirst(<span class="string">&quot;JavaScript&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;\n栈弹出顺序：&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;- &quot;</span> + stack.pollFirst()); <span class="comment">// 后入先出</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// === ArrayList 实现队列（借助 ArrayDeque）===</span></span><br><span class="line">        Deque&lt;Integer&gt; arrayQueue = <span class="keyword">new</span> <span class="title class_">ArrayDeque</span>&lt;&gt;(Arrays.asList(<span class="number">10</span>, <span class="number">20</span>));</span><br><span class="line">        arrayQueue.offer(<span class="number">30</span>); <span class="comment">// 尾部添加</span></span><br><span class="line">        System.out.println(<span class="string">&quot;\nArrayDeque 包装 ArrayList 队列：&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (!arrayQueue.isEmpty()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;- &quot;</span> + arrayQueue.poll()); <span class="comment">// 10, 20, 30</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="六、总结与注意事项"><a href="#六、总结与注意事项" class="headerlink" title="六、总结与注意事项"></a>六、总结与注意事项</h2><h3 id="1-数据结构决定性能："><a href="#1-数据结构决定性能：" class="headerlink" title="1. 数据结构决定性能："></a>1. 数据结构决定性能：</h3><ul><li>ArrayList 适合 “读多写少” 场景，尤其随机访问频繁时。</li><li>LinkedList 适合 “写多读少” 场景，尤其需要队列 &#x2F; 栈操作时。</li></ul><h3 id="2-队列操作的最佳实践："><a href="#2-队列操作的最佳实践：" class="headerlink" title="2. 队列操作的最佳实践："></a>2. 队列操作的最佳实践：</h3><ul><li>优先使用 LinkedList 或 ArrayDeque 实现队列 &#x2F; 栈，而非包装 ArrayList。</li><li>ArrayDeque 的性能通常优于 LinkedList，因数组连续存储减少内存跳转。</li></ul><h3 id="3-线程安全："><a href="#3-线程安全：" class="headerlink" title="3. 线程安全："></a>3. 线程安全：</h3><ul><li>两者均非线程安全，多线程环境需用 Collections.synchronizedList() 包装或使用 CopyOnWriteArrayList。</li></ul><p>通过理解两者的底层实现与方法特性，可根据业务场景（如数据访问模式、操作频率）选择更合适的集合类，优化程序效率。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Java】集合类</title>
      <link href="/2025/05/23/07_Java_collection%20/"/>
      <url>/2025/05/23/07_Java_collection%20/</url>
      
        <content type="html"><![CDATA[<p>Java 集合类是用于存储和操作数据的核心工具，主要分为Collection和Map两大接口体系，每个接口下有多种实现类。以下是 Java 集合类的完整介绍：</p><h1 id="一、集合框架核心接口"><a href="#一、集合框架核心接口" class="headerlink" title="一、集合框架核心接口"></a>一、集合框架核心接口</h1><h2 id="1-Collection-接口"><a href="#1-Collection-接口" class="headerlink" title="1. Collection 接口"></a>1. Collection 接口</h2><p>集合体系的根接口，定义了添加、删除、遍历等基本操作</p><p>三个主要子接口：</p><ul><li>List：有序、可重复元素的集合，支持索引访问</li><li>Set：唯一元素集合，不允许重复值</li><li>Queue：按特定顺序（如 FIFO）处理元素的集合</li></ul><h2 id="2-Map-接口"><a href="#2-Map-接口" class="headerlink" title="2. Map 接口"></a>2. Map 接口</h2><p>以键值对（Key-Value）形式存储数据，键唯一</p><h1 id="二、常用集合类及其特点"><a href="#二、常用集合类及其特点" class="headerlink" title="二、常用集合类及其特点"></a>二、常用集合类及其特点</h1><h2 id="1-List-实现类"><a href="#1-List-实现类" class="headerlink" title="1. List 实现类"></a>1. List 实现类</h2><table><thead><tr><th>类名</th><th>特点</th></tr></thead><tbody><tr><td>ArrayList</td><td>基于动态数组实现，随机访问高效，中间位置增删效率低</td></tr><tr><td>LinkedList</td><td>基于双向链表实现，增删效率高，随机访问效率低</td></tr><tr><td>Vector</td><td>线程安全，性能低于 ArrayList，已逐渐被淘汰</td></tr></tbody></table><h2 id="2-Set-实现类"><a href="#2-Set-实现类" class="headerlink" title="2. Set 实现类"></a>2. Set 实现类</h2><table><thead><tr><th>类名</th><th>特点</th></tr></thead><tbody><tr><td>HashSet</td><td>基于哈希表（底层为 HashMap），不保证元素顺序</td></tr><tr><td>LinkedHashSet</td><td>继承 HashSet，用链表维护插入顺序，插入和访问顺序一致</td></tr><tr><td>TreeSet</td><td>基于红黑树实现，元素自然排序或按比较器排序</td></tr></tbody></table><h2 id="3-Queue-实现类"><a href="#3-Queue-实现类" class="headerlink" title="3. Queue 实现类"></a>3. Queue 实现类</h2><table><thead><tr><th>类名</th><th>特点</th></tr></thead><tbody><tr><td>LinkedList</td><td>实现 Deque 接口，可作为双向队列使用</td></tr><tr><td>PriorityQueue</td><td>基于堆结构，按优先级排序，不允许 null 元素</td></tr><tr><td>ArrayDeque</td><td>基于数组的双端队列，不允许 null 元素</td></tr></tbody></table><h2 id="4-Map-实现类"><a href="#4-Map-实现类" class="headerlink" title="4. Map 实现类"></a>4. Map 实现类</h2><table><thead><tr><th>类名</th><th>特点</th></tr></thead><tbody><tr><td>HashMap</td><td>基于哈希表，不保证顺序，允许键值为 null</td></tr><tr><td>LinkedHashMap</td><td>继承 HashMap，用链表维护插入顺序或访问顺序</td></tr><tr><td>TreeMap</td><td>基于红黑树，按键排序，不允许键为 null</td></tr><tr><td>Hashtable</td><td>线程安全，性能低于 HashMap，不允许键值为 null</td></tr></tbody></table><h1 id="三、线程安全的集合类"><a href="#三、线程安全的集合类" class="headerlink" title="三、线程安全的集合类"></a>三、线程安全的集合类</h1><ul><li>同步包装类</li></ul><p>通过Collections.synchronizedXXX()转换非线程安全集合：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; syncList = Collections.synchronizedList(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;());</span><br></pre></td></tr></table></figure><ul><li>并发集合类（java.util.concurrent 包）</li></ul><p>ConcurrentHashMap：分段锁实现的线程安全 HashMap，性能优于 Hashtable<br>CopyOnWriteArrayList：写操作时创建副本的线程安全 ArrayList</p><h1 id="四、集合常用操作示例"><a href="#四、集合常用操作示例" class="headerlink" title="四、集合常用操作示例"></a>四、集合常用操作示例</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CollectionDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// ============== List 操作示例 ==============</span></span><br><span class="line">        System.out.println(<span class="string">&quot;===== List 操作 =====&quot;</span>);</span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 添加元素</span></span><br><span class="line">        list.add(<span class="string">&quot;apple&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;banana&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;cherry&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;添加后的List: &quot;</span> + list); <span class="comment">// [apple, banana, cherry]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 基础查询（可能抛出异常）</span></span><br><span class="line">        System.out.println(<span class="string">&quot;索引1的元素: &quot;</span> + list.get(<span class="number">1</span>)); <span class="comment">// banana</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 安全查询（带默认值）</span></span><br><span class="line">        System.out.println(<span class="string">&quot;索引10的元素: &quot;</span> + getOrDefault(list, <span class="number">10</span>, <span class="string">&quot;default&quot;</span>)); <span class="comment">// default</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 判断元素是否存在</span></span><br><span class="line">        System.out.println(<span class="string">&quot;是否包含apple: &quot;</span> + list.contains(<span class="string">&quot;apple&quot;</span>)); <span class="comment">// true</span></span><br><span class="line">        System.out.println(<span class="string">&quot;是否包含grape: &quot;</span> + list.contains(<span class="string">&quot;grape&quot;</span>)); <span class="comment">// false</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 遍历元素</span></span><br><span class="line">        System.out.print(<span class="string">&quot;遍历方式1: &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String fruit : list) &#123;</span><br><span class="line">            System.out.print(fruit + <span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        System.out.print(<span class="string">&quot;\n遍历方式2: &quot;</span>);</span><br><span class="line">        list.forEach(fruit -&gt; System.out.print(fruit + <span class="string">&quot; &quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// ============== Set 操作示例 ==============</span></span><br><span class="line">        System.out.println(<span class="string">&quot;\n\n===== Set 操作 =====&quot;</span>);</span><br><span class="line">        Set&lt;Integer&gt; set = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 添加元素</span></span><br><span class="line">        set.add(<span class="number">10</span>);</span><br><span class="line">        set.add(<span class="number">20</span>);</span><br><span class="line">        set.add(<span class="number">20</span>); <span class="comment">// 重复元素会被忽略</span></span><br><span class="line">        System.out.println(<span class="string">&quot;添加后的Set: &quot;</span> + set); <span class="comment">// [10, 20]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 基础查询</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Set大小: &quot;</span> + set.size()); <span class="comment">// 2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 查询带默认值</span></span><br><span class="line">        <span class="type">Integer</span> <span class="variable">value</span> <span class="operator">=</span> set.contains(<span class="number">20</span>) ? <span class="number">20</span> : -<span class="number">1</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;存在的值: &quot;</span> + value); <span class="comment">// 20</span></span><br><span class="line">        </span><br><span class="line">        value = set.contains(<span class="number">40</span>) ? <span class="number">40</span> : -<span class="number">1</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;不存在的值: &quot;</span> + value); <span class="comment">// -1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 遍历元素</span></span><br><span class="line">        System.out.print(<span class="string">&quot;遍历Set: &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Integer num : set) &#123;</span><br><span class="line">            System.out.print(num + <span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// ============== Map 操作示例 ==============</span></span><br><span class="line">        System.out.println(<span class="string">&quot;\n\n===== Map 操作 =====&quot;</span>);</span><br><span class="line">        Map&lt;String, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 添加元素</span></span><br><span class="line">        map.put(<span class="string">&quot;one&quot;</span>, <span class="number">1</span>);</span><br><span class="line">        map.put(<span class="string">&quot;two&quot;</span>, <span class="number">2</span>);</span><br><span class="line">        map.put(<span class="string">&quot;three&quot;</span>, <span class="number">3</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;添加后的Map: &quot;</span> + map); <span class="comment">// &#123;one=1, two=2, three=3&#125;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 基础查询</span></span><br><span class="line">        System.out.println(<span class="string">&quot;键two对应的值: &quot;</span> + map.get(<span class="string">&quot;two&quot;</span>)); <span class="comment">// 2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 安全查询（带默认值）</span></span><br><span class="line">        <span class="type">Integer</span> <span class="variable">val</span> <span class="operator">=</span> map.get(<span class="string">&quot;four&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;键four的值（无默认值）: &quot;</span> + val); <span class="comment">// null</span></span><br><span class="line">        </span><br><span class="line">        val = map.getOrDefault(<span class="string">&quot;four&quot;</span>, <span class="number">0</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;键four的值（带默认值）: &quot;</span> + val); <span class="comment">// 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 使用computeIfAbsent初始化默认值</span></span><br><span class="line">        <span class="type">Integer</span> <span class="variable">count</span> <span class="operator">=</span> map.computeIfAbsent(<span class="string">&quot;four&quot;</span>, k -&gt; <span class="number">4</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;键four的值: &quot;</span> + count); <span class="comment">// 4</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// ============== Stream 操作示例 ==============</span></span><br><span class="line">        System.out.println(<span class="string">&quot;\n===== Stream 操作 =====&quot;</span>);</span><br><span class="line">        <span class="comment">// 查找第一个以&quot;b&quot;开头的元素（基础方式）</span></span><br><span class="line">        Optional&lt;String&gt; resultOpt = list.stream()</span><br><span class="line">            .filter(s -&gt; s.startsWith(<span class="string">&quot;b&quot;</span>))</span><br><span class="line">            .findFirst();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (resultOpt.isPresent()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;找到的元素: &quot;</span> + resultOpt.get()); <span class="comment">// banana</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;未找到匹配元素&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 查找第一个以&quot;z&quot;开头的元素（带默认值）</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> list.stream()</span><br><span class="line">            .filter(s -&gt; s.startsWith(<span class="string">&quot;z&quot;</span>))</span><br><span class="line">            .findFirst()</span><br><span class="line">            .orElse(<span class="string">&quot;未找到&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;找到的元素: &quot;</span> + result); <span class="comment">// 未找到</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 自定义工具方法：获取List元素，不存在时返回默认值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; T <span class="title function_">getOrDefault</span><span class="params">(List&lt;T&gt; list, <span class="type">int</span> index, T defaultValue)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (list == <span class="literal">null</span> || index &lt; <span class="number">0</span> || index &gt;= list.size()) &#123;</span><br><span class="line">            <span class="keyword">return</span> defaultValue;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="五、字典（Map）遍历"><a href="#五、字典（Map）遍历" class="headerlink" title="五、字典（Map）遍历"></a>五、字典（Map）遍历</h1><h2 id="1-遍历键值对（EntrySet）"><a href="#1-遍历键值对（EntrySet）" class="headerlink" title="1. 遍历键值对（EntrySet）"></a>1. 遍历键值对（EntrySet）</h2><p>这是最高效的遍历方式，尤其适合需要同时访问键和值的场景。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">map.put(<span class="string">&quot;apple&quot;</span>, <span class="number">1</span>);</span><br><span class="line">map.put(<span class="string">&quot;banana&quot;</span>, <span class="number">2</span>);</span><br><span class="line">map.put(<span class="string">&quot;cherry&quot;</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用for-each循环遍历EntrySet</span></span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Key: &quot;</span> + entry.getKey() + <span class="string">&quot;, Value: &quot;</span> + entry.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用Iterator（适合需要在遍历中删除元素的场景）</span></span><br><span class="line">Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator = map.entrySet().iterator();</span><br><span class="line"><span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">    Map.Entry&lt;String, Integer&gt; entry = iterator.next();</span><br><span class="line">    <span class="keyword">if</span> (entry.getValue() == <span class="number">2</span>) &#123;</span><br><span class="line">        iterator.remove(); <span class="comment">// 安全删除元素</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-单独遍历键（KeySet）"><a href="#2-单独遍历键（KeySet）" class="headerlink" title="2. 单独遍历键（KeySet）"></a>2. 单独遍历键（KeySet）</h2><p>如果只需要访问键，可以使用这种方式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用for-each循环遍历键</span></span><br><span class="line"><span class="keyword">for</span> (String key : map.keySet()) &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Key: &quot;</span> + key);</span><br><span class="line">    <span class="comment">// 如果需要值，可以通过map.get(key)获取，但效率稍低</span></span><br><span class="line">    System.out.println(<span class="string">&quot;Value: &quot;</span> + map.get(key));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用Stream API（Java 8+）</span></span><br><span class="line">map.keySet().stream()</span><br><span class="line">    .forEach(key -&gt; System.out.println(<span class="string">&quot;Key: &quot;</span> + key));</span><br></pre></td></tr></table></figure><h2 id="3-单独遍历值（Values）"><a href="#3-单独遍历值（Values）" class="headerlink" title="3. 单独遍历值（Values）"></a>3. 单独遍历值（Values）</h2><p>若只需要访问值，可采用此方式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用for-each循环遍历值</span></span><br><span class="line"><span class="keyword">for</span> (Integer value : map.values()) &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Value: &quot;</span> + value);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用Stream API收集值到List</span></span><br><span class="line">List&lt;Integer&gt; valuesList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(map.values());</span><br></pre></td></tr></table></figure><h1 id="六、集合选择原则"><a href="#六、集合选择原则" class="headerlink" title="六、集合选择原则"></a>六、集合选择原则</h1><ul><li>有序可重复场景<br>频繁随机访问：选ArrayList<br>频繁增删操作：选LinkedList</li><li>唯一元素场景<br>不要求顺序：选HashSet<br>需插入顺序：选LinkedHashSet<br>需排序：选TreeSet</li><li>键值对存储场景<br>不要求顺序：选HashMap<br>需排序：选TreeMap</li><li>多线程环境<br>优先使用ConcurrentHashMap、CopyOnWriteArrayList等并发集合类</li></ul><h1 id="七、注意事项"><a href="#七、注意事项" class="headerlink" title="七、注意事项"></a>七、注意事项</h1><ul><li>迭代修改异常<br>遍历集合时修改结构会触发ConcurrentModificationException，建议使用Iterator.remove()或for-each循环</li><li>哈希方法重写<br>HashMap和HashSet依赖hashCode()和equals()，自定义类需正确重写这两个方法</li><li>性能优化<br>ArrayList可指定初始容量减少扩容开销<br>LinkedList随机访问需遍历链表，避免高频索引操作</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Java】数组</title>
      <link href="/2025/05/21/06_Java_array/"/>
      <url>/2025/05/21/06_Java_array/</url>
      
        <content type="html"><![CDATA[<p>在 Java 中，数组是一种存储固定大小同类型元素的数据结构。以下是几种常见的数组创建方法：</p><h2 id="1-声明并初始化数组"><a href="#1-声明并初始化数组" class="headerlink" title="1. 声明并初始化数组"></a>1. 声明并初始化数组</h2><p>使用 new 关键字创建数组，并指定长度。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 声明数组变量</span></span><br><span class="line"><span class="type">int</span>[] numbers;</span><br><span class="line"><span class="comment">// 创建数组并分配内存空间</span></span><br><span class="line">numbers = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">5</span>]; <span class="comment">// 长度为5的整数数组</span></span><br></pre></td></tr></table></figure><h2 id="2-声明并直接初始化数组元素"><a href="#2-声明并直接初始化数组元素" class="headerlink" title="2. 声明并直接初始化数组元素"></a>2. 声明并直接初始化数组元素</h2><p>使用花括号 {} 直接初始化数组元素，无需指定长度。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>[] numbers = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;; <span class="comment">// 长度为5的整数数组</span></span><br><span class="line">String[] names = &#123;<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Charlie&quot;</span>&#125;; <span class="comment">// 字符串数组</span></span><br></pre></td></tr></table></figure><h2 id="3-动态初始化数组元素"><a href="#3-动态初始化数组元素" class="headerlink" title="3. 动态初始化数组元素"></a>3. 动态初始化数组元素</h2><p>先创建数组，再逐个赋值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>[] numbers = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">3</span>];</span><br><span class="line">numbers[<span class="number">0</span>] = <span class="number">100</span>;</span><br><span class="line">numbers[<span class="number">1</span>] = <span class="number">200</span>;</span><br><span class="line">numbers[<span class="number">2</span>] = <span class="number">300</span>;</span><br></pre></td></tr></table></figure><h2 id="4-多维数组"><a href="#4-多维数组" class="headerlink" title="4. 多维数组"></a>4. 多维数组</h2><p>创建二维或多维数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二维数组 - 3行4列</span></span><br><span class="line"><span class="type">int</span>[][] matrix = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">3</span>][<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 直接初始化二维数组</span></span><br><span class="line"><span class="type">int</span>[][] matrix2 = &#123;</span><br><span class="line">    &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;,</span><br><span class="line">    &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;,</span><br><span class="line">    &#123;<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 不规则二维数组（每行长度不同）</span></span><br><span class="line"><span class="type">int</span>[][] jaggedArray = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">3</span>][];</span><br><span class="line">jaggedArray[<span class="number">0</span>] = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">jaggedArray[<span class="number">1</span>] = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">4</span>];</span><br><span class="line">jaggedArray[<span class="number">2</span>] = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">3</span>];</span><br></pre></td></tr></table></figure><h2 id="5-使用-Arrays-工具类填充数组"><a href="#5-使用-Arrays-工具类填充数组" class="headerlink" title="5. 使用 Arrays 工具类填充数组"></a>5. 使用 Arrays 工具类填充数组</h2><p>使用 java.util.Arrays 类的方法快速填充数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span>[] numbers = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">5</span>];</span><br><span class="line">Arrays.fill(numbers, <span class="number">10</span>); <span class="comment">// 所有元素填充为10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出: [10, 10, 10, 10, 10]</span></span><br><span class="line">System.out.println(Arrays.toString(numbers));</span><br></pre></td></tr></table></figure><h2 id="6-匿名数组"><a href="#6-匿名数组" class="headerlink" title="6. 匿名数组"></a>6. 匿名数组</h2><p>创建临时数组，无需显式声明变量。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 作为方法参数使用</span></span><br><span class="line">printArray(<span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printArray</span><span class="params">(<span class="type">int</span>[] arr)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> num : arr) &#123;</span><br><span class="line">        System.out.print(num + <span class="string">&quot; &quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-数组长度"><a href="#7-数组长度" class="headerlink" title="7. 数组长度"></a>7. 数组长度</h2><p>数组的长度是固定的，通过 length 属性获取。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>[] numbers = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">10</span>];</span><br><span class="line">System.out.println(<span class="string">&quot;数组长度: &quot;</span> + numbers.length); <span class="comment">// 输出: 10</span></span><br></pre></td></tr></table></figure><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>数组长度不可变：一旦创建，数组的长度不能改变。</li><li>索引越界：访问数组元素时，索引必须在 0 到 length-1 范围内，否则会抛出 ArrayIndexOutOfBoundsException。</li><li>默认值：数组创建后，元素会被自动初始化为默认值（例如 int 为 0，Object 为 null）。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Java】反射</title>
      <link href="/2025/05/20/05_Java_reflect/"/>
      <url>/2025/05/20/05_Java_reflect/</url>
      
        <content type="html"><![CDATA[<p>在 Java 里，反射是一项强大的功能，它允许程序在运行时对自身进行检查，或者操作程序的内部属性。借助反射，Java 代码能够在运行时动态获取类的信息、创建对象、调用方法以及访问或修改字段，即便在编译时这些类的信息可能并不明确。</p><h1 id="1-反射的核心类"><a href="#1-反射的核心类" class="headerlink" title="1. 反射的核心类"></a>1. 反射的核心类</h1><p>Java 反射的功能主要由以下几个位于java.lang.reflect包和java.lang包中的类实现：</p><ul><li>Class 类：代表一个类，是反射的基础。通过它可以获取类的结构信息。</li><li>Field 类：表示类的成员变量（字段），可用于获取或设置字段值。</li><li>Method 类：表示类的方法，可用于调用方法。</li><li>Constructor 类：表示类的构造方法，可用于创建对象。</li><li>Modifier 类：提供静态方法和常量，用于解析类、方法和字段的修饰符。</li></ul><h1 id="2-获取-Class-对象的三种方式"><a href="#2-获取-Class-对象的三种方式" class="headerlink" title="2. 获取 Class 对象的三种方式"></a>2. 获取 Class 对象的三种方式</h1><p>要使用反射，首先得获取目标类的Class对象，有以下三种常用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式一：通过类名.class获取</span></span><br><span class="line">Class&lt;?&gt; clazz1 = String.class;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式二：通过对象.getClass()获取</span></span><br><span class="line"><span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">Class&lt;?&gt; clazz2 = str.getClass();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式三：通过全类名的字符串获取（可能抛出ClassNotFoundException）</span></span><br><span class="line">Class&lt;?&gt; clazz3 = Class.forName(<span class="string">&quot;java.lang.String&quot;</span>);</span><br></pre></td></tr></table></figure><h1 id="3-反射的常见应用场景"><a href="#3-反射的常见应用场景" class="headerlink" title="3. 反射的常见应用场景"></a>3. 反射的常见应用场景</h1><h2 id="1-动态创建对象"><a href="#1-动态创建对象" class="headerlink" title="(1) 动态创建对象"></a>(1) 动态创建对象</h2><p>可以利用反射调用类的构造方法来创建对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取Class对象</span></span><br><span class="line">Class&lt;?&gt; clazz = Class.forName(<span class="string">&quot;java.util.ArrayList&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取无参构造方法</span></span><br><span class="line">Constructor&lt;?&gt; constructor = clazz.getConstructor();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建实例</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">instance</span> <span class="operator">=</span> constructor.newInstance();</span><br></pre></td></tr></table></figure><h2 id="2-动态调用方法"><a href="#2-动态调用方法" class="headerlink" title="(2) 动态调用方法"></a>(2) 动态调用方法</h2><p>通过反射能够在运行时调用对象的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取Class对象</span></span><br><span class="line">Class&lt;?&gt; clazz = Class.forName(<span class="string">&quot;java.lang.String&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建实例</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">instance</span> <span class="operator">=</span> clazz.getConstructor(String.class).newInstance(<span class="string">&quot;Hello&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取方法</span></span><br><span class="line"><span class="type">Method</span> <span class="variable">method</span> <span class="operator">=</span> clazz.getMethod(<span class="string">&quot;substring&quot;</span>, <span class="type">int</span>.class, <span class="type">int</span>.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用方法</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">result</span> <span class="operator">=</span> method.invoke(instance, <span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line">System.out.println(result); <span class="comment">// 输出 &quot;el&quot;</span></span><br></pre></td></tr></table></figure><h2 id="3-访问和修改字段"><a href="#3-访问和修改字段" class="headerlink" title="(3) 访问和修改字段"></a>(3) 访问和修改字段</h2><p>使用反射可以在运行时访问和修改对象的字段：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取Class对象</span></span><br><span class="line">Class&lt;?&gt; clazz = Class.forName(<span class="string">&quot;java.util.Date&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建实例</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">instance</span> <span class="operator">=</span> clazz.getConstructor().newInstance();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取字段</span></span><br><span class="line"><span class="type">Field</span> <span class="variable">field</span> <span class="operator">=</span> clazz.getDeclaredField(<span class="string">&quot;fastTime&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置可访问（针对private字段）</span></span><br><span class="line">field.setAccessible(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取字段值</span></span><br><span class="line"><span class="type">long</span> <span class="variable">time</span> <span class="operator">=</span> (<span class="type">long</span>) field.get(instance);</span><br><span class="line">System.out.println(time);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改字段值</span></span><br><span class="line">field.set(instance, System.currentTimeMillis());</span><br></pre></td></tr></table></figure><h2 id="4-获取类的结构信息"><a href="#4-获取类的结构信息" class="headerlink" title="(4) 获取类的结构信息"></a>(4) 获取类的结构信息</h2><p>借助反射能够获取类的各种结构信息，例如:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Class&lt;?&gt; clazz = String.class;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取类名</span></span><br><span class="line"><span class="type">String</span> <span class="variable">className</span> <span class="operator">=</span> clazz.getName();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取修饰符</span></span><br><span class="line"><span class="type">int</span> <span class="variable">modifiers</span> <span class="operator">=</span> clazz.getModifiers();</span><br><span class="line"><span class="type">boolean</span> <span class="variable">isPublic</span> <span class="operator">=</span> Modifier.isPublic(modifiers);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有公共方法</span></span><br><span class="line">Method[] methods = clazz.getMethods();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有声明的字段</span></span><br><span class="line">Field[] fields = clazz.getDeclaredFields();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有接口</span></span><br><span class="line">Class&lt;?&gt;[] interfaces = clazz.getInterfaces();</span><br></pre></td></tr></table></figure><h1 id="4-反射的优缺点"><a href="#4-反射的优缺点" class="headerlink" title="4. 反射的优缺点"></a>4. 反射的优缺点</h1><h2 id="1-优点"><a href="#1-优点" class="headerlink" title="(1) 优点"></a>(1) 优点</h2><ul><li><p>灵活性高：可以在运行时处理未知的类，适用于框架开发，像 Spring、Hibernate 等。</p></li><li><p>可扩展性强：能够通过配置文件动态加载类，无需修改源代码。</p></li><li><p>适合工具开发：IDE、调试器等工具可以利用反射分析类的结构。</p><h2 id="2-缺点"><a href="#2-缺点" class="headerlink" title="(2) 缺点"></a>(2) 缺点</h2></li><li><p>性能开销大：反射操作涉及动态解析，比直接调用效率低。</p></li><li><p>破坏封装性：可以访问和修改私有成员，违背了面向对象的设计原则。</p></li><li><p>安全性风险：可能会导致敏感信息泄露，或者执行不安全的操作。</p><h1 id="5-反射的注意事项"><a href="#5-反射的注意事项" class="headerlink" title="5. 反射的注意事项"></a>5. 反射的注意事项</h1></li><li><p>异常处理：反射操作可能会抛出多种异常，如ClassNotFoundException、NoSuchMethodException、IllegalAccessException等，需要进行适当的处理。</p></li><li><p>性能优化：对于需要频繁调用的反射操作，可以考虑缓存Method、Field等对象。</p></li><li><p>安全性检查：在使用setAccessible(true)时要格外谨慎，避免破坏类的封装性。</p></li><li><p>兼容性问题：反射依赖于类的具体结构，类结构发生变化可能会导致反射代码失效。</p><h1 id="6-反射的典型应用场景"><a href="#6-反射的典型应用场景" class="headerlink" title="6. 反射的典型应用场景"></a>6. 反射的典型应用场景</h1></li><li><p>框架开发：Spring 通过反射实现依赖注入和 AOP。</p></li><li><p>序列化与反序列化：JSON 库（如 Jackson）使用反射将对象转换为 JSON。</p></li><li><p>单元测试：可以测试类的私有方法。</p></li><li><p>注解处理：通过反射读取和处理注解。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Java】学习笔记</title>
      <link href="/2025/03/15/09_Java_learn/"/>
      <url>/2025/03/15/09_Java_learn/</url>
      
        <content type="html"><![CDATA[<ol><li><p>java基本数据类型是否相等使用”==“运算符；对象是否相等使用equals()方法，使用”==”是判断是否引用到同一个对象上。</p></li><li><p>实例变量（非静态成员变量） 和类变量（静态成员变量） 可以不赋初值，Java 会自动分配默认值；其他类型的变量（尤其是局部变量）必须显式赋值后才能使用。（实例变量是声明在类中、方法之外的非静态变量（不含static修饰），属于对象的属性。当对象被创建时，Java 会自动为实例变量赋予默认值。）</p></li><li><p>同一类所有的实例共享一份静态变量。</p></li><li><p>判断是修改了引用的对象内容还是只修改了方法内的局部引用：（1）查看是否有修改对象内容的方法调用，如果方法中调用了修改对象内容的方法（如append()、replace()、put()等），则可能是修改了对象内容；（2）检查是否直接重新赋值，如果方法中直接对参数重新赋值（如x &#x3D; new…或x &#x3D; other），则只是修改了方法内的局部引用。</p></li><li><p>当try和finally块都有return语句时，finally块中的return会覆盖try块中的return，这是因为finally块在方法返回前执行。</p></li><li><p>final不能修饰接口和抽象类。final修饰的方法不能被重写(Override)，但是可以被重载(Overload)。重写是子类对父类方法的覆盖，而重载是同一个类中方法名相同但参数不同。</p></li><li><p>在Java中，final变量(成员变量)必须在以下位置之一进行初始化：（1）声明时直接初始化；（2）在构造方法中初始化；（3）在初始化块中初始化</p></li><li><p>事务隔离级别是由数据库系统来实现的,这是因为数据库系统负责管理并发事务的执行以及事务之间的隔离性。数据库系统通过锁机制、多版本并发控制(MVCC)等技术来实现不同级别的事务隔离。</p></li><li><p>java中byte类型是转换成int来进行计算的，结果也是int型；int型范围大于byte型，int型数据不能自动转换成byte，故不能直接赋给byte，无法编译。（byte b &#x3D; 1；b&#x3D; b + b；）</p></li><li><p>构造方法的名称与类名相同，构造方法没有返回类型（包括 void）。</p></li><li><p>在Java中synchronized关键字可以用来实现线程同步,通过获取对象的互斥锁来保证同步代码块或同步方法在同一时刻只能被一个线程执行。</p></li><li><p>使用了Thread类创建线程，t.run()是普通方法调用，不会启动新线程，而是在当前主线程中执行，t.start()才会调用新线程。</p></li><li><p>字符串连接操作是从左到右进行的；如果加法运算没有括号，且左边是字符串，则后续的数字会被当作字符串处理；数字之间的加法运算会先执行；括号内的运算具有高优先级，会先计算。<br><img src="/images/09_1.png" alt="在这里插入图片描述"></p></li><li><p>JVM在判定两个class是否相同时，不仅要判断类名是否相同，还要判断加载这个类的类加载器是否相同。这就是”类的唯一性”原则，同一个类文件被不同的类加载器加载，在JVM中会被认为是不同的类。</p></li><li><p>Java的虚拟机确实不支持泛型，所有的泛型在编译阶段都会被擦除，变成普通的类和方法；在编译阶段，所有的泛型类型参数都会被擦除，替换为其边界类型（如果没有指定边界，则替换为Object）；在创建泛型对象时明确指定类型参数是一个很好的实践。这样编译器可以在编译阶段就进行类型检查，避免运行时出现类型转换异常；类型擦除是Java泛型的重要特性，但这并不意味着完全无法获取泛型的类型信息，通过反射机制，我们依然可以在运行时获取到泛型的实际类型参数。</p></li><li><p>volatile关键字的作用</p></li></ol><ul><li>保证内存可见性：当一个线程修改了volatile变量的值，会立即强制将修改后的值刷新到主内存；其他线程读取该变量时，会强制从主内存重新加载最新值，而不是使用工作内存中的缓存副本。</li><li>禁止指令重排序（确保代码执行顺序与预期一致）</li></ul><ol start="17"><li>volatile的局限性</li></ol><ul><li>不保证原子性：volatile只能保证可见性和有序性，但无法保证复合操作的原子性。例如，i++（包含 “读取 - 修改 - 写入” 三步）即使被volatile修饰，在多线程下仍可能出现数据不一致。</li><li>适用场景有限：通常用于状态标记（如示例中的flag）、双重检查锁定等简单场景，复杂的并发控制仍需依赖锁机制。</li></ul><ol start="18"><li>Java中的yield和sleep<br>在 Java 中，yield()和sleep()都是Thread类的静态方法，用于控制线程的执行状态，但它们的作用和行为有显著区别。</li></ol><ul><li>Thread.yield() 方法：让当前正在执行的线程主动放弃 CPU 资源，回到 “就绪状态”，允许其他具有相同优先级的线程获得执行机会。yield()只是向线程调度器发出 “自愿让出 CPU” 的建议，调度器可能忽略该建议（例如，没有其他同优先级线程等待时，当前线程可能立即重新获得 CPU）。无法指定让出 CPU 的时长，完全由调度器决定何时再次执行当前线程。</li><li>Thread.sleep(long millis) 方法：让当前线程暂停执行指定的时间（毫秒），进入 “阻塞状态”，期间不参与 CPU 调度。时间结束后，线程回到 “就绪状态”，等待调度器再次分配 CPU。</li></ul><ol start="19"><li>抽象类可以有构造函数。虽然抽象类不能被实例化，但它的构造函数可以被子类调用(通过super关键字)。</li><li>final 是 Java 中的修饰符，可用于修饰类、方法、变量，作用是限制其 “可变性”，具体行为如下：</li></ol><ul><li>被final修饰的类不能被继承（即无法有子类）</li><li>被final修饰的方法不能被子类重写（Override）</li><li>被final修饰的变量一旦赋值后就不能再修改（即 “常量”）：<ul><li>对于基本数据类型（如int、double）：final保证值不可变</li><li>对于引用数据类型（如对象、数组）：final保证引用地址不可变，但对象内部的属性可以改变</li></ul></li><li>final变量必须在声明时、构造方法中或初始化块中赋值，否则编译报错</li></ul><ol start="21"><li>枚举用于定义一组固定的常量（如季节、星期、状态等），本质是一个继承了java.lang.Enum的类</li></ol><ul><li>枚举的实例是预定义且唯一的，无法在运行时创建新实例</li><li>可包含属性和方法</li><li>枚举中的每个常量本质上都是该枚举类的一个对象，且是单例的（每个常量在内存中只有一个实例）</li></ul><ol start="22"><li>继承与重写案例一</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="comment">// A类的构造函数，打印&quot;A-&quot;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">A</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.print(<span class="string">&quot;A-&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// A类的print方法，打印methodA</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">print</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.print(<span class="string">&quot;methodA&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">B</span> <span class="keyword">extends</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="comment">// B类的构造函数，打印&quot;B-&quot;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">B</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.print(<span class="string">&quot;B-&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// B类的print方法，打印methodB</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">print</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>.print();</span><br><span class="line">        System.out.print(<span class="string">&quot;methodB&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">A</span> <span class="variable">a</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">B</span>();</span><br><span class="line">        a.print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码展示了Java中的继承和方法重写机制</p><ul><li><p>代码结构</p><ul><li>有三个类：A（父类）、B（子类）和Main（包含主方法）</li><li>类A和类B都包含构造函数和print方法</li><li>Main类中创建了B类的实例并调用了print方法</li></ul></li><li><p>实现原理</p><ul><li><p><strong>继承关系</strong>：类B通过<code>extends A</code>继承了类A，因此B类拥有A类的所有非private成员。</p></li><li><p><strong>构造函数执行顺序</strong>：</p><ul><li>当执行<code>new B()</code>时，首先会调用父类A的构造函数，打印”A-“<ul><li>然后调用子类B的构造函数，打印”B-“</li><li>所以创建B对象时，控制台会输出”A-B-“</li></ul></li></ul></li></ul></li><li><p>方法重写</p><ul><li>B类重写了A类的print方法</li><li>在B类的print方法中，首先通过<code>super.print()</code>调用了父类的print方法，打印”methodA”</li><li>然后打印”methodB”</li><li>所以调用print方法时，控制台会输出”methodAmethodB”</li></ul></li><li><p>执行结果<br>当执行<code>Main</code>类的main方法时，输出结果为：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A-B-methodAmethodB</span><br></pre></td></tr></table></figure><ul><li>注意事项<ul><li><p><strong>构造函数调用顺序</strong>：在Java中，创建子类对象时，会先调用父类的构造函数，然后再调用子类的构造函数。这是为了保证父类部分先被正确初始化。</p></li><li><p><strong>方法重写</strong>：子类可以重写父类的方法，但方法签名（方法名和参数列表）必须相同。重写的方法可以通过<code>super</code>关键字调用父类的版本。</p></li><li><p><strong>多态</strong>：虽然声明的是<code>A a = new B()</code>，但运行时会使用B类的print方法实现，这是Java多态的体现。</p></li><li><p><strong>访问权限</strong>：如果父类的方法是private的，则不会被继承，也就无法被重写。</p></li><li><p><strong>设计考虑</strong>：在这个例子中，子类B在重写print方法时先调用了父类的版本，这通常是一种良好的设计实践，可以保持父类功能的同时扩展子类功能。</p></li></ul></li></ul><ol start="23"><li>子类构造方法必须先调用父类构造方法（显式通过 super(参数) 或隐式默认调用 super()）；普通方法重写后，通过子类对象调用该方法时，会优先执行子类的重写实现（多态特性），父类的方法可以通过 super.方法名() 在子类中被调用。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【BLIP-2】Bootstrapping Language-Image Pre-training</title>
      <link href="/2024/09/22/24_%E3%80%90BLIP-2%E3%80%91Bootstrapping%20Language-Image%20Pre-training/"/>
      <url>/2024/09/22/24_%E3%80%90BLIP-2%E3%80%91Bootstrapping%20Language-Image%20Pre-training/</url>
      
        <content type="html"><![CDATA[<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>由于大规模模型的端到端训练，视觉和语言预训练的成本变得越来越高</li><li>解决方法<br>提出了一种通用且高效的预训练策略BLIP-2，通过类似于Adapter的Qformer将图像特征对齐到文本空间，并提出了两阶段的训练策略，第一阶段从固定图像编码器中引导视觉语言表示学习，第二阶段从一个固定的语言模型中引导视觉到语言的生成学习。</li><li>注意问题和细节<br>（1）query有多个，选哪个token算ITC loss和ITM Loss：由于Z包含多个输出嵌入(每个query嵌入一个)，首先计算每个query输出与t之间的成对相似度，然后选择最高的一个作为图像-文本相似度；将每个输出查询嵌入到一个两类线性分类器中以获得logit，并将所有查询的logit平均值作为输出匹配分数<br>（2）和BLIP的区别：BLIP的目标是和原始CLIP相同的，让图像和文本特征对齐，BLIP-2则是让图像特征往文本方向对齐，这是因为文本方面有基于更大的数据训练的大模型，去利用这个预训练对多模态是有益的<br>（3）这个工作的意义：架构通用且高效，且论文中提到LLM越大，性能还能进一步提升<br>（3）Qformer的三种mask：第一种是Q和T是完全互通的，用于计算ITM；第二种是Q只能看Q，T能看T还能看Q，用于算LM；第三种是Q只能看Q，T只能看T，用于算ITC。</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>由于大规模模型的端到端训练，视觉和语言预训练的成本变得越来越高。本文提出了一种通用且高效的预训练策略BLIP-2，它从现成的冻结预训练图像编码器和冻结的大型语言模型中引导视觉语言预训练。BLIP-2使用轻量级Querying Transformer弥补了模式上的差距，该Transformer分两个阶段进行预训练。第一阶段从固定图像编码器中引导视觉语言表示学习。第二阶段从一个固定的语言模型中引导视觉到语言的生成学习。尽管比现有方法具有更少的可训练参数，但BLIP-2在各种视觉语言任务上实现了最先进的性能。例如，我们的模型在零射击VQAv2上的表现比Flamingo80B高出8.7%，可训练参数减少了54倍。我们还演示了该模型可以遵循自然语言指令的zero-shot图像到文本生成功能。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>视觉语言预训练(VLP)研究在过去几年中取得了快速进展，开发了规模越来越大的预训练模型，不断将最先进的技术推向各种下游任务(Radford et al.,2021;Li et al.,2021;2022;Wang et al.,2022a;Alayrac等,2022;Wang et al.,2022b)。然而，由于使用大规模模型和数据集进行端到端训练，大多数最先进的视觉语言模型在预训练过程中会产生很高的计算成本。</p><p>视觉语言研究处于视觉和语言的交叉点，因此人们自然期望视觉语言模型能够从视觉和自然语言社区中现成的单模态模型中收获。在本文中，我们提出了一种通用且计算效率高的VLP方法，该方法从现成的预训练视觉模型和语言模型中进行引导。预训练的视觉模型提供高质量的视觉表现。预训练的语言模型，特别是大型语言模型(LLM)，提供了强大的语言生成和zero-shot能力。为了减少计算成本和解决灾难性遗忘问题，单模态预训练模型在预训练过程中保持冻结状态。</p><p>为了利用VLP的预训练单模态模型，促进跨模态对齐是关键。然而，由于LLM在单模态预训练期间没有看到图像，因此冻结图像使得视觉语言对齐特别具有挑战性。在这方面，现有的方法(例如Frozen (Tsimpoukelli等人,2021)，Flamingo (Alayrac等人,2022))采用图像到文本的生成损失，我们认为这不足以弥合模态差距。</p><p>为了实现与冻结单峰模型的有效视觉语言对齐，我们提出了一个使用新的两阶段预训练策略进行预训练的Querying Transformer(QFormer)。如图1所示，Q-Former是一个轻量级的转换器，它使用一组可学习的查询向量从冻结的图像编码器中提取视觉特征。它充当冻结图像编码器和冻结LLM之间的信息瓶颈，为LLM输出所需文本提供最有用的视觉特征。在第一个预训练阶段，我们执行视觉语言表征学习，强制Q-Former学习与文本最相关的视觉表征。在第二个预训练阶段，我们通过将Q-Former的输出连接到一个冻结的LLM来执行视觉到语言的生成学习，并训练Q-Former，使其输出的视觉表示可以被LLM解释。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_9.png)</p><p>我们将我们的VLP框架命名为BLIP-2:使用冻结单模态模型的Bootstrapping Language-Image预训练。BLIP-2的主要优势包括:</p><ul><li>BLIP-2有效地利用了冻结的预训练图像模型和语言模型。我们使用在两个阶段预训练的Q-Former来弥合模态差距:表征学习阶段和生成学习阶段。BLIP-2在各种视觉语言任务上实现了最先进的性能，包括视觉问答、图像字幕和图像文本检索。</li><li>在LLM(例如OPT (Zhang et al.,2022)， FlanT5 (Chung et al.,2022))的支持下，BLIP-2可以按照自然语言指令执行zero-sho图像到文本的生成，从而实现诸如视觉知识推理，视觉对话等新兴功能(参见图4示例)。</li><li>由于使用了冷冻单峰模型和轻型Q-Former, BLIP-2比现有的最先进的计算效率更高。例如，在零射击VQAv2上，BLIP-2比Flamingo (Alayrac et al.,2022)的性能高出8.7%，同时使用的可训练参数减少了54%。此外，我们的研究结果表明，BLIP-2是一种通用方法，可以获得更先进的单峰模型，以获得更好的VLP性能。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h3 id="2-1-End-to-end-Vision-Language-Pre-training"><a href="#2-1-End-to-end-Vision-Language-Pre-training" class="headerlink" title="2.1 End-to-end Vision-Language Pre-training"></a>2.1 End-to-end Vision-Language Pre-training</h3><p>视觉语言预训练旨在学习多模态基础模型，提高其在各种视觉语言任务中的表现。根据下游任务的不同，提出了不同的模型架构，包括双编码器架构(Radford et al.,2021;Jia等人,2021)，融合编码器架构(Tan &amp; Bansal,2019;Li et al.,2021)，编码器-解码器架构(Cho et al.,2021;Wang et al.,2021b;Chen et al.,2022b)，以及最近的统一变压器架构(Li et al.,2022;Wang et al.,2022b)。多年来，各种预训练目标也被提出，并逐渐收敛为几个经过时间考验的目标:图像-文本对比学习(Radford et al.,2021;Yao et al.,2022;Li et al.,2021;2022)，图像-文本匹配(Li et al.,2021;2022;Wang et al.,2021a)和(掩码)语言建模(Li et al.,2021;2022;Yu et al.,2022;Wang et al.,2022b)。大多数VLP方法使用大规模图像-文本对数据集执行端到端预训练。随着模型规模的不断增大，预训练的计算成本会非常高。此外，端到端预训练模型不灵活，无法利用现成的单模预训练模型，如LLM(Brown等人,2020;Zhang等人,2022;Chung et al.,2022)。</p><h3 id="2-2-Modular-Vision-Language-Pre-training"><a href="#2-2-Modular-Vision-Language-Pre-training" class="headerlink" title="2.2 Modular Vision-Language Pre-training"></a>2.2 Modular Vision-Language Pre-training</h3><p>与我们更相似的是利用现成的预训练模型并在VLP期间保持冻结的方法。一些方法冻结了图像编码器，包括早期的工作采用冻结对象检测器提取视觉特征(Chen et al.,2020;Li et al.,2020;Zhang等人,2021)，以及最近使用冻结预训练图像编码器进行CLIP预训练的LiT (Zhai等人，2022)(Radford等人,2021)。一些方法冻结语言模型，将LLM的知识用于视觉到语言的生成任务(Tsimpoukelli等人，2021;Alayrac等，2022;Chen et al.,2022a;Tiong et al.,2022;郭等人,2022)。使用冻结LLM的关键挑战是将视觉特征与文本空间对齐。为了实现这一点，Frozen (Tsimpoukelli等人，2021)对图像编码器进行了微调，其输出直接用作LLM的软提示。Flamingo (Alayrac et al.,2022)将新的交叉注意层插入到LLM中以注入视觉特征，并在数十亿个图像-文本对上对新层进行预训练。这两种方法都采用了语言建模损失，语言模型根据图像条件生成文本。</p><p>与现有方法不同，BLIP-2可以有效地利用冻结图像编码器和冻结LLM来完成各种视觉语言任务，以更低的计算成本获得更强的性能。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><p>我们提出了一种新的视觉语言预训练方法BLIP-2，它从冻结的预训练单峰模型中启动。为了弥合模态差距，我们提出了一个分两个阶段预训练的查询转换器(Q-Former):(1)使用冻结图像编码器的视觉语言表示学习阶段和(2)使用冻结LLM的视觉到语言生成学习阶段。本节首先介绍Q-Former的模型架构，然后描述两阶段的预训练过程。</p><h3 id="3-1-Model-Architecture"><a href="#3-1-Model-Architecture" class="headerlink" title="3.1 Model Architecture"></a>3.1 Model Architecture</h3><p>我们提出Q-Former作为可训练模块，以弥合冻结图像编码器和冻结LLM之间的差距。它从图像编码器中提取固定数量的输出特征，与输入图像分辨率无关。如图2所示，Q-Former由两个transformer子模块组成，它们共享相同的自关注层:(1)与冻结图像编码器交互以提取视觉特征的图像转换器，(2)既可以作为文本编码器又可以作为文本解码器的文本transformer。我们创建了一组可学习的query嵌入作为图像转换器的输入。query通过自关注层交互，并通过交叉关注层(每隔一个transformer块插入)与冻结的图像特征交互。query还可以通过相同的自关注层与文本交互。根据预训练任务的不同，我们应用不同的自注意掩码来控制query-text交互。我们使用BERTbase的预训练权重初始化QFormer (Devlin et al.,2019)，而交叉注意层是随机初始化的。Q-Former共包含188M个参数。注意，query被视为模型参数。</p><p>在我们的实验中，我们使用32个query，其中每个查询的维度为768(与Q-Former的隐藏维度相同)。我们使用Z表示输出查询表示。Z的大小(32 × 768)远远小于冻结图像特征的大小(例如viti - l &#x2F;14的257 × 1024)。这个瓶颈架构与我们的预训练目标一起工作，迫使查询提取与文本最相关的视觉信息。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_3.png)</p><h3 id="3-2-Bootstrap-Vision-Language-Representation-Learning-from-a-Frozen-Image-Encoder"><a href="#3-2-Bootstrap-Vision-Language-Representation-Learning-from-a-Frozen-Image-Encoder" class="headerlink" title="3.2 Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder"></a>3.2 Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder</h3><p>在表示学习阶段，我们将Q-Former连接到一个固定的图像编码器，并使用图像-文本对进行预训练。我们的目标是训练Q-Former，这样查询就可以学习提取文本中信息量最大的视觉表示。受BLIP (Li et al.,2022)的启发，我们共同优化了三个共享相同输入格式和模型参数的预训练目标。每个目标在查询和文本之间使用不同的注意力屏蔽策略来控制它们的交互(参见图2)。</p><p>图像-文本对比学习(ITC)学习对齐图像表示和文本表示，使它们的相互信息最大化。它通过对比正对和负对的图像-文本相似性来实现这一目标。我们将来自图像transformer的输出查询表示Z与来自文本transformer的文本表示t对齐，其中t是[CLS]令牌的输出嵌入。**由于Z包含多个输出嵌入(每个查询嵌入一个)，我们首先计算每个查询输出与t之间的成对相似度，然后选择最高的一个作为图像-文本相似度。**为了避免信息泄露，我们使用了单模态自关注掩码，其中查询和文本不允许相互看到。由于使用了冻结图像编码器，与端到端方法相比，我们可以在每个GPU上拟合更多的样本。因此，我们使用in-batch negatives而不是在BLIP中的动量队列。</p><p>基于图像的文本生成(ITG)损失训练Q-Former在给定输入图像的条件下生成文本。由于Q-Former的体系结构不允许冻结图像编码器和文本标记之间的直接交互，因此生成文本所需的信息必须首先由查询提取，然后通过自关注层传递给文本标记。因此，查询被迫提取视觉特征来捕获关于文本的所有信息。我们使用多模态因果自注意掩码来控制查询-文本交互，类似于UniLM中使用的掩码(Dong et al.， 2019)。查询可以相互处理，但不能处理文本令牌。每个文本令牌都可以处理所有查询及其前一个文本令牌。我们还用一个新的[DEC]令牌替换[CLS]令牌，作为第一个文本令牌，向解码任务发出信号。</p><p>图像-文本匹配(ITM)旨在学习图像和文本表示之间的细粒度对齐。这是一个二元分类任务，其中要求模型预测图像-文本对是正的(匹配)还是负的(不匹配)。我们使用双向自关注掩码，所有查询和文本都可以相互关注。因此，输出查询嵌入Z捕获多模态信息。<strong>我们将每个输出查询嵌入到一个两类线性分类器中以获得logit，并将所有查询的logit平均值作为输出匹配分数</strong>。我们采用Li等人(2021;2022)来创建信息丰富的负对。</p><h3 id="3-3-Bootstrap-Vision-to-Language-Generative-Learning-from-a-Frozen-LLM"><a href="#3-3-Bootstrap-Vision-to-Language-Generative-Learning-from-a-Frozen-LLM" class="headerlink" title="3.3 Bootstrap Vision-to-Language Generative Learning from a Frozen LLM"></a>3.3 Bootstrap Vision-to-Language Generative Learning from a Frozen LLM</h3><p>在生成预训练阶段，我们将QFormer(附带冻结图像编码器)连接到冻结的LLM，以获取LLM的生成语言能力。如图3所示，我们使用一个全连接(FC)层将输出查询嵌入Z线性投影到与LLM的文本嵌入相同的维度中。然后将投影的query嵌入添加到输入文本嵌入中。它们作为软视觉提示，对由Q-Former提取的视觉表示进行条件约束。由于Q-Former已被预先训练以提取语言信息的视觉表示，因此它有效地充当信息瓶颈，为LLM提供最有用的信息，同时删除无关的视觉信息。这减少了LLM学习视觉语言一致性的负担，从而减轻了灾难性遗忘问题。</p><p>我们实验了两种类型的LLM:基于解码器的LLM和基于编码器-解码器的LLM。对于基于解码器的LLM，我们使用语言建模损失进行预训练，其中冻结的LLM的任务是根据Q-Former的视觉表示生成文本。对于基于编码器-解码器的LLM，我们使用前缀语言建模损失进行预训练，其中我们将文本分成两部分。前缀文本与可视化表示相连接，作为LLM编码器的输入。后缀文本用作LLM解码器的生成目标。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_6.png)</p><h3 id="3-4-Model-Pre-training"><a href="#3-4-Model-Pre-training" class="headerlink" title="3.4 Model Pre-training"></a>3.4 Model Pre-training</h3><p>训练的数据。我们使用与BLIP相同的预训练数据集，总共有1.29亿张图像，包括COCO (Lin等人，2014)、Visual Genome (Krishna等人，2017)、CC3M (Sharma等人，2018)、CC12M (Changpinyo等人，2021)、SBU (Ordonez等人，2011)和来自LAION400M数据集的1.15亿张图像(Schuhmann等人，2021)。我们采用CapFilt方法(Li et al.， 2022)为web图像创建合成字幕。具体来说，我们使用blipllarge字幕模型生成了10个字幕，并根据CLIP vitl &#x2F;14模型产生的图像-文本相似性将合成字幕与原始网页字幕进行排序。我们将每张图像的前两个标题作为训练数据，并在每个预训练步骤随机抽取一个。</p><p>预训练图像编码器和LLM。对于冻结图像编码器，我们探索了两种最先进的预训练视觉转换器模型:(1)来自CLIP的ViT-L&#x2F;14 (Radford等人，2021)和(2)来自EVA-CLIP的ViT-G&#x2F;14 (Fang等人，2022)。我们删除了ViT的最后一层，并使用最后一层的输出特征，这导致性能稍微好一些。对于冻结语言模型，我们探索了基于解码器的非监督训练的OPT模型族(Zhang et al.,2022)和基于编码器-解码器的FlanT5模型族(Chung et al., 2022)。</p><p>训练的设置。我们在第一阶段预训练250k步，在第二阶段预训练80k步。我们在第一阶段使用2320&#x2F;1680批量生产ViT-L &#x2F; ViT-G，在第二阶段使用1920&#x2F;1520批量生产OPT&#x2F;FlanT5。在预训练期间，我们将冻结的ViTs和LLM参数转换为FP16，除了FlanT5，我们使用BFloat16。与使用32位模型相比，我们没有发现性能下降。由于使用了冻结模型，我们的预训练比现有的大规模VLP方法更具计算友好性。例如，使用一台16-A100(40G)的机器，我们最大的型号使用ViT-G和FlanT5-XXL，第一阶段需要不到6天，第二阶段需要不到3天。</p><p>所有模型都使用相同的预训练超参数集。我们使用AdamW (Loshchilov &amp; Hutter, 2017)优化器，其中β1 &#x3D; 0.9， β1 &#x3D; 0.98，权重衰减为0.05。我们使用余弦学习率衰减，峰值学习率为1e-4，线性热身为2k步。第二阶段的最低学习率为5e-5。我们使用大小为224×224的图像，增强了随机调整大小的裁剪和水平翻转。</p><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><p>表1概述了BLIP-2在各种零射击视觉语言任务中的表现。与之前最先进的模型相比，BLIP-2在视觉语言预训练过程中需要的可训练参数数量大大减少，从而提高了性能。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_1.png)</p><h3 id="4-1-Instructed-Zero-shot-Image-to-Text-Generation"><a href="#4-1-Instructed-Zero-shot-Image-to-Text-Generation" class="headerlink" title="4.1 Instructed Zero-shot Image-to-Text Generation"></a>4.1 Instructed Zero-shot Image-to-Text Generation</h3><p>BLIP-2有效地使LLM能够理解图像，同时保留其在以下文本提示中的能力，这使我们能够用指令控制图像到文本的生成。我们只需将文本提示附加在可视提示之后，作为LLM的输入。图4展示了一些示例，这些示例展示了广泛的零图像到文本功能，包括视觉知识推理、视觉常识推理、视觉对话、个性化图像到文本生成等。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_4.png)</p><p>Zero-shot VQA。我们对zero-shot视觉问答任务进行定量评价。对于OPT模型，我们使用提示符“Question: {} Answer:”。对于FlanT5模型，我们使用提示符“Question: {} Short answer:”。在生成过程中，我们使用波束搜索，波束宽度为5。我们还将长度惩罚设置为-1，以鼓励更短的答案，更符合人类注释。</p><p>如表2所示。BLIP-2在VQAv2 (Goyal等人,2017)和GQA (Hudson &amp; Manning, 2019)数据集上取得了最先进的结果。它在VQAv2上的性能比Flamingo80B高出8.7%，尽管可训练参数减少了54倍。在OK-VQA (Marino et al.,2019)数据集上，BLIP-2仅次于Flamingo80B。我们假设这是因为OK-VQA更关注开放世界知识而不是视觉理解，并且来自Flamingo80B的70B Chinchilla (Hoffmann et al.,2022)语言模型比11B FlanT5xxL拥有更多的知识。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_7.png)</p><p>我们从表2中得出了一个有希望的观察结果:更强的图像编码器或更强的LLM都能带来更好的性能。这一观察结果得到了几个事实的支持:(1ViT-G在OPT和FlanT5上都优于ViT-L。(2)在同一LLM家族中，较大的模型优于较小的模型。(3)指令调优LLM FlanT5在VQA上优于无监督训练的OPT。这一观察结果验证了BLIP-2作为一种通用的视觉语言预训练方法，可以有效地收获视觉和自然语言社区的快速进展。</p><p>视觉语言表征学习的效果。第一阶段表征学习预训练QFormer学习与文本相关的视觉特征，减少了LLM学习视觉语言对齐的负担。没有表征学习阶段，Q-Former仅依靠视觉到语言的生成学习来弥合模态差距，类似于Flamingo中的percepver Resampler。图5展示了表征学习对生成学习的影响。没有表示学习，这两种llm在零射击VQA上的性能都明显较低。特别是，OPT患有灾难性遗忘，随着训练的进行，表现会急剧下降。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_10.png)</p><h3 id="4-2-Image-Captioning"><a href="#4-2-Image-Captioning" class="headerlink" title="4.2 Image Captioning"></a>4.2 Image Captioning</h3><p>我们为图像字幕任务调整了BLIP-2模型，该任务要求模型为图像的视觉内容生成文本描述。我们使用提示“a photo of”作为LLM的初始输入，并训练模型生成具有语言建模损失的标题。我们在微调过程中保持LLM冻结，并与图像编码器一起更新Q-Former的参数。我们用ViT-G和各种LLM进行实验。详细的超参数可以在附录中找到。我们对COCO进行微调，并对COCO测试集和零射击转移到NoCaps (Agrawal et al.,2019)验证集进行评估。</p><p>结果如表3所示。BLIP-2实现了最先进的性能，在NoCaps方面比现有方法有了显著的改进，对域外图像表现出较强的泛化能力。<br>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_2.png)</p><h3 id="4-3-Visual-Question-Answering"><a href="#4-3-Visual-Question-Answering" class="headerlink" title="4.3 Visual Question Answering"></a>4.3 Visual Question Answering</h3><p>给定带注释的VQA数据，我们在保持LLM冻结的同时微调Q-Former和图像编码器的参数。我们使用开放式答案生成损失进行微调，其中LLM接收Q-Former的输出和问题作为输入，并被要求生成答案。为了提取与问题更相关的图像特征，我们在问题上附加Q-Former条件。具体来说，问题标记作为Q-Former的输入，并通过自关注层与查询交互，这可以引导Q-Former的交叉关注层关注更多信息的图像区域。</p><p>在BLIP之后，我们的VQA数据包括来自VQAv2的训练和验证分割，以及来自Visual Genome的训练样本。表4展示了开放式代模型中最先进的BLIP-2结果。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_11.png)</p><h3 id="4-4-Image-Text-Retrieval"><a href="#4-4-Image-Text-Retrieval" class="headerlink" title="4.4 Image-Text Retrieval"></a>4.4 Image-Text Retrieval</h3><p>由于图像-文本检索不涉及语言生成，因此我们直接对第一阶段预训练模型进行微调。具体来说，我们在COCO上使用相同的目标(即ITC, ITM和ITG)对图像编码器和Q-Former进行微调。然后，我们在COCO和Flickr30K (Plummer et al.,2015)数据集上评估图像到文本检索和文本到图像检索模型。在推理过程中，我们遵循Li等人(2021;2022)，首先根据图像文本特征相似度选择k &#x3D; 128个候选对象，然后根据成对的ITM分数重新排序。我们用vitl和vitg作为图像编码器进行了实验。详细的超参数可以在附录中找到。</p><p>结果如表5所示。BLIP-2实现了最先进的性能，比现有的零拍摄图像文本检索方法有了显著的改进。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_5.png)</p><p>ITC和ITM损失对于图像-文本检索至关重要，因为它们直接学习图像-文本相似度。在表6中，我们展示了ITG(基于图像的文本生成)损失也有利于图像文本检索。这个结果支持我们在设计表示学习目标时的直觉:ITG损失强制查询提取与文本最相关的视觉特征，从而提高视觉语言一致性。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【BLIP-2】Bootstrapping Language-Image Pre-training&#x2F;img_8.png)</p><h2 id="5-Limitation"><a href="#5-Limitation" class="headerlink" title="5. Limitation"></a>5. Limitation</h2><p>最近的LLM可以在给定几个例子的情况下进行上下文学习。然而，我们对BLIP-2的实验在为LLM提供上下文中的VQA示例时并没有观察到VQA性能的改善。我们将缺乏上下文学习能力归因于我们的预训练数据集，该数据集每个样本只包含单个图像-文本对。LLM不能从中学习单个序列中多个图像-文本对之间的相关性。在Flamingo的论文中也报告了相同的观察结果，该论文使用了一个封闭来源的交错图像和文本数据集(M3W)，每个序列有多个图像-文本对。我们的目标是在未来的工作中创建一个类似的数据集。</p><p>由于各种原因，包括来自LLM的不准确的知识，激活不正确的推理路径，或者没有关于新图像内容的最新信息，BLIP-2的图像到文本生成可能会产生不满意的结果(见图7)。此外，由于使用冻结模型，BLIP-2继承了LLM的风险，例如输出攻击性语言，传播社会偏见，或泄露私人信息。补救方法包括使用指令来指导模型的生成或在删除有害内容的过滤数据集上进行训练。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>我们提出了一种通用的、计算效率高的视觉语言预训练方法BLIP-2，它利用了冷冻预训练图像编码器和LLM。在预训练过程中，BLIP-2在具有少量可训练参数的情况下，在各种视觉语言任务上实现了最先进的性能。BLIP-2还展示了零射击指示图像到文本生成的新兴功能。我们认为BLIP-2是构建多模态会话AI代理的重要一步。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【SAM】Segment Anything</title>
      <link href="/2024/09/22/25_%E3%80%90SAM%E3%80%91Segment%20Anything/"/>
      <url>/2024/09/22/25_%E3%80%90SAM%E3%80%91Segment%20Anything/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything<br>2023</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>分割任务上缺乏可以做zero shot的基础模型</li><li>解决方法<br>（1）提出了一个新任务：promot segmentation任务，通过promot的方式使得网络可以实现zero shot泛化<br>（2）模型架构：图像编码器用的是ViT，文本编码器用的是CLIP的文本编码器，点和框的嵌入方法是转换成位置编码，掩码的嵌入方式是卷积<br>（3）数据集：人工标注+模型生成</li><li>注意问题和细节<br>（1）点和框的输入：首先会进行坐标上的缩放，然后将坐标转成位置编码，再加上可学习参数得到坐标的embedding<br>（2）mask的处理：通过卷积得到embedding<br>![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_2.png)</li></ul><p>（3）解码：mask的embedding直接加到图像上，坐标的embedding和iou token、mask token作为query<br>![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_3.png)</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们介绍了Segment Anything(SA)项目：一个新的任务，模型和数据集的图像分割。在数据收集循环中使用我们的高效模型，我们建立了迄今为止(到目前为止)最大的分割数据集，在1100万张许可和尊重隐私的图像上拥有超过10亿个掩模。该模型被设计和训练为提示，因此它可以zero shot转移到新的图像分布和任务。我们评估了它在许多任务中的能力，发现它的zero shot性能令人印象深刻-通常与之前的完全监督结果相竞争甚至优于。我们正在<a href="https://segment-anything.com上发布segment/">https://segment-anything.com上发布Segment</a> Anything Model(SAM)和对应的1B个掩模和11M张图像的数据集(SA-1B)，以促进对计算机视觉基础模型的研究。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在网络规模的数据集上预训练的大型语言模型具有强大的zero shot和few shot泛化能力，正在彻底改变NLP[10]。这些“基础模型”[8]可以推广到训练期间看到的任务和数据分布之外的任务和数据分布。此功能通常通过提示工程实现，其中使用手工编写的文本来提示语言模型为手头的任务生成有效的文本响应。当使用来自网络的大量文本语料库进行缩放和训练时，这些模型的零和少镜头性能与微调模型(在某些情况下甚至匹配)相比惊人地好[10,21]。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算量的增加而改善[56,10,21,51]。</p><p>基础模型也在计算机视觉中进行了探索，尽管程度较低。也许最突出的示例是对齐来自网络的配对文本和图像。例如，CLIP[82]和ALIGN[55]使用对比学习来训练对齐两种模式的文本和图像编码器。经过训练后，工程文本提示可以对新的视觉概念和数据分布进行zero shot泛化。这种编码器还可以与其他模块有效组合，以实现下游任务，例如图像生成(例如，DALL-E[83])。虽然在视觉和语言编码器方面已经取得了很大的进展，但计算机视觉包括超出这个范围的广泛问题，并且对于其中的许多问题，没有丰富的训练数据。</p><p>在这项工作中，我们的目标是建立一个图像分割的基础模型。也就是说，我们寻求开发一个提示模型，并使用一个能够实现强大泛化的任务在广泛的数据集上对其进行预训练。有了这个模型，我们的目标是使用即时工程解决新数据分布上的一系列下游分割问题。</p><p>该计划的成功取决于三个组成部分:任务、模型和数据。为了开发它们，我们解决了以下关于图像分割的问题:</p><p>(1) 什么<strong>任务</strong>可以实现zero shot泛化?<br>(2) 相应的<strong>模型</strong>体系结构是什么?<br>(3) 哪些<strong>数据</strong>可以为这项任务和模型提供支持?</p><p>这些问题纠缠在一起，需要综合解决。我们首先定义一个提示的分割任务，它足够通用，可以提供一个强大的预训练目标，并支持广泛的下游应用程序。此任务需要一个支持灵活提示的模型，并且可以在提示时实时输出分段掩码以允许交互使用。为了训练我们的模型，我们需要一个多样化的、大规模的数据源。不幸的是，没有网络规模的数据来源的分割;为了解决这个问题，我们构建了一个“数据引擎”，也就是说，我们在使用我们的高效模型来协助数据收集和使用新收集的数据来改进模型之间进行迭代。接下来我们介绍每个相互关联的组件，然后是我们创建的数据集和证明我们方法有效性的实验。</p><p><strong>任务</strong>。在NLP和最近的计算机视觉中，基础模型是一个很有前途的发展，它可以通过使用“提示”技术对新的数据集和任务执行零射击和少射击学习。受这一行工作的启发，我们提出了<strong>提示分割任务</strong>，其目标是在给定任何分割提示的情况下返回一个有效的分割掩码(见图la)。提示符只是指定图像中要分割的内容，例如，提示符可以包括识别对象的空间或文本信息。有效输出掩码的要求意味着，即使提示是模糊的，并且可能引用多个对象(例如，衬衫上的一个点可能表示衬衫或穿着它的人)，输出也应该是这些对象中至少一个对象的合理掩码。我们使用提示分割任务作为预训练目标，并通过提示工程解决一般的下游分割任务。</p><p><strong>模型</strong>。可提示的分割任务和实际使用的目标对模型体系结构施加了约束。特别是，模型必须支持灵活的提示，需要在平摊实时中计算掩码以允许交互使用，并且必须具有歧义意识。令人惊讶的是，我们发现一个简单的设计满足所有三个约束:一个强大的<strong>图像编码器</strong>计算图像嵌入，一个<strong>提示编码器</strong>嵌入提示，然后将两个信息源组合在一个轻量级的<strong>掩码解码器</strong>中，该解码器预测分割掩码。我们将此模型称为SAM(Segment Anything model，简称SAM)(见图1b)。通过将SAM分为图像编码器和快速提示编码器&#x2F;掩码解码器，可以使用不同的提示重复使用相同的图像嵌入(并平摊其成本)。给定图像嵌入，提示编码器和掩码解码器在web浏览器中约50ms内从提示预测掩码。我们将重点放在点、框和掩码提示上，并使用自由格式的文本提示来呈现初始结果。为了使SAM能够感知歧义，我们将其设计为预测单个提示的多个掩码，从而允许SAM自然地处理歧义，例如衬衫与人的例子。<br>![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_4.png)</p><p><strong>数据引擎</strong>。为了实现对新数据分布的强泛化，我们发现有必要在一个大而多样的掩码集上训练SAM，而不仅仅是已经存在的任何分割数据集。虽然基础模型的典型方法是在线获取数据[82]，但掩模并不自然丰富，因此我们需要一种替代策略。我们的解决方案是建立一个“数据引擎”，也就是说，我们与模型在循环数据集注释共同开发我们的模型(见图1c)。我们的数据引擎有三个阶段:辅助手动、半自动和全自动。在第一阶段，SAM帮助注释者注释掩码，类似于经典的交互式分段设置。在第二阶段，SAM可以通过提示可能的对象位置来自动为对象子集生成掩码，而注释器则专注于注释剩余的对象，从而帮助增加掩码的多样性。在最后阶段，我们用前景点的规则网格提示SAM，平均每张图像产生约100个高质量掩模。</p><p>数据集。我们的最终数据集SA-1B包括来自11M张授权和隐私保护图像的超过1B个掩码(见图2)。SA-1B是使用我们的数据引擎的最后阶段完全自动收集的，其掩码比任何现有的分割数据集都多400倍[66,44,117,60]，并且经过我们的广泛验证，掩码具有高质量和多样性。除了将SA-1B用于训练SAM具有鲁棒性和通用性之外，我们希望SA-1B成为旨在建立新基础模型的研究的宝贵资源。</p><p>负责任的人工智能。我们研究并报告了使用SA-1B和SAM时潜在的公平性问题和偏见。SA-1B中的图像跨越了地理和经济上不同的国家，我们发现SAM在不同人群中的表现相似。总之，我们希望这将使我们的工作对现实世界的用例更加公平。我们在附录中提供了模型和数据集卡。</p><p>实验。我们广泛评估SAM。首先，使用23个不同的新分割数据集，我们发现SAM从单个前景点产生高质量的掩模，通常仅略低于手动注释的地面真值。其次，我们在zero shot协议下使用即时工程的各种下游任务上发现了一致的强定量和定性结果，包括边缘检测，目标提案生成，实例分割以及文本到掩码预测的初步探索。这些结果表明，SAM可以使用开箱即用的快速工程来解决涉及SAM训练数据之外的对象和图像分布的各种任务。然而，改进的空间仍然存在，正如我们在第八章中讨论的那样。</p><h2 id="2-Segment-Anything-Task"><a href="#2-Segment-Anything-Task" class="headerlink" title="2. Segment Anything Task"></a>2. Segment Anything Task</h2><p>我们从NLP中获得灵感，其中下一个令牌预测任务用于基础模型预训练，并通过提示工程解决各种下游任务[10]。为了建立分割的基础模型，我们的目标是定义一个具有类似功能的任务。</p><p>任务。我们首先将提示的概念从NLP翻译到分割，其中提示可以是一组前景&#x2F;背景点，一个粗略的框或蒙版，自由格式的文本，或者一般情况下，任何指示图像中要分割的信息。因此，提示分段任务是在给定任何提示的情况下返回一个有效的分段掩码。“有效”掩码的要求仅仅意味着，即使提示符是模糊的，并且可以引用多个对象(例如:，回想一下衬衫与人的例子，并参见图3)，输出应该是这些对象中至少一个的合理掩码。这个需求类似于期望语言模型对不明确的提示输出一致的响应。我们之所以选择这个任务，是因为它带来了一种自然的预训练算法和一种通过提示将zero shot转移到下游分割任务的通用方法。</p><p>训练。提示分割任务提出了一种自然的预训练算法，该算法为每个训练样本模拟一系列提示(例如，点、框、掩码)，并将模型的掩码预测与基本事实进行比较。我们从交互式分割中采用了这种方法[109,70]，尽管与交互式分割不同，交互式分割的目的是在足够的用户输入后最终预测一个有效的掩码，但我们的目标是始终预测任何提示的有效掩码，即使提示是模糊的。这确保了预训练模型在涉及歧义的用例中是有效的，包括我们的数据引擎所要求的自动注释。我们注意到，在这项任务中表现良好是具有挑战性的，需要专门的建模和训练损失选择，我们将在第3节中讨论。</p><p>Zero-shot。直观地说，我们的预训练任务赋予了模型在推理时对任何提示作出适当响应的能力，因此下游任务可以通过设计适当的提示来解决。例如，如果有一个猫的边界框检测器，猫实例分割可以通过提供检测器的框输出作为提示给我们的模型来解决。一般来说，许多实际的分割任务都可以作为提示。除了自动数据集标记之外，我们在第7章的实验中探索了五个不同的示例任务。</p><p>相关的任务。分割是一个很广阔的领域，有交互式分割[57,109]、边缘检测[3]、超像素化[85]、目标建议生成[2]、前景分割[94]、语义分割[90]、实例分割[66]、全视分割[59]等。我们的提示分割任务的目标是产生一个广泛的功能模型，可以通过提示工程适应许多(尽管不是全部)现有的和新的分割任务。这种能力是任务泛化的一种形式[26]。请注意，这与之前在多任务分割系统上的工作不同。在多任务系统中，单个模型执行一组固定的任务，例如联合语义分割、实例分割和全视分割[114,19,54]，但训练和测试任务是相同的。我们工作中的一个重要区别是，训练用于提示分割的模型可以作为更大系统中的组件在推理时间执行新的不同任务，例如，执行实例分割，提示分割模型与现有的对象检测器相结合。</p><p>讨论。提示和组合是功能强大的工具，可以以可扩展的方式使用单个模型，从而潜在地完成模型设计时未知的任务。这种方法类似于其他基础模型的使用方式，例如CLIP[82]是DALL-E[83]图像生成系统的文本-图像对齐组件。我们预计，由诸如快速工程等技术提供支持的可组合系统设计，将比专门针对固定任务集进行训练的系统能够实现更广泛的应用。通过组合的镜头来比较提示式和交互式分割也是很有趣的:虽然交互式分割模型是为人类用户设计的，但正如我们将演示的那样，为提示式分割训练的模型也可以组成一个更大的算法系统。</p><p>![在这里插入图片描述](.&#x2F;2025-09-22-【SAM】Segment Anything&#x2F;img_1.png)</p><h2 id="3-Segment-Anything-Model"><a href="#3-Segment-Anything-Model" class="headerlink" title="3. Segment Anything Model"></a>3. Segment Anything Model</h2><p>接下来，我们描述了分段任意模型(SAM)，用于提示分段。SAM有三个组件，如图4所示:一个图像编码器，一个灵活的提示编码器和一个快速掩码解码器。我们建立在Transformer视觉模型[14,33,20,62]的基础上，对(平摊)实时性能进行了特定的权衡。我们在这里高层次地描述这些组件，详细信息以SA表示。</p><p>图像编码器。在可扩展性和强大的预训练方法的激励下，我们使用了MAE[47]预训练的视觉变压器(ViT)[33]，以最小程度适应处理高分辨率输入[62]。图像编码器每个图像运行一次，可以在提示模型之前应用。</p><p>提示编码器。我们考虑两组提示:稀疏(点、框、文本)和密集(掩码)。我们<strong>通过位置编码[95]来表示点和框</strong>，并对每个提示类型和使用CLIP[82]的现成文本编码器的自由格式文本进行学习嵌入求和。<strong>密集提示(即掩码)使用卷积嵌入</strong>，并在图像嵌入中按元素求和。</p><p>掩码译码器。掩码解码器有效地将图像嵌入、提示嵌入和输出令牌映射到掩码。该设计受到[14,20]的启发，采用了对Transformer解码器块[103]的修改，然后是动态掩码预测头。我们改进的解码器块在两个方向上使用提示自注意和交叉注意(提示到图像嵌入，反之亦然)来更新所有嵌入。在运行两个块之后，我们对图像嵌入进行上采样，MLP将输出标记映射到动态线性分类器，然后该分类器计算每个图像位置的掩码前景概率。</p><p>解决歧义。对于一个输出，如果给出一个模糊的提示，该模型将平均多个有效掩码。为了解决这个问题，我们修改了模型，以预测单个提示符的多个输出掩码(见图3)。我们发现3个掩码输出足以解决大多数常见情况(嵌套掩码通常最多有三个深度:整体、部分和子部分)。在训练期间，我们只在面罩上进行最小损失[15,45,64]。为了对掩码进行排序，该模型预测每个掩码的置信度分数(即估计的借据)。</p><p>效率。整体模型设计很大程度上是由效率驱动的。给定预先计算的图像嵌入，提示编码器和掩码解码器在web浏览器中运行，在CPU上，大约50ms。这种运行时性能支持模型的无缝、实时交互式提示。</p><p>损失和训练。我们使用[14]中使用的focal loss[65]和dice loss[73]的线性组合来监督掩模预测。我们使用几何提示的混合来训练可提示的分割任务(关于文本提示，请参阅$7.5)。接下来[92,37]，我们通过在每个掩码中随机抽取11轮提示来模拟交互式设置，从而使SAM无缝地集成到我们的数据引擎中。</p><h2 id="4-Segment-Anything-Data-Engine"><a href="#4-Segment-Anything-Data-Engine" class="headerlink" title="4. Segment Anything Data Engine"></a>4. Segment Anything Data Engine</h2><p>由于分割掩码在互联网上并不丰富，我们构建了一个数据引擎来收集我们的1.1 b掩码数据集SA-1B。数据引擎有三个阶段:(1)模型辅助的手动注释阶段，(2)混合了自动预测掩码和模型辅助注释的半自动阶段，以及(3)我们的模型在没有注释器输入的情况下生成掩码的全自动阶段。接下来我们将详细介绍每一个。</p><p>Assisted-manual阶段。在第一阶段，类似于经典的交互式分割，一个专业的注释者团队使用基于浏览器的交互式分割工具，通过点击前景&#x2F;背景对象点来标记蒙版。可以使用像素精确的“笔刷”和“橡皮擦”工具来改进蒙版。我们的模型辅助注释直接在浏览器中实时运行(使用预先计算的图像嵌入)，从而实现真正的交互式体验。我们没有对标记对象施加语义约束，注释器可以自由地标记“stuff”和“things”[1]。我们建议注释者标记他们可以命名或描述的对象，但没有收集这些名称或描述。注释者被要求按突出顺序标记对象，并被鼓励在一个蒙版超过30秒的注释时间后继续进行下一个图像。</p><p>在这一阶段的开始，SAM使用公共分割数据集进行训练。在充分标注数据后，SAM仅使用新标注的掩码进行再训练。随着蒙版收集的增加，图像编码器从ViT-B缩放到ViT-H，其他建筑细节也随之演变;我们总共重新训练了我们的模型6次。随着模型的改进，每个掩码的平均注释时间从34秒减少到14秒。我们注意到，14秒比COCO的掩码标注快6.5倍[66]，只比带极值点的边界盒标注慢2倍[76,71]。随着SAM的改进，每张图像的平均掩模数从20个增加到44个。总的来说，我们在这个阶段从120k张图像中收集了430万个遮罩。</p><p>半自动的阶段。在这个阶段，我们的目标是增加面具的多样性，以提高我们的模型分割任何东西的能力。为了将注释器集中在不太突出的对象上，我们首先自动检测自信掩码。然后，我们向注释者展示预先填充了这些蒙版的图像，并要求他们注释任何其他未注释的对象。为了检测自信蒙版，我们使用通用的“对象”类别在所有第一阶段蒙版上训练了一个边界盒检测器[84]。在这个阶段，我们在18k张图片中收集了额外的590万个遮罩(总共1020万个遮罩)。与第一阶段一样，我们定期在新收集的数据上重新训练模型(5次)。每个蒙版的平均注释时间恢复到34秒(不包括自动蒙版)，因为这些对象的标记更具挑战性。每张图像的平均掩模数量从44个增加到72个(包括自动掩模)。</p><p>全自动阶段。在最后阶段，注释是全自动的。这是可行的，因为我们的模型有两个主要的改进。首先，在这一阶段的开始，我们收集了足够的遮罩来极大地改进模型，包括前一阶段的各种遮罩。其次，在这个阶段，我们已经开发了模糊感知模型，它允许我们预测有效的掩码，即使在模糊的情况下。具体来说，我们用32×32规则的点网格提示模型，并为每个点预测一组可能对应于有效对象的掩码。使用模糊感知模型，如果一个点位于部件或子部件上，我们的模型将返回子部件、部件和整个对象。我们的模型的IoU预测模块用于选择置信掩码;此外，我们只识别和选择稳定的掩模(如果在0.5-δ和0.5 +δ的概率图上设置阈值导致相似的掩模，我们认为掩模是稳定的)。最后，在选择自信和稳定的掩码后，我们使用非最大抑制(NMS)来过滤重复。为了进一步提高小蒙版的质量，我们还处理了多个重叠的放大图像裁剪。有关此阶段的进一步详细信息，请参见$B。我们对数据集中的所有11M张图像应用了全自动蒙版生成，总共产生了11亿个高质量的蒙版。接下来，我们描述和分析结果数据集SA-1B。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【DETR】End-to-End Object Detection with Transformers</title>
      <link href="/2024/09/20/23_%E3%80%90DETR%E3%80%91End-to-End%20Object%20Detection%20with%20Transformers/"/>
      <url>/2024/09/20/23_%E3%80%90DETR%E3%80%91End-to-End%20Object%20Detection%20with%20Transformers/</url>
      
        <content type="html"><![CDATA[<p>End-to-End Object Detection with Transformers<br>2024</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>先前的目标检测方法流程复杂，需要anchor和nms操作，不是端到端的<br>![在这里插入图片描述](.&#x2F;2025-09-20-【DETR】End-to-End Object Detection with Transformers&#x2F;img_1.png)</li><li>解决方法<br>将检测视为集合预测问题，简化了目标检测的整体流程，不需要anchor和nms操作，做到了真正的end-to-end。编码器部分很常规，关键部分在解码器，设置一系列可学习query，从编码的特征中进行查询，然后通过FFN回归得到box，推理的时候只需要根据置信度对框进行筛选。<br>![在这里插入图片描述](.&#x2F;2025-09-20-【DETR】End-to-End Object Detection with Transformers&#x2F;img_2.png)</li><li>注意的问题和细节<br>（1）损失函数计算：通过匈牙利算法与GT进行匹配，没有类别的框则和背景匹配<br>（2）可学习的query的数量：DETR中设置的是100</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【LLaVA】Visual Instruction Tuning</title>
      <link href="/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/"/>
      <url>/2024/07/25/22_%E3%80%90LLaVA%E3%80%91Visual%20Instruction%20Tuning/</url>
      
        <content type="html"><![CDATA[<p>Visual Instruction Tuning</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>指令微调再多模态领域的探索较少</li><li>解决方法<br>利用GPT-4来生成多模态语言图像instruction-following数据，然后进行微调。微调分为两步，第一步只微调embedding，第二步微调embedding和LLM。</li><li>注意问题和细节<br>（1）和InstructBLIP的区别：InstructBLIP只微调Q-former，LLaVA在第二阶段微调了embedding和LLM；InstructBLIP会将instruction输入到Q-former来辅助视觉特征的抽取，LLaVA没有；InstructBLIP的指令都是人工设计的，LLaVA基于GPT-4，可能会更丰富。<br>（2）LLaVA用的视觉模型和LLM：视觉模，型用的ViT-L，LLM用的是Vicuna（有7B和13B两个版本，13B的版本大概需要三十多G的显存；后续的LLaVA-NeXT支持了其他的LLM，比如LLama-3(8B)、Qwen-1.5(72B&#x2F;110B)）</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>使用机器生成的指令跟随数据对大型语言模型(LLM)进行指令调优已被证明可以提高新任务的零射击能力，但该想法在多模态领域的探索较少。我们首次尝试使用仅语言的GPT-4来生成多模态语言图像instruction-following数据。通过对这些生成的数据进行指令调优，我们引入了LLaVA:大型语言和视觉助理，这是一个端到端的训练大型多模态模型，它将视觉编码器和LLM连接起来，用于通用的视觉和语言理解。为了促进视觉指令跟随的进一步研究，我们构建了两个具有多样化和挑战性的应用导向任务的评估基准。我们的实验表明，LLaVA展示了令人印象深刻的多模态聊天能力，有时在看不见的图像&#x2F;指令上表现出多模态GPT-4的行为，与GPT-4在合成的多模态指令遵循数据集上相比，产生了85.1%的相对分数。当在Science QA上进行微调时，LLaVA和GPT-4的协同作用达到了92.53%的最新水平。我们让GPT-4生成的可视化指令调优数据、我们的模型和代码公开可用。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>人类通过视觉和语言等多种渠道与世界互动，因为每一种渠道在表达和交流某些概念方面都具有独特的优势，从而有助于更好地理解世界。人工智能的核心目标之一是开发一种通用的助手，它可以有效地遵循多模态视觉和语言指令，与人类的意图保持一致，在野外完成各种现实世界的任务[4,27,26]。</p><p>为此，社区对开发语言增强基础视觉模型产生了浓厚的兴趣[27,16]，该模型具有强大的开放世界视觉理解能力，如分类[40,21,57,54,39]、检测[29,62,33]、分割[25,63,58]和字幕[50,28]，以及视觉生成和编辑[42,43,56,15,44,30]。我们建议读者参考《Computer Vision in the Wild》的阅读清单，以获得更多最新的文献汇编[12]。在这条工作线中，每个任务由一个单一的大视觉模型独立解决，在模型设计中隐式地考虑任务指令。此外，语言仅用于描述图像内容。虽然这允许语言在将视觉信号映射到语言语义(人类交流的常见渠道)方面发挥重要作用，但它导致模型通常具有固定接口，交互性和对用户指令的适应性有限。</p><p>另一方面，大型语言模型(LLM)已经表明，语言可以发挥更广泛的作用:作为通用助手的通用接口，各种任务指令可以显式地用语言表示，并引导端到端训练的神经助手切换到感兴趣的任务来解决它。例如，最近ChatGPT[35]和GPT-4[36]的成功证明了对齐LLM在遵循人类指令方面的强大功能，并激发了开发开源LLM的巨大兴趣。其中，LLaMA[49]是一个与GPT-3性能相匹配的开源LLM。Alpaca[48]、Vicuna[9]、GPT-4-LLM[38]利用各种机器生成的高质量指令遵循样本来提高LLM的对齐能力，与专有LLM相比，报告了令人印象深刻的性能。重要的是，这一行工作是纯文本的。</p><p>在本文中，我们提出了视觉指令调谐，这是将指令调谐扩展到语言-图像多模态空间的第一次尝试，为构建通用的视觉助手铺平了道路。特别地，我们的论文做出了以下贡献:</p><ul><li>多模态指令跟随数据。一个关键的挑战是缺乏视觉语言指令跟随数据。我们使用ChatGPT&#x2F;GPT-4提出了一个数据转换视角和管道，将图像-文本对转换为适当的指令遵循格式。</li><li>大型多模态模型。我们通过将CLIP的开放集视觉编码器[40]与语言解码器Vicuna[9]连接起来，开发了一个大型多模态模型(LMM)，并对我们生成的教学视觉语言数据进行端到端的微调。我们的实证研究验证了使用生成数据进行LMM指令调优的有效性，并提出了构建通用指令遵循视觉代理的实用技巧。当与GPT-4集成时，我们的方法在Science QA[34]多模态推理数据集上实现了SoTA。</li><li>多模态指令遵循基准。我们现在的LLaVA-Bench有两个具有挑战性的基准，与配对的图像，说明和详细的注释的不同选择。</li><li>开源的。我们向公众发布了以下资产:生成的多模态指令数据、代码库、模型检查点和可视聊天演示。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>多模态指令跟随代理。在计算机视觉中，现有的构建指令跟随智能体的工作可以大致分为两类:(i)端到端训练模型，它们分别针对每个特定的研究主题进行探索。例如，视觉语言导航任务[3,19]和Habitat[47]要求嵌入的AI智能体遵循自然语言指令，并采取一系列动作来完成视觉环境中的目标。在图像编辑领域，给定输入图像和告诉代理该做什么的书面指令，InstructPix2Pix[6]通过遵循人类指令来编辑图像。(ii)通过LangChain [1] &#x2F; LLMs[35]协调各种模型的系统，如Visual ChatGPT[53]、X-GPT[63]、MM-REACT[55]、VisProg[18]和ViperGPT[46]。虽然在构建指令遵循代理方面有相同的目标，但我们专注于为多个任务开发端到端的训练语言视觉多模态模型。</p><p>指令调优。在自然语言处理(NLP)领域，为了使GPT-3[7]、T5[41]、PaLM[10]和OPT[60]等LLM能够遵循自然语言指令并完成现实世界的任务，研究人员探索了LLM指令调优的方法[37,52,51]，从而分别产生了指令调优的对应物，如InstructGPT [37]&#x2F;ChatGPT[35]、FLAN-T5[11]、FLAN-PaLM[11]和OPT- iml[22]。结果表明，这种简单的方法可以有效地提高LLM的零弹和少弹泛化能力。因此，将NLP的思想借用到计算机视觉是很自然的。更广泛地说，基于基础模型的师生蒸馏思想已经在图像分类等其他主题中得到了研究[14]。Flamingo[2]可以被看作是多模态域的GPT-3时刻，因为它在零射击任务迁移和情境学习方面表现出色。其他在图像文本对上训练的LMM包括BLIP-2[28]、FROMAGe[24]和KOSMOS-1[20]。PaLM-E[13]是一个用于嵌入AI的LMM。基于最近“最好的”开源LLM LLaMA, OpenFlamingo[5]和LLaMA-Adapter[59]是开源的努力，使LLaMA能够使用图像输入，为构建开源多模态LLM铺平了道路。虽然这些模型表现出很好的任务转移泛化性能，但它们没有明确地与视觉语言指令数据进行调整，并且它们在多模态任务中的性能通常低于仅语言任务。本文旨在填补这一空白，并研究其有效性。最后，需要注意的是，视觉指令调优与视觉提示调优不同[23]:前者旨在提高模型的指令跟随能力，后者旨在提高模型自适应的参数效率。</p><h2 id="3-GPT-assisted-Visual-Instruction-Data-Generation"><a href="#3-GPT-assisted-Visual-Instruction-Data-Generation" class="headerlink" title="3. GPT-assisted Visual Instruction Data Generation"></a>3. GPT-assisted Visual Instruction Data Generation</h2><p>从CC[8]到LAION[45]，图像-文本对等公共多模式数据的数量在社区中激增。然而，当涉及到多模态指令遵循数据时，可用的数量是有限的，部分原因是创建此类数据的过程非常耗时，并且在考虑到人类群体搜索时定义不明确。受到最近GPT模型在文本注释任务中的成功[17]的启发，我们提出利用ChatGPT&#x2F;GPT-4进行多模态指令跟随数据收集，基于广泛存在的图像对数据。</p><p>对于图像XvXvXv及其相关的标题XcXcXc，创建一组问题XqXqXq是很自然的，目的是指示助手描述图像内容。我们提示GPT-4整理这样一个问题列表(详见附录)。因此，将图像-文本对扩展到其指令后续版本的一个简单方法是Human:XqXv<STOP>Assistant:Xc<STOP>Human: Xq Xv<STOP> Assistant: Xc<STOP>Human:XqXv<STOP>Assistant:Xc<STOP>。虽然构建成本低廉，但这种简单的扩展版本在指令和响应方面缺乏多样性和深度推理。</STOP></STOP></STOP></STOP></STOP></STOP></p><p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_1.png)</p><p>为了缓解这个问题，我们利用仅语言的GPT-4或ChatGPT作为强大的教师(两者都只接受文本作为输入)，以创建包含视觉内容的指令跟随数据。具体来说，为了将图像编码为其视觉特征以提示纯文本GPT，我们使用两种类型的符号表示:(i)通常从不同角度描述视觉场景的字幕;(ii)边界框通常对场景中的物体进行定位，每个边界框对物体概念及其空间位置进行编码。表14的顶部块显示了一个示例。</p><p>这种符号表示允许我们将图像编码为LLM可识别的序列。我们使用COCO图像[31]，生成三种类型的指令跟随数据。表14的底部块显示了每种类型的一个示例。对于每种类型，我们首先手动设计一些示例。它们是我们在数据收集过程中唯一的人工注释，并被用作在上下文学习中查询GPT-4的种子示例。</p><ul><li>谈话。我们设计了一个对话，在助理和一个询问这张照片问题的人之间进行。回答的语气就好像助理正在看图像并回答问题一样。关于图像的视觉内容有一系列不同的问题，包括对象类型、对象计数、对象动作、对象位置、对象之间的相对位置。只考虑有明确答案的问题。详细提示请参见附录。</li><li>详细的描述。为了包含丰富而全面的图像描述，我们创建了一个带有这种意图的问题列表。我们提示GPT-4，然后整理列表(参见附录中详细的提示和整理过程)。对于每个图像，我们从列表中随机抽取一个问题，让GPT-4生成详细的描述。</li><li>复杂的推理。以上两种类型关注的是视觉内容本身，在此基础上我们进一步打造深度推理题。答案通常需要遵循严格的逻辑，一步一步地推理。</li></ul><p>我们总共收集了158K个独特的语言图像指令遵循样本，其中对话58K，详细描述23K，复杂推理77k。我们在早期实验中减少了ChatGPT和GPT-4的使用，发现GPT-4始终如一地提供更高质量的指令跟随数据，例如空间推理。</p><h2 id="4-Visual-Instruction-Tuning"><a href="#4-Visual-Instruction-Tuning" class="headerlink" title="4. Visual Instruction Tuning"></a>4. Visual Instruction Tuning</h2><h3 id="4-1-Architecture"><a href="#4-1-Architecture" class="headerlink" title="4.1 Architecture"></a>4.1 Architecture</h3><p>主要目标是有效地利用预训练的LLM和视觉模型的功能。网络架构如图1所示。我们选择Vicuna[9]作为我们的LLM fϕ(·)参数化的ϕ，因为它在公开可用的检查点中具有最佳的语言任务指令跟踪能力[48,9,38]。<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_5.png)</p><p>对于输入图像XvXvXv，我们考虑预训练的CLIP视觉编码器ViT-L&#x2F;14[40]，它提供视觉特征Zv&#x3D;g(Xv)Zv &#x3D; g(Xv)Zv&#x3D;g(Xv)。我们的实验考虑了最后一层Transformer前后的网格特征。我们考虑一个简单的线性层来连接图像特征到词嵌入空间。具体来说，我们使用一个可训练的投影矩阵W将ZvZvZv转换为语言嵌入令牌HvHvHv，它与语言模型中的词嵌入空间具有相同的维数:</p><p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_3.png)</p><p>这样，我们就有了一系列视觉标记HvHvHv。请注意，我们的简单投影方案是轻量级的，它允许我们快速迭代以数据为中心的实验。还可以考虑更复杂的方案来连接图像和语言表征，例如Flamingo中的门控交叉注意[2]和BLIP-2中的Q-former[28]。我们将为LLaVA探索更有效、更复杂的架构设计作为未来的工作。</p><h3 id="4-2-Training"><a href="#4-2-Training" class="headerlink" title="4.2 Training"></a>4.2 Training</h3><p>对于每张图像XvXvXv，我们生成多回合对话数据(Xq1,Xa1,…,XqT,XaT)(X_{q}^{1},X_{a}^{1},…,X_{q}^{T},X_{a}^{T})(Xq1​,Xa1​,…,XqT​,XaT​)，其中TTT为总回合数。我们将它们组织成一个序列，将所有的回答视为助手的响应，并将指令XtXtXt指令在第t个转弯处为:<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_4.png)<br>这导致了表2中所示的多模态指令跟随序列的统一格式。我们使用其原始的自回归训练目标对预测令牌执行LLM的指令调优。</p><p>具体来说，对于长度为L的序列，我们计算目标答案XaXaXa的概率为:<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_7.png)<br>其中，θ为可训练参数，Xinstruct,&lt;iX_{instruct,&lt;i}Xinstruct,&lt;i​和Xa,&lt;iX_{a,&lt;i}Xa,&lt;i​分别为当前预测令牌xix_{i}xi​之前所有回合的指令令牌和回答令牌。有关预测令牌的说明，请参见表2。对于(3)中的条件，我们显式地添加了XvXvXv，以强调图像是基于所有答案的事实，并且为了更好的可读性，我们省略了Xsystem−messageX_{system-message}Xsystem−message​和所有前面的。对于LLaVA模型训练，我们考虑一个两阶段的指令调优过程。<br>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_2.png)</p><p><strong>阶段1:</strong> 特征对齐的预训练。为了在概念覆盖率和训练效率之间取得平衡，我们将CC3M过滤到595K图像-文本对。有关过滤过程的详情，请参阅附录。使用第3节中描述的朴素展开方法将这些数据对转换为跟随指令的数据。每个样本都可以视为单回合对话。为了构造(2)中的输入x指令，对于图像XvXvXv，随机采样一个问题XqXqXq，这是一个语言指令，要求助手对图像进行简要描述。最基本的预测答案XaXaXa是原始的标题。在训练中，我们保持视觉编码器和LLM权值不变，并最大化(3)的似然值，只有可训练参数θ &#x3D;W(投影矩阵)。这样，图像特征HvHvHv可以与预训练的LLM词嵌入对齐。这个阶段可以理解为为冻结的LLM训练一个兼容的视觉标记器。</p><p><strong>阶段2:</strong> 端到端微调。我们始终保持视觉编码器权值不变，并不断更新投影层和LLM的预训练权值;即，可训练的参数是(3)中的θ&#x3D;{W,ϕ}。我们考虑两个特定的用例场景:</p><ul><li>多通道聊天机器人。我们通过对第3节中的158K语言图像指令跟踪数据进行微调来开发聊天机器人。在这三种类型的响应中，会话是多回合的，而其他两种是单回合的。它们在训练中被统一采样。</li><li>科学QA。我们在ScienceQA基准上研究了我们的方法[34]，这是第一个大规模的多模态科学问题数据集，它用详细的讲座和解释注释了答案。每个问题都以自然语言或图像的形式提供上下文。该助手以自然语言提供推理过程，并从多个选项中选择答案。对于(2)中的训练，我们将数据组织为单回合对话，问题和上下文作为XinstructX_{instruct}Xinstruct​，推理和答案作为XaXaXa。</li></ul><h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><p>我们通过两个主要的实验设置分别评估了LLaVA在指令跟随和视觉推理能力方面的性能:多模态聊天机器人和ScienceQA数据集。我们使用8张A100训练所有模型，遵循Vicuna的超参数[9]。我们在过滤后的CC-595K子集上对模型进行了1 epoch的预训练，学习率为2e-3，批大小为128，并对提出的LLaVA-Instruct-158K数据集进行了3 epoch的微调，学习率为2e-5，批大小为32。更多培训细节见附录。</p><h3 id="5-1-Multimodal-Chatbot"><a href="#5-1-Multimodal-Chatbot" class="headerlink" title="5.1 Multimodal Chatbot"></a>5.1 Multimodal Chatbot</h3><p>我们开发了一个聊天机器人演示，以展示LLaVA的图像理解和对话能力，并研究LLaVA在消化视觉输入和展示指令遵循能力方面的能力。我们首先使用原始GPT-4论文[36]中的示例，如表3所示(更多示例见附录)，这些示例需要深入的图像理解。为了比较，我们从他们的论文中引用了多模态GPT-4的提示和响应，并查询了BLIP-2和OpenFlamingo模型检查点来获得它们的响应。</p><p>![在这里插入图片描述](.&#x2F;2024-07-25-【LLaVA】Visual Instruction Tuning&#x2F;img_6.png)</p><p>令人惊讶的是，尽管LLaVA是用一个小的多模态指令跟随数据集(~ 80K唯一图像)训练的，但它在这些示例上展示了与多模态GPT-4非常相似的推理结果。请注意，虽然这些图像是LLaVA的域外图像，但LLaVA仍然能够理解场景并按照问题说明提供合理的响应。相比之下，BLIP-2和OpenFlamingo侧重于描述图像，而不是按照用户的指示以适当的方式回答。</p><p>定量评价。为了系统地了解LLaVA的性能，我们提出了一个定量度量来衡量模型在多模态数据上的指令跟随能力。受[9]的启发，我们利用GPT-4来衡量生成的响应的质量。具体来说，我们创建了由图像、基本事实文本描述和问题组成的三元组。候选模型(例如，LLaVA)根据问题和图像预测答案。为了提供一个近似的理论上限，我们使用纯文本GPT-4，基于问题和基本事实文本描述创建了一个参考预测。在获得两个模型的响应后，我们将问题、视觉信息(以文本描述的格式)和两个助手生成的响应馈送给裁判(即纯文本GPT-4)。它评估助手回答的有用性、相关性、准确性和详细程度，并给出1到10分的总分，分数越高表明整体表现越好。并要求对评估提供一个全面的解释，以便我们更好地理解模型。我们报告了相对分数，而不是纯文本GPT-4模型，该模型使用纹理基础真值描述作为视觉输入。我们创建了两个基准来评估模型的性能。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>本文论证了视觉指令调优的有效性。我们提出了一个自动管道来创建语言图像指令跟随数据，并在此基础上训练多模态模型LLaVA来跟随人类意图完成视觉任务。当在ScienceQA上进行微调时，它达到了新的SoTA精度，当在多模态聊天数据上进行微调时，它具有出色的视觉聊天功能。此外，我们提出了第一个研究多模态指令跟随能力的基准。本文是视觉教学调优的第一步，主要关注现实生活中的任务。关于LLaVA在学术基准上的更多定量结果，请参考使用视觉指令调优的改进基线[32]。我们希望我们的工作能够启发未来建立更有能力的多模态模型的研究。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning</title>
      <link href="/2024/07/24/21_%E3%80%90InstructBLIP%E3%80%91Towards%20General-purpose%20Vision-Language%20Models%20with%20Instruction%20Tuning/"/>
      <url>/2024/07/24/21_%E3%80%90InstructBLIP%E3%80%91Towards%20General-purpose%20Vision-Language%20Models%20with%20Instruction%20Tuning/</url>
      
        <content type="html"><![CDATA[<p>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>视觉语言预训练已经得到了广泛的研究，但视觉语言指令调优仍未得到充分的探索</li><li>解决方法<br>（1）收集了26个公开可用的数据集，涵盖了各种各样的任务和功能，并将它们转换为指令调优格式<br>（2）还引入了一个指令感知的Query Transformer，它可以提取针对给定指令的信息特征</li><li>注意问题和细节<br>（1）Query Transformer和BLIP-2中的区别：InstructBLIP中的QFormer增加了instruction，此外LLM的输入增加了instruction<br>（2）亮点：instruction不仅会指导LLM生成文本，同时也会指导image encoder提取不同的视觉特征。这样的好处在于对于同一张图片，根据不同的instruction，可以得到基于instruction偏好更强的视觉特征，同时对于两个不一样的图片，基于instruction内嵌的通用知识，可以使得模型有更好的知识迁移效果。</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>大规模的预训练和指令调优已经成功地创建了具有广泛能力的通用语言模型。然而，由于额外的视觉输入带来的丰富的输入分布和任务多样性，构建通用的视觉语言模型是具有挑战性的。尽管视觉语言预训练已经得到了广泛的研究，但视觉语言指令调优仍未得到充分的探索。在本文中，我们对基于预训练的BLIP-2模型的视觉语言教学调优进行了系统而全面的研究。我们收集了26个公开可用的数据集，涵盖了各种各样的任务和功能，并将它们转换为指令调优格式。此外，我们还引入了一个指令感知的Query Transformer，它可以提取针对给定指令的信息特征。在13个手持数据集上进行训练，InstructBLIP在所有13个手持数据集上实现了最先进的zero-shot性能，大大优于BLIP-2和更大的Flamingo模型。当对单个下游任务进行微调时，我们的模型也会带来最先进的性能(例如，在带有图像上下文的ScienceQA问题上，准确率为90.7%)。此外，我们定性地证明了InstructBLIP相对于并发多模态模型的优势。所有的InstructBLIP模型都是开源的。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>人工智能(AI)研究的一个长期愿望是建立一个单一的模型，可以解决用户指定的任意任务。在自然语言处理(NLP)中，指令调优[46,7]被证明是实现这一目标的一种很有前途的方法。通过在自然语言指令所描述的广泛任务上对大型语言模型(LLM)进行微调，指令调优使模型能够遵循任意指令。最近，指令调优的LLM也被用于视觉语言任务。例如，BLIP-2[20]有效地适应冻结指令调优的LLM来理解视觉输入，并显示出在图像到文本生成中遵循指令的初步能力。</p><p>与自然语言处理任务相比，视觉语言任务由于来自不同领域的额外视觉输入，在本质上更加多样化。这对一个统一的模型提出了更大的挑战，这个模型应该推广到不同的视觉语言任务，许多任务在训练期间是看不见的。大多数以前的工作可以分为两种方法。第一种方法是多任务学习[6,27]，它将各种视觉语言任务制定成相同的输入输出格式。然而，我们从经验上发现，没有指令的多任务学习(表4)不能很好地推广到未见过的数据集和任务。第二种方法[20,4]使用额外的视觉组件扩展预训练的LLM，并使用图像标题数据训练视觉组件。然而，这些数据太过有限，无法对需要更多视觉描述的视觉语言任务进行广泛的概括。</p><p>为了解决上述挑战，本文提出了InstructBLIP，这是一个视觉语言指令调优框架，它使通用模型能够通过统一的自然语言接口解决广泛的视觉语言任务。InstructBLIP使用不同的指令数据集来训练多模态LLM。具体来说，我们使用预训练的BLIP-2模型初始化训练，该模型由图像编码器，LLM和查询转换器(Q-Former)组成，以桥接两者。在指令调优期间，我们在保持图像编码器和LLM冻结的同时对Q-Former进行了调优。本文的主要贡献如下:</p><ul><li>我们对视觉语言指令调优进行了全面而系统的研究。我们将26个数据集转换为指令调优格式，并将其分为11个任务类别。我们使用13个保留数据集用于指令调优，13个保留数据集用于零射击评估。此外，我们保留了四个完整的任务类别，以便在任务水平上进行零射击评估。详尽的定量和定性结果证明了instructlip在视觉语言零点泛化方面的有效性。</li><li>我们提出了指令感知的视觉特征提取，这是一种新的机制，可以根据给定的指令灵活地提取信息丰富的特征。具体来说，除了给冻结的LLM文本指令外，还给Q-Former文本指令，使其能够从冻结的图像编码器中提取指令感知的视觉特征。此外，我们提出了一种平衡采样策略来同步跨数据集的学习进度。</li><li>我们使用两个LLM家族评估并开源了一套InstructBLIP模型:1)FlanT5[7]，一个从T5[34]微调的编码器-解码器LLM;2) Vicuna[2]，由LLaMA[41]改进而来的仅解码器LLM。InstructBLIP模型在广泛的视觉语言任务中实现了最先进的零射击性能。此外，当将InstructBLIP模型用作单个下游任务的模型初始化时，会导致最先进的调优性能。</li></ul><p>![在这里插入图片描述](.&#x2F;2024-07-24-【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning&#x2F;img_1.png)</p><h2 id="2-Vision-Language-Instruction-Tuning"><a href="#2-Vision-Language-Instruction-Tuning" class="headerlink" title="2. Vision-Language Instruction Tuning"></a>2. Vision-Language Instruction Tuning</h2><p>InstructBLIP旨在解决视觉语言教学调优中的独特挑战，并对模型对未知数据和任务的改进泛化能力进行系统研究。在本节中，我们首先介绍指令调优数据的构造，然后介绍训练和评估协议。接下来，我们分别从模型和数据的角度描述了两种提高指令调优性能的技术。最后，给出了实现细节。</p><h3 id="2-1-Tasks-and-Datasets"><a href="#2-1-Tasks-and-Datasets" class="headerlink" title="2.1 Tasks and Datasets"></a>2.1 Tasks and Datasets</h3><p>为了保证指令调优数据的多样性，同时考虑其可访问性，我们收集了一套全面的公开可用的视觉语言数据集，并将其转换为指令调优格式。如图2所示，最终的集合涵盖了11个任务类别和26个数据集，包括图像字幕[23,3,51]、带有阅读理解的图像字幕[38]、视觉推理[16,24,29]、图像问答[11,12]、基于知识的图像问答[30,36,28]、带有阅读理解的图像问答[31,39]、图像问题生成(改编自QA数据集)、视频问答[47,49]、视觉会话问答[8]，图像分类[18]，以及LLaVA-Instruct-150K[25]。我们在附录C中包含了每个数据集的详细描述和统计。</p><p>![在这里插入图片描述](.&#x2F;2024-07-24-【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning&#x2F;img_2.png)</p><p>对于每个任务，我们都精心制作了10到15个不同的自然语言指令模板。这些模板作为构造指令调优数据的基础，指令调优数据阐明了任务和目标。对于本质上倾向于短响应的公共数据集，我们在一些相应的指令模板中使用short和brief等术语，以减少模型过度拟合的风险，从而始终生成短输出。对于LLaVA-Instruct-150K数据集，我们没有合并额外的指令模板，因为它自然地以指令格式结构化。完整的指令模板列表可在附录D中找到。</p><h3 id="2-2-Training-and-Evaluation-Protocols"><a href="#2-2-Training-and-Evaluation-Protocols" class="headerlink" title="2.2 Training and Evaluation Protocols"></a>2.2 Training and Evaluation Protocols</h3><p>为了保证有足够的数据和任务用于训练和零射击评估，我们将这26个数据集分为13个held-in数据集和13个held-out数据集，如图2中分别用黄色和白色表示。我们使用保留数据集的训练集进行指令调优，并使用保留数据集的验证或测试集进行评估。</p><p>对于held-out评估，我们的目标是了解指令调优如何在未见过的数据上提高模型的零射击性能。我们定义了两种类型的held-out数据:1)在训练期间不暴露给模型的数据集，但其任务存在于暂存集群中;2)在训练过程中完全看不见的数据集及其相关任务。由于数据分布在保留数据集和保留数据集之间发生了变化，因此处理第一种类型的保留评估是非常有意义的。对于第二种类型，我们完全承担了几个任务，包括视觉推理、视频问答、视觉对话QA和图像分类。</p><p>为了避免数据污染，需要仔细选择数据集，以便在不同数据集的保留训练聚类中不会出现评估数据。在指令调优过程中，我们对每个数据集统一混合所有保留的训练集和样本指令模板。使用标准语言建模损失对模型进行训练，以直接生成给定指令的响应。此外，对于涉及场景文本的数据集，我们在指令中添加OCR标记作为补充信息。</p><h3 id="2-3-Instruction-aware-Visual-Feature-Extraction"><a href="#2-3-Instruction-aware-Visual-Feature-Extraction" class="headerlink" title="2.3 Instruction-aware Visual Feature Extraction"></a>2.3 Instruction-aware Visual Feature Extraction</h3><p>现有的零镜头图像到文本生成方法，包括BLIP-2，在提取视觉特征时采用了指令不可知的方法。这将导致一组静态视觉表示被输入到LLM中，而不管任务是什么。而指令感知视觉模型能够适应任务指令，产生最有利于当前任务的视觉表征。如果我们期望同一输入图像的任务指令有很大的不同，这显然是有利的。</p><p>我们在图3中展示了instructlip的体系结构。与BLIP-2[20]类似，InstructBLIP利用查询转换器(Query Transformer)或Q-Former从冻结的图像编码器中提取视觉特征。Q-Former的输入包含一组K个可学习的查询嵌入，它们通过交叉注意与图像编码器的输出交互。Q-Former的输出由K个编码的视觉向量组成，每个查询嵌入一个，然后经过线性投影并馈送到冻结的LLM。与BLIP-2一样，Q-Former在指令调优之前使用图像标题数据分两个阶段进行预训练。第一阶段使用冻结图像编码器对Q-Former进行预训练，用于视觉语言表示学习。第二阶段将Q-Former的输出调整为使用冻结的LLM生成文本的软视觉提示。在预训练之后，我们通过指令调优对Q-Former进行微调，其中LLM接收来自Q-Former和任务指令的视觉编码作为输入。</p><p>在对BLIP-2的扩展中，instructlip提出了一个指令感知的Q-former模块，该模块接受指令文本令牌作为附加输入。该指令通过Q-Former的自关注层与查询嵌入交互，并鼓励提取与任务相关的图像特征。因此，LLM接收到的视觉信息有利于后续的教学。我们通过经验证明(表2)，指令感知的视觉特征提取提供了实质性的性能改进。</p><p>![在这里插入图片描述](.&#x2F;2024-07-24-【InstructBLIP】Towards General-purpose Vision-Language Models with Instruction Tuning&#x2F;img_3.png)</p><h3 id="2-4-Balancing-Training-Datasets"><a href="#2-4-Balancing-Training-Datasets" class="headerlink" title="2.4 Balancing Training Datasets"></a>2.4 Balancing Training Datasets</h3><p>由于训练数据集数量众多，且每个数据集的大小差异显著，因此将它们均匀混合可能导致模型过拟合较小的数据集，欠拟合较大的数据集。为了缓解这个问题，我们建议用与其大小或训练样本数量的平方根成比例的概率对数据集进行采样。通常，给定D个大小为{S1, S2，…， Sp)，在训练过程中从数据集d中选择数据样本的概率为paΣΕSSa。在此公式之上，我们对某些数据集的权重进行手动调整以改进优化。这是由于数据集和任务的内在差异所保证的，尽管规模相似，但需要不同水平的训练强度。具体来说，我们降低了以选择题为特征的A-OKVQA的权重，增加了需要开放式文本生成的OKVQA的权重。在表2中，我们展示了平衡数据集采样策略提高了持有评估和持有泛化的整体性能。</p><h3 id="2-5-Inference-Methods"><a href="#2-5-Inference-Methods" class="headerlink" title="2.5 Inference Methods"></a>2.5 Inference Methods</h3><p>在推理期间，我们采用两种略有不同的生成方法对不同的数据集进行评估。对于大多数数据集，如图像字幕和开放式VQA，指令调整模型被直接提示生成响应，随后将其与基本事实进行比较以计算指标。另一方面，对于分类和多选择VQA任务，我们采用了先前工作[46,22,21]的词汇排名方法。具体来说，我们仍然提示模型生成答案，但将其词汇表限制为候选列表。然后，我们计算每个候选的对数似然，并选择值最高的一个作为最终预测。该排序方法应用于ScienceQA、IconQA、A-OKVQA(多项选择)、HatefulMemes、Visual Dialog、MSVD和MSRVTT数据集。此外，对于二元分类，我们将积极和消极标签扩展到一个稍微更广泛的语言器集合中，以利用自然文本中的单词频率(例如，积极类的yes和true;否定类为No和false)。</p><p>对于视频问答任务，我们在每个视频中使用四个均匀采样帧。每一帧分别由图像编码器和Q-Former进行处理，提取的视觉特征进行拼接，然后送入LLM。</p><h3 id="2-6-Implementation-Details"><a href="#2-6-Implementation-Details" class="headerlink" title="2.6 Implementation Details"></a>2.6 Implementation Details</h3><p>体系结构。由于BLIP-2模块化架构设计的灵活性，我们可以快速调整该模型以适应各种llm。在我们的实验中，我们采用了四种具有相同图像编码器(ViT-G&#x2F;14[10])但不同冷冻llm的BLIP-2变体，包括FlanT5XL (3B)、FlanT5-XXL (11B)、Vicuna-7B和Vicuna-13B。FlanT5[7]是基于编码器-解码器Transformer T5[34]的指令调优模型。另一方面，Vicuna[2]是最近发布的仅支持解码器的Transformer指令，该指令由LLaMA[41]调整而来。在视觉语言指令调优期间，我们从预训练的BLIP-2检查点初始化模型，并在<strong>保持图像编码器和LLM冻结的同时仅微调Q-Former的参数</strong>。由于最初的BLIP-2模型不包括Vicuna的检查点，我们使用与BLIP-2相同的程序对Vicuna进行预训练。</p><p>体系结构。由于BLIP-2模块化架构设计的灵活性，我们可以快速调整该模型以适应各种LLM。在我们的实验中，我们采用了四种具有相同图像编码器(ViT-G&#x2F;14[10])但不同冷冻llm的BLIP-2变体，包括FlanT5XL (3B)、FlanT5-XXL (11B)、Vicuna-7B和Vicuna-13B。FlanT5[7]是基于编码器-解码器Transformer T5[34]的指令调优模型。另一方面，Vicuna[2]是最近发布的仅支持解码器的Transformer指令，该指令由LLaMA[41]调整而来。在视觉语言指令调优期间，我们从预训练的BLIP-2检查点初始化模型，并在保持图像编码器和LLM冻结的同时仅微调Q-Former的参数。由于最初的BLIP-2模型不包括Vicuna的检查点，我们使用与BLIP-2相同的程序对Vicuna进行预训练。</p><h2 id="3-Experimental-Results-and-Analysis"><a href="#3-Experimental-Results-and-Analysis" class="headerlink" title="3. Experimental Results and Analysis"></a>3. Experimental Results and Analysis</h2><h2 id="4-Related-Work"><a href="#4-Related-Work" class="headerlink" title="4. Related Work"></a>4. Related Work</h2><p>指令调优的目的是教语言模型遵循自然语言指令，这已经被证明可以提高它们对未知任务的泛化性能。一些方法通过使用模板将现有的NLP数据集转换为指令格式来收集指令调优数据[46,7,35,45]。其他人则使用LLM(例如GPT-3[5])生成指令数据[2,13,44,40]，提高了多样性。</p><p>通过向LLM注入视觉信息，指令调优LLM已经适应了视觉到语言的生成任务。BLIP-2[20]使用冻结的FlanT5模型，训练Q-Former提取视觉特征作为LLM的输入。MiniGPT-4[52]使用与BLIP-2相同的预训练视觉编码器和Q-Former，但使用Vicuna[2]作为LLM，并使用ChatGPT[1]生成的比BLIP-2训练数据更长的图像标题进行训练。LLaVA[25]直接将视觉编码器的输出作为LLaMA&#x2F;Vinuca LLM的输入，并根据GPT-4生成的视觉语言会话数据对LLM进行微调[33]。mPLUG-owl[50]使用来自LLaVA的文本指令数据和视觉语言指令数据对LLaMA[41]模型进行低秩自适应[14]。另一项独立的工作是multidirective[48]，它在没有预训练的LLM的情况下执行视觉语言指令调整，导致竞争力下降。</p><p>与现有的方法相比，InstructBLIP使用了更广泛的视觉语言指令数据，涵盖了基于模板的转换数据和llm生成的数据。在架构方面，InstructBLIP提出了一种指令感知的视觉特征提取机制。此外，本文还对视觉语言指令调整的各个方面进行了全面分析，验证了其在推广到未见任务方面的优势。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>在本文中，我们提出了一个简单而新颖的针对广义视觉语言模型的指令调优框架InstructBLIP。我们对视觉语言指令调优进行了全面的研究，并证明了InstructBLIP模型能够推广到具有最先进性能的各种未知任务。定性示例也展示了InstructBLIP在指令遵循方面的各种能力，例如复杂的视觉推理、基于知识的图像描述和多回合对话。此外，我们表明InstructBLIP可以作为下游任务微调的增强模型初始化，从而获得最先进的结果。我们希望InstructBLIP能够促进通用多模态人工智能及其应用的新研究。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【BLIP】Bootstrapping Language-Image Pre-training</title>
      <link href="/2024/07/21/20_%E3%80%90BLIP%E3%80%91Bootstrapping%20Language-Image%20Pre-training/"/>
      <url>/2024/07/21/20_%E3%80%90BLIP%E3%80%91Bootstrapping%20Language-Image%20Pre-training/</url>
      
        <content type="html"><![CDATA[<p>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>针对问题<br>（1）基于编码器的模型不太容易直接转移到文本生成任务(例如图像caption)，而编码器-解码器模型尚未成功地用于图像-文本检索任务<br>（2）现有方法使用的网络数据集存在噪声，带来次优结果</li><li>解决方法<br>（1）提出编码器-解码器的多模态混合架构，结合检索和生成caption的功能<br>（2）一种新的数据集bootstrap方法：先预训练，然后用精标注数据集微调，通过captioner生成数据，通过filter过滤数据</li><li>注意的问题和细节<br>（1）ITM Loss的作用：细粒度上的对齐<br>（2）为什么使用LM Loss而不使用MLM Loss：使用LM loss才能让模型具有生成能力<br>（3）在常规训练时，文本编码器和文本解码器共享除SA层之外的所有参数：原因是编码和解码任务之间的差异最好由SA层捕获，特别是，编码器使用双向自注意来构建当前输入标记的表示，而解码器使用因果自注意来预测下一个标记；另一方面，编码和解码任务之间的嵌入层、CA层和FFN的作用相似，因此共享这些层可以提高训练效率，同时受益于多任务学习。<br>（4）在微调captioner和filter时，文本编码器和文本解码器参数独立：实验发现参数共享时，由captioner产生的噪声字幕不太可能被filter滤除<br>（5）生成caption时用的提示词是“a picture of”</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>视觉语言预训练(VLP)提高了许多视觉语言任务的性能。然而，大多数现有的预训练模型只擅长基于理解的任务或基于生成的任务。此外，性能的提高很大程度上是通过使用从网络收集的噪声图像-文本对扩展数据集来实现的，这是一种次优的监督来源。本文提出了一种新的VLP框架BLIP，它可以灵活地转移到视觉语言理解和生成任务中。BLIP通过引导标题有效地利用了带有噪声的web数据，其中标题生成合成标题，滤波器去除噪声。我们在广泛的视觉语言任务上取得了最先进的结果，例如图像文本检索(平均+2.7% recall@1)，图像字幕(CIDEr +2.8%)和VQA (VQA分数+1.6%)。当以零射击的方式直接应用于视频语言任务时，BLIP也表现出较强的泛化能力。代码和模型可在<a href="https://github.com/salesforce/BLIP%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/salesforce/BLIP上获得。</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>近年来，视觉语言预训练在各种多模态下游任务中取得了巨大的成功。然而，现有的方法有两个主要的局限性:</p><p>(1) 模型视角:大多数方法要么采用基于编码器的模型(Radford et al.,2021;Li等人,2021a)，或编码器-解码器(Cho等人，2021;Wang et al.， 2021)模型。然而，基于编码器的模型不太容易直接转移到文本生成任务(例如图像字幕)，而编码器-解码器模型尚未成功地用于图像-文本检索任务。<br>(2) 数据视角:大多数最先进的方法(例如CLIP (Radford等人,2021)，ALBEF (Li等人,2021a)， SimVLM (Wang等人,2021))对从网络收集的图像-文本对进行预训练。尽管通过扩大数据集获得了性能提升，但我们的论文表明，噪声web文本对于视觉语言学习来说是次优的。</p><p>为此，我们提出了BLIP: Bootstrapping language-image Pre-training，用于统一的视觉语言理解和生成。BLIP是一种新的VLP框架，比现有方法能够实现更广泛的下游任务。它分别从模型和数据的角度介绍了两个贡献:</p><p>(a) 编码器-解码器的多模态混合(MED):一种有效的多任务预训练和灵活迁移学习的新模型架构。MED可以作为单模编码器、基于图像的文本编码器或基于图像的文本解码器操作。该模型采用图像文本对比学习、图像文本匹配和图像条件语言建模三个视觉语言目标进行联合预训练。<br>(b) Captioning and Filtering (CapFilt):一种新的数据集bootstrap方法，用于从噪声图像-文本对中学习。我们将预训练的MED调整为两个模块:一个captioner用于生成给定web图像的合成字幕，一个filter用于从原始web文本和合成文本中去除嘈杂的字幕。</p><p>我们进行了大量的实验和分析，并做出了以下主要观察结果。</p><ul><li>我们展示了captioner和filter一起工作，通过引导字幕来实现各种下游任务的实质性性能改进。我们还发现，标题越多样化，收益越大。</li><li>BLIP在广泛的视觉语言任务上实现了最先进的性能，包括图像文本检索、图像字幕、视觉问答、视觉推理和视觉对话。当直接将我们的模型转移到两个视频语言任务:文本到视频检索和视频QA时，我们也实现了最先进的zero-shot性能。</li></ul><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_6.png)</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h3 id="2-1-Vision-language-Pre-training"><a href="#2-1-Vision-language-Pre-training" class="headerlink" title="2.1 Vision-language Pre-training"></a>2.1 Vision-language Pre-training</h3><p>视觉语言预训练(VLP)旨在通过对大规模图像-文本对模型进行预训练来提高下游视觉和语言任务的性能。由于获取人工注释文本的费用过高，大多数方法(Chen et al.,2020;Li et al.,2020;2021年;Wang et al.,2021;Radford等人,2021)使用从网络抓取的图像和替代文本对(Sharma等人,2018;Changpinyo等,2021;Jia et al.,2021)。尽管使用了简单的基于规则的过滤器，噪声在网络文本中仍然普遍存在。然而，噪声的负面影响在很大程度上被忽略了，被通过扩大数据集获得的性能增益所掩盖。我们的论文表明，噪声网络文本对于视觉语言学习是次优的，并提出了以更有效的方式利用网络数据集的CapFilt。</p><p>已经有很多尝试将各种视觉和语言任务统一到一个框架中(Zhou et al.,2020;Cho等人,2021;Wang等人,2021)。最大的挑战是设计既能执行基于理解的任务(如图像文本检索)又能执行基于生成的任务(如图像字幕)的模型架构。基于编码器的模型(Li et al.，2021a;b; Radford et al.,2021)和编码器-解码器模型(Cho et al.,2021;Wang et al.,2021)两者都不可以同时在这两种类型的任务中表现出色，而单一的统一编码器-解码器(Zhou et al.,2020)也限制了模型的能力。我们提出的多模态编码器-解码器混合模型在广泛的下游任务上提供了更大的灵活性和更好的性能，同时保持了预训练的简单和高效。</p><h3 id="2-2-Knowledge-Distillation"><a href="#2-2-Knowledge-Distillation" class="headerlink" title="2.2 Knowledge Distillation"></a>2.2 Knowledge Distillation</h3><p>知识蒸馏(Knowledge distillation, KD) (Hinton et al.,2015)旨在通过从教师模型中提取知识来提高学生模型的性能。自蒸馏是KD的一种特殊情况，其中老师和学生的尺寸相等。它已被证明对图像分类是有效的(Xie等人,2020)，最近对VLP也是有效的(Li等人,2021a)。与大多数现有的KD方法不同，这些方法只是简单地强制学生与教师具有相同的类别预测，我们提出的CapFilt可以被解释为在VLP上下文中执行KD的更有效方法，其中captioner通过语义丰富的合成标题提取其知识，而过滤器通过去除嘈杂的标题提取其知识。</p><h3 id="2-3-Data-Augmentation"><a href="#2-3-Data-Augmentation" class="headerlink" title="2.3 Data Augmentation"></a>2.3 Data Augmentation</h3><p>虽然数据增强(DA)在计算机视觉中已被广泛采用(Shorten &amp; Khoshgoftaar,2019)，但语言任务的DA并不那么简单。最近，生成语言模型已被用于合成各种NLP任务的示例(Kumar等人,2020;anbytavor et al.,2020;Puri et al.,2020;Yang等人,2020)。不同于这些方法只关注低资源的语言任务，我们的方法展示了合成字幕在大规模视觉语言预训练中的优势。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><p>我们提出了一个统一的VLP框架BLIP，用于从噪声图像-文本对中学习。本节首先介绍我们的新模型架构MED及其预训练目标，然后描述用于数据集引导的CapFilt。</p><h3 id="3-1-Model-Architecture"><a href="#3-1-Model-Architecture" class="headerlink" title="3.1 Model Architecture"></a>3.1 Model Architecture</h3><p>我们使用ViT(Dosovitskiy等人,2021)作为我们的图像编码器，它将输入图像划分为补丁并将其编码为嵌入序列，并使用额外的[CLS]令牌来表示全局图像特征。与使用预训练的目标检测器进行视觉特征提取相比(Chen et al.,2020)，使用ViT更加计算友好，并且已被最新的方法所采用(Li et al.,2021a;Kim et al.,2021)。</p><p>为了预训练具有理解和生成能力的统一模型，我们提出了编码器-解码器的多模态混合(MED)，这是一种多任务模型，可以在三种功能之一中运行:</p><p>(1)单模态编码器，分别对图像和文本进行编码。文本编码器与BERT(Devlin等人,2019)相同，其中在文本输入的开头附加一个[CLS]令牌以总结句子。</p><p>(2)基于图像的文本编码器，通过在文本编码器的每个transformer块的自注意层(SA)和前馈网络(FFN)之间额外插入一层交叉注意层(CA)注入视觉信息。一个特定于任务的[Encode]标记被附加到文本中，[Encode]的输出嵌入被用作图像-文本对的多模态表示。</p><p>(3)基于图像的文本解码器，将基于图像的文本编码器中的双向自注意层替换为因果自注意层。[Decode]标记用于表示序列的开始，序列结束标记用于表示序列的结束。<br>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_16.png)</p><h3 id="3-2-Pre-training-Objectives"><a href="#3-2-Pre-training-Objectives" class="headerlink" title="3.2 Pre-training Objectives"></a>3.2 Pre-training Objectives</h3><p>我们在预训练中共同优化了三个目标，两个基于理解的目标和一个基于生成的目标。每个图像-文本对只需要通过计算量较大的视觉transformer进行一次前向传递，并通过文本转换器进行三次前向传递，其中激活不同的功能以计算如下所述的三种损失。</p><p>图像-文本对比损失(ITC)激活单模态编码器。它的目的是通过鼓励正面的图像-文本对与负面的图像-文本对具有相似的表示来对齐视觉转换器和文本转换器的特征空间。它已被证明是提高视觉和语言理解的有效目标(Radford et al.,2021;Li et al.,2021a)。我们遵循Li等人(2021a)的ITC损失，其中引入动量编码器来产生特征，并从动量编码器创建软标签作为训练目标，以解释负对中的潜在正。</p><p>图像-文本匹配损失(ITM)激活基于图像的文本编码器。它旨在学习图像-文本多模态表示，以捕获视觉和语言之间的<strong>细粒度对齐</strong>。ITM是一个二元分类任务，其中模型使用ITM头(线性层)来预测给定图像-文本对的多模态特征是正的(匹配的)还是负的(不匹配的)。为了找到更多信息丰富的负，我们采用Li等人(2021a)的hard negative挖掘策略，其中批中具有较高对比相似性的负对更有可能被选择来计算损失。</p><p>语言建模损失(LM)激活基于图像的文本解码器，其目的是生成给定图像的文本描述。它优化了交叉熵损失，训练模型以自回归的方式最大化文本的可能性。在计算损失时，我们使用0.1的标签平滑。与广泛用于VLP的MLM损失相比，<strong>LM使具有泛化能力的模型能够将视觉信息转换为连贯的字幕</strong>。</p><p>为了在利用多任务学习的同时执行有效的预训练，<strong>文本编码器和文本解码器共享除SA层之外的所有参数</strong>。原因是编码和解码任务之间的差异最好由SA层捕获。特别是，<strong>编码器使用双向自注意来构建当前输入标记的表示，而解码器使用因果自注意来预测下一个标记</strong>。另一方面，编码和解码任务之间的嵌入层、CA层和FFN的作用相似，因此共享这些层可以提高训练效率，同时受益于多任务学习。</p><h3 id="3-3-CapFilt"><a href="#3-3-CapFilt" class="headerlink" title="3.3 CapFilt"></a>3.3 CapFilt</h3><p>由于标注成本过高，高质量的人工标注图像-文本对(Ih,Th)(I_{h},T_{h})(Ih​,Th​)数量有限(例如COCO (Lin et al.,2014))。近期工作(Li et al.,2021a;Wang等人,2021)使用了大量的图像和替代文本对(Iw,Tw)(I_{w},T_{w})(Iw​,Tw​)，这些图像和文本对是从网络上自动收集的。然而，替代文本通常不能准确地描述图像的视觉内容，使它们成为一个嘈杂的信号，对于学习视觉语言对齐来说是次优的。</p><p>本文提出了一种提高文本语料库质量的新方法CapFilt。图3给出了CapFilt的示例。它引入了两个模块:一个用于生成给定web图像的标题的captioner，以及一个用于去除噪声图像-文本对的过滤器。captioner和filter都是从相同的预训练MED模型初始化的，并在COCO数据集上分别进行微调。调优是一个轻量级的过程。</p><p>具体地说，captioner是一个基于图像的文本解码器。它与LM目标进行了微调，以解码给定图像的文本。给定web图像IwI_{w}Iw​, captioner生成合成字幕t，每个图像一个字幕。该滤波器是一个基于图像的文本编码器。它与ITC和ITM目标进行了微调，以了解文本是否与图像匹配。过滤器去除原始web文本TwT_{w}Tw​和合成文本TsT_{s}Ts​中的噪声文本，如果ITM头预测文本与图像不匹配，则认为文本是噪声文本。最后，我们将过滤后的图像文本对与人工注释的图像文本对结合起来形成一个新的数据集，我们使用它来预训练一个新的模型。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_14.png)</p><h2 id="4-Experiments-and-Discussions"><a href="#4-Experiments-and-Discussions" class="headerlink" title="4. Experiments and Discussions"></a>4. Experiments and Discussions</h2><p>在本节中，我们首先介绍训练前的细节。然后对该方法进行了详细的实验分析。更多的消融实验可以在附录中找到。</p><h3 id="4-1-Pre-training-Details"><a href="#4-1-Pre-training-Details" class="headerlink" title="4.1 Pre-training Details"></a>4.1 Pre-training Details</h3><p>我们的模型是在PyTorch (Paszke et al.,2019)中实现的，并在两个16个gpu节点上进行预训练。图像transformer从ImageNet预训练的ViT初始化(Touvron et al.,2020;Dosovitskiy等人,2021)，文本转换器从BERTbase初始化(Devlin等人,2019)。我们研究了ViT的两种变体:ViT-B&#x2F;16和ViT-L&#x2F;16。除非另有说明，本文中以“BLIP”形式报道的所有结果均使用ViT-B。我们使用2880(ViT-B) &#x2F; 2400(ViT-L)的批大小对模型进行了20个epoch的预训练。我们使用AdamW(Loshchilov &amp; Hutter,2017)优化器，权重衰减为0.05。学习率被预热到3e-4(ViT-B)&#x2F;2e-4(ViT-L)，并以0.85的速率线性衰减。我们在预训练时采用分辨率为224 × 224的随机图像裁剪，在微调时将图像分辨率提高到384 × 384。我们使用与Li等人(2021a)相同的预训练数据集，总共有14M张图像，包括两个人工注释数据集(COCO和Visual Genome (Krishna等人,2017))和三个web数据集(Conceptual Captions (Changpinyo等人,2021)、Conceptual 12M (Changpinyo等人,2021)、SBU Captions (Ordonez等人,2011))。我们还对另外一个网络数据集LAION (Schuhmann et al.,2021)进行了实验，该数据集包含1.15亿张带有更多噪声文本的图像。关于数据集的更多细节可以在附录中找到。</p><h3 id="4-2-Effect-of-CapFilt"><a href="#4-2-Effect-of-CapFilt" class="headerlink" title="4.2 Effect of CapFilt"></a>4.2 Effect of CapFilt</h3><p>在表1中，我们比较了在不同数据集上预训练的模型，以证明CapFilt在下游任务上的有效性，包括图像-文本检索和带有微调和零采样设置的图像cap-tioning。</p><p>当仅将captioner或filter应用于14M张图像的数据集时，可以观察到性能的提高。当一起应用时，它们的效果相互补充，与使用原始嘈杂的网络文本相比，会有实质性的改进。</p><p>CapFilt可以通过更大的数据集和更大的视觉骨干进一步提高性能，这验证了其在数据大小和模型大小方面的可扩展性。此外，通过使用大型captioner和带有ViT-L的filter，也可以提高基本模型的性能。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_1.png)</p><p>在图4中，我们展示了一些示例标题及其相应的图像，定性地演示了标题器生成新文本描述的效果，以及过滤器从原始web文本和合成文本中去除噪声标题的效果。更多的例子可以在附录中找到。<br>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_3.png)</p><h3 id="4-3-Diversity-is-Key-for-Synthetic-Captions"><a href="#4-3-Diversity-is-Key-for-Synthetic-Captions" class="headerlink" title="4.3 Diversity is Key for Synthetic Captions"></a>4.3 Diversity is Key for Synthetic Captions</h3><p>在CapFilt中，我们使用核采样(Holtzman et al.,2020)来生成合成字幕。核采样是一种随机解码方法，其中每个标记从一组累积概率质量超过阈值p的标记中采样。我们的实验表明，p &#x3D;{0.85, 0.9, 0.95}给出了类似的预训练结果，因此我们将p &#x3D; 0.9设置为CapFilt。在表2中，我们将其与波束搜索进行比较，波束搜索是一种确定性解码方法，旨在以最高概率生成字幕。</p><p>核采样导致明显更好的性能，尽管更大的噪声表明，从滤波器的高噪声比。我们假设原因是核采样产生了更多样化和令人惊讶的标题，其中包含了更多的新信息，模型可以从中受益。另一方面，光束搜索倾向于生成数据集中常见的安全标题，因此提供的额外知识较少。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_9.png)</p><h3 id="4-4-Parameter-Sharing-and-Decoupling"><a href="#4-4-Parameter-Sharing-and-Decoupling" class="headerlink" title="4.4 Parameter Sharing and Decoupling"></a>4.4 Parameter Sharing and Decoupling</h3><p>在预训练期间，文本编码器和解码器共享除自关注层以外的所有参数。在表3中，我们评估了使用不同参数共享策略预训练的模型，其中对14M张带有web文本的图像进行预训练。结果表明，与不共享相比，共享除SA之外的所有层可以获得更好的性能，同时也减小了模型大小，从而提高了训练效率。如果SA层是共享的，则由于编码任务和解码任务之间的冲突，会降低模型的性能。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_2.png)</p><p><strong>在CapFilt期间，captioner和filter在COCO上分别进行端到端微调</strong>。在表4中，我们研究了当captioner和filter以与预训练相同的方式共享参数时的效果。在下游任务上的性能下降，这主要归因于确认偏差。由于参数共享，由captioner产生的噪声字幕不太可能被filter滤除，正如较低的噪声比(8%与25%相比)所示。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_7.png)</p><h2 id="5-Comparison-with-State-of-the-arts"><a href="#5-Comparison-with-State-of-the-arts" class="headerlink" title="5. Comparison with State-of-the-arts"></a>5. Comparison with State-of-the-arts</h2><p>在本节中，我们将BLIP与现有的VLP方法在广泛的视觉语言下游任务上进行比较。接下来，我们简要介绍每个任务和调优策略。更多的细节可以在附录中找到。</p><h3 id="5-1-Image-Text-Retrieval"><a href="#5-1-Image-Text-Retrieval" class="headerlink" title="5.1 Image-Text Retrieval"></a>5.1 Image-Text Retrieval</h3><p>我们评估了BLIP在COCO和Flickr30K (Plummer et al.,2015)数据集上的图像到文本检索(TR)和文本到图像检索(IR)。我们使用ITC和ITM损失对预训练模型进行微调。为了实现更快的推理速度，我们遵循Li等人(2021a)的方法，<strong>首先根据图像-文本特征相似度选择k个候选对象，然后根据候选对象的成对ITM分数对其重新排序</strong>。我们为COCO设置k &#x3D; 256，为Flickr30K设置k &#x3D; 128。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_15.png)</p><p>如表5所示，与现有方法相比，BLIP的性能有了较大提升。使用同样的14M张预训练图像，BLIP在COCO上的表现比之前的最佳模型ALBEF平均高出2.7% recall@1。我们还通过将在COCO上调优的模型直接传输到Flickr30K来进行zero-shot检索。结果如表6所示，其中BLIP的性能也大大优于现有方法。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_11.png)</p><h3 id="5-2-Image-Captioning"><a href="#5-2-Image-Captioning" class="headerlink" title="5.2 Image Captioning"></a>5.2 Image Captioning</h3><p>我们考虑了两个用于图像字幕的数据集:NoCaps (Agrawal等人,2019)和COCO，两者都使用在COCO上微调的模型进行评估，其中包含LM损失。与Wang et al.(2021)类似，我们在每个标题的开头添加了一个提示“a picture of”，结果稍好一些。如表7所示，使用14M预训练图像的BLIP大大优于使用类似预训练数据量的方法。拥有1.29亿张图片的BLIP达到了与拥有2亿张图片的LEMON一样的竞争性能。注意，LEMON需要一个计算量大的预训练对象检测器和更高分辨率的输入图像(800×1333)，导致推理时间比使用低分辨率(384×384)输入图像的无检测器BLIP慢得多。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_4.png)</p><h3 id="5-3-Visual-Question-Answering-VQA"><a href="#5-3-Visual-Question-Answering-VQA" class="headerlink" title="5.3 Visual Question Answering (VQA)"></a>5.3 Visual Question Answering (VQA)</h3><p>VQA (Antol et al.,2015)要求模型在给定图像和问题的情况下预测答案。而不是将VQA定义为一个多答案分类任务(Chen et al.,2020;Li et al.,2020)，我们遵循Li et al. (2021a)，并将其视为答案生成任务，从而实现开放式VQA。如图5(a)所示，在微调期间，我们重新排列预训练的模型，其中图像问题首先被编码为多模态嵌入，然后给出答案解码器。VQA模型使用真值答案作为目标，对LM损失进行微调。<br>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_5.png)</p><p>结果如表8所示。使用14M张图像，BLIP在测试集上优于ALBEF +1.64%。使用1.29亿张图像，BLIP的性能优于SimVLM, SimVLM使用了13倍多的预训练数据和更大的视觉主干，并增加了一个卷积阶段。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_12.png)</p><h3 id="5-4-Natural-Language-Visual-Reasoning-NLVR2"><a href="#5-4-Natural-Language-Visual-Reasoning-NLVR2" class="headerlink" title="5.4 Natural Language Visual Reasoning (NLVR2)"></a>5.4 Natural Language Visual Reasoning (NLVR2)</h3><p>NLVR2 (Suhr et al.,2019)要求模型预测一个句子是否描述了一对图像。为了能够对两幅图像进行推理，我们对预训练模型进行了简单的修改，从而获得了比以前的方法更具计算效率的架构(Li et al.,2021a;Wang等人，2021)。如图5(b)所示，对于基于图像的文本编码器中的每个变压器块，存在两个交叉注意层来处理两个输入图像，并将其输出合并并提供给FFN。两个CA层由相同的预训练权值初始化。合并层在编码器的前6层执行简单的平均池化，并在第6-12层执行串联，然后进行线性投影。在[Encode]标记的输出嵌入上应用MLP分类器。如表8所示，除了ALBEF执行额外的定制预训练步骤外，BLIP优于所有现有方法。有趣的是，NLVR2上的性能并没有从额外的web图像中获益，这可能是由于web数据和下游数据之间的域差距。</p><h3 id="5-5-Visual-Dialog-VisDial"><a href="#5-5-Visual-Dialog-VisDial" class="headerlink" title="5.5 Visual Dialog (VisDial)"></a>5.5 Visual Dialog (VisDial)</h3><p>VisDial (Das等人，2017)在自然会话环境中扩展了VQA，其中模型不仅需要基于图像-问题对预测答案，还需要考虑对话历史和图像的标题。我们遵循判别设置，其中模型对候选答案池进行排名(Gan等人,2019;Wang et al.,2020;Murahari et al.,2020)。如图5©所示，我们将图像和标题嵌入连接起来，并通过交叉关注将它们传递给对话编码器。对话编码器使用ITM损失进行训练，以区分问题的答案是真还是假，给定整个日志历史和图像标题嵌入。如表9所示，我们的方法在VisDial v1.0验证集上实现了最先进的性能。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_13.png)</p><h3 id="5-6-Zero-shot-Transfer-to-Video-Language-Tasks"><a href="#5-6-Zero-shot-Transfer-to-Video-Language-Tasks" class="headerlink" title="5.6 Zero-shot Transfer to Video-Language Tasks"></a>5.6 Zero-shot Transfer to Video-Language Tasks</h3><p>我们的图像语言模型对视频语言任务具有很强的泛化能力。在表10和表11中，我们对文本到视频检索和视频问答进行了零射击转移，我们分别直接评估了在coco -检索和VQA上训练的模型。为了处理视频输入，我们对每个视频统一采样n帧(检索n &#x3D; 8, QA n &#x3D; 16)，并将帧特征连接到单个序列中。注意，这个简单的方法忽略了所有时间信息。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_10.png)</p><p>尽管领域差异和缺乏时间建模，我们的模型在两个视频语言任务上都达到了最先进的性能。对于文本到视频的检索，zero-shot BLIP甚至比在recall@1中对目标视频数据集进行微调的模型要好+9.4%。如果BLIP模型用于初始化具有时间建模的视频语言模型(例如，用time-former (Bertasius et al.,2021)替换我们的ViT)并对视频数据进行微调，则可以实现进一步的性能改进。</p><p>![在这里插入图片描述](.&#x2F;2024-07-21-【BLIP】Bootstrapping Language-Image Pre-training&#x2F;img_8.png)</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>我们提出BLIP，这是一个新的VLP框架，在广泛的下游视觉语言任务上具有最先进的性能，包括基于理解和基于生成的任务。BLIP使用从大规模噪声图像-文本对中启动的数据集，通过注入不同的合成字幕和去除噪声字幕来预训练编码器-解码器的多模态混合模型。我们的bootstrapped数据集将被发布，以促进未来的视觉语言研究。</p><p>有几个潜在的方向可以进一步提高BLIP的性能，但由于这些方法的计算成本增加，我们没有在本文中进行探讨:(1)多轮数据集bootstrapping;(2)每张图像生成多个合成字幕，进一步扩大预训练语料库;(3)通过训练多个不同的capators和filters，并在CapFilt中组合它们的作用力来实现模型集成。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】【CLIP】Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/"/>
      <url>/2024/07/16/19_%E3%80%90CLIP%E3%80%91Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/</url>
      
        <content type="html"><![CDATA[<p>CLIP</p><p>总结：与一些自监督的预训练的方法(如MoCo、MAE)相比，CLIP最大的贡献是实现了文本和图像的对齐。</p><ul><li>损失函数<br>和info nce的区别：作用类似，CLIP计算交叉熵，info nce就是计算相似度后计算交叉熵</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch import nn</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def get_logits(image_features, text_features, logit_scale):</span><br><span class="line">    # 计算image_features @ text_features.T相似度矩阵</span><br><span class="line">    logits_per_image = logit_scale * image_features @ text_features.T</span><br><span class="line">    logits_per_text = logit_scale * text_features @ image_features.T</span><br><span class="line">    return logits_per_image, logits_per_text</span><br><span class="line"></span><br><span class="line">def cal_clip_loss(image_features, text_features, logit_scale):</span><br><span class="line">    device = image_features.device</span><br><span class="line">    logits_per_image, logits_per_text = get_logits(image_features, text_features, logit_scale)</span><br><span class="line">    labels = torch.arange(logits_per_image.shape[0], device=device, dtype=torch.long)</span><br><span class="line">    total_loss = (</span><br><span class="line">                         F.cross_entropy(logits_per_image, labels) +</span><br><span class="line">                         F.cross_entropy(logits_per_text, labels)</span><br><span class="line">                 ) / 2</span><br><span class="line"></span><br><span class="line">    return &#123;&quot;contrastive_loss&quot;: total_loss&#125;</span><br><span class="line"></span><br><span class="line"># 实际代码：image_features =encode_image(image, normalize=True)</span><br><span class="line">image_features = torch.randn(32, 768)</span><br><span class="line"># 实际代码：text_features =encode_text(text, normalize=True)</span><br><span class="line">text_features = torch.randn(32, 768)</span><br><span class="line">logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)).exp()</span><br><span class="line">print(cal_clip_loss(image_features, text_features, logit_scale))</span><br></pre></td></tr></table></figure><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限制的监督形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从原始文本中学习图像是一种很有前途的选择，它利用了更广泛的监督来源。我们证明了预测哪个标题与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方法，可以在从互联网收集的4亿对(图像，文本)数据集上从头开始学习SOTA图像表示。在预训练之后，使用自然语言来参考学习到的视觉概念(或描述新的概念)，从而实现模型向下游任务的zero shot转移。我们通过对30多个不同的现有计算机视觉数据集进行基准测试来研究这种方法的性能，这些数据集涵盖了OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务。该模型不平凡地转移到大多数任务，并且通常与完全监督的基线竞争，而不需要任何数据集特定的训练。例如，我们在ImageNet zero-shot上匹配原始ResNet-50的精度，而不需要使用它所训练的128万个训练样本中的任何一个。我们在<a href="https://github.com/OpenAI/CLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/OpenAI/CLIP上发布我们的代码和预训练的模型权重。</a></p><h2 id="1-Introduction-and-Motivating-Work"><a href="#1-Introduction-and-Motivating-Work" class="headerlink" title="1. Introduction and Motivating Work"></a>1. Introduction and Motivating Work</h2><p>在过去几年中，直接从原始文本中学习的预训练方法彻底改变了NLP (Dai &amp; Le, 2015;Peters et al.， 2018;Howard &amp; Ruder, 2018;Rad- ford et al.， 2018;Devlin et al.， 2018;拉斐尔等人，2019)。与任务无关的目标，如自回归和屏蔽语言建模，已经在计算、模型容量和数据方面扩展了许多数量级，稳步提高了能力。“文本到文本”作为标准化输入输出接口的发展(McCann et al.， 2018;Radford等人，2019;rafael等人，2019)使任务无关架构能够zero shot转移到下游数据集，从而消除了对专门输出头或数据集特定定制的需要。像GPT-3这样的旗舰系统(Brown et al.， 2020)现在在使用定制模型的许多任务中具有竞争力，而几乎不需要特定数据集的训练数据。</p><p>这些结果表明，在网络规模的文本集合中，现代预训练方法可获得的总体监督优于高质量的人群标记NLP数据集。然而，在计算机视觉等其他领域，在人群标记数据集(如ImageNet)上预训练模型仍然是标准做法(Deng et al.， 2009)。直接从网络文本中学习的可扩展预训练方法能否在计算机视觉领域取得类似的突破?先前的工作令人鼓舞。</p><p>20多年前，Mori等人(1999)通过训练一个模型来预测与图像配对的文本文档中的名词和形容词，探索了改进基于内容的图像检索。Quattoni等人(2007)证明，通过训练分类器的权重空间中的流形学习，可以学习更有效的数据图像表示，以预测与图像相关的标题中的单词。Srivastava和Salakhutdinov(2012)通过在低级图像和文本标签特征之上训练多模态深度玻尔兹曼机(multimodal deep Boltzmann Machines)来探索深度表示学习。Joulin等人(2016)对这方面的工作进行了现代化，并证明cnn训练用来预测图像标题中的单词可以学习有用的图像表示。他们将YFCC100M数据集中图像的标题、描述和标签元数据(Thomee et al.， 2016)转换为一个词袋多标签分类任务，并表明预训练AlexNet (Krizhevsky et al.， 2012)来预测这些标签学习表征，其表现与基于imagenet的预训练在迁移任务上的表现相似。Li等人(2017)随后将这种方法扩展到预测除了单个单词之外的短语n-gram，并展示了他们的系统zero shot转移到其他图像分类数据集的能力，方法是基于他们的学习视觉n-gram字典对目标类进行评分，并预测得分最高的类别。采用最新的架构和预训练方法，VirTex (Desai &amp; Johnson, 2020)、ICMLM (Bulent Sariyildiz等，2020)和ConVIRT (Zhang等，2020)最近展示了基于transformer的语言建模、掩码语言建模和对比目标从文本中学习图像表示的潜力。</p><p>虽然作为概念证明令人兴奋，但使用自然语言监督进行图像表示学习仍然很少。这可能是因为在通用基准测试上的演示性能比其他方法低得多。例如，Li等人(2017)在zero shot设置下在ImageNet上仅达到11.5%的准确率。这远低于目前技术水平的88.4%的准确率(Xie et al.， 2020)。它甚至低于经典计算机视觉方法的50%准确率(Deng et al.， 2012)。相反，范围更窄但目标更明确的弱监管的使用改善了绩效。Mahajan等人(2018)表明，预测Instagram图像上与imagenet相关的标签是一项有效的预训练任务。当对ImageNet进行微调时，这些预训练模型的准确率提高了5%以上，并改善了当时的整体技术水平。Kolesnikov等人(2019)和Dosovitskiy等人(2020)也通过预训练模型来预测带有噪声标记的JFT-300M数据集的类别，在更广泛的迁移基准集上取得了巨大的收益。</p><p>这条工作路线代表了当前实用主义的中间立场，即从有限数量的监督“黄金标签”和从几乎无限数量的原始文本中学习之间进行学习。然而，这并非没有妥协。两者的作品都经过精心设计，并在工艺限制下，他们的监督类别分别为1000类和18291类。自然语言能够通过其通用性来表达并监督更广泛的视觉概念。这两种方法还使用静态softmax分类器来执行预测，并且缺乏动态输出的机制。这严重削弱了它们的灵活性，限制了它们的“zero shot”能力。</p><p>这些弱监督模型与最近直接从自然语言中学习图像表示的探索之间的一个关键区别是规模。Mahajan等人(2018)和Kolesnikov等人(2019)在数百万到数十亿张图像上训练了他们的模型，而VirTex、ICMLM和ConVIRT在一到二十万张图像上训练了加速器日。在这项工作中，我们缩小了这一差距，并研究了大规模自然语言监督训练的图像分类器的行为。在互联网上大量公开可用的这种形式的数据的支持下，我们创建了一个包含4亿对(图像，文本)的新数据集，并证明了从头开始训练的简化版本的ConVIRT，我们称之为CLIP，即对比语言-图像预训练，是一种从自然语言监督中学习的有效方法。我们通过训练一系列8个模型来研究CLIP的可扩展性，这些模型跨越了近2个数量级的计算，并观察到传输性能是计算的平滑可预测函数(hetness等人，2017;Kaplan et al.， 2020)。我们发现CLIP，类似于GPT家族，在预训练期间学习执行广泛的任务，包括OCR，地理定位，动作识别等。我们通过在30多个现有数据集上对CLIP的零射击转移性能进行基准测试来衡量这一点，并发现它可以与先前的任务特定监督模型相竞争。我们还通过线性探针表示学习分析证实了这些发现，并表明CLIP优于公开可用的最佳ImageNet模型，同时也具有更高的计算效率。我们还发现zero shot CLIP模型比同等精度监督的ImageNet模型更健壮，这表明任务不可知模型的zero shot评估更能代表模型的能力。这些结果具有重要的政策和伦理意义，我们将在第7节中加以考虑。</p><p>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_4.png)</p><h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2. Approach"></a>2. Approach</h2><h2 id="2-1-Natural-Language-Supervision"><a href="#2-1-Natural-Language-Supervision" class="headerlink" title="2.1 Natural Language Supervision"></a>2.1 Natural Language Supervision</h2><p>我们方法的核心思想是从自然语言的监督中学习感知。正如在引言中所讨论的，这并不是一个新想法，但是用于描述这一领域工作的术语是多种多样的，甚至看起来是矛盾的，并且所陈述的动机也是多种多样的。<strong>Zhang等人(2020)、Gomez等人(2017)、Joulin等人(2016)和Desai &amp; Johnson(2020)都引入了从与图像配对的文本中学习视觉表示的方法</strong>，但分别将其方法描述为无监督、自监督、弱监督和监督。</p><p>我们要强调的是，这一行的共同之处并不是使用特定方法的任何细节，而是对自然语言作为训练信号的欣赏。所有这些方法都是从自然语言监督中学习的。尽管早期的工作在使用主题模型和n-gram表示时与自然语言的复杂性作斗争，但深度上下文表示学习的改进表明，我们现在有了有效利用这种丰富的监督来源的工具(McCann等人，2017)。</p><p>与其他训练方法相比，从自然语言中学习有几个<strong>潜在的优势</strong>。与用于图像分类的标准众包标签相比，<strong>扩展自然语言监督要容易得多</strong>，因为它不要求注释采用经典的“机器学习兼容格式”，例如标准的1-of-N多数投票“金标”。相反，研究自然语言的方法可以被动地从互联网上大量文本的监督中学习。与大多数无监督或自监督学习方法相比，<strong>从自然语言中学习也有重要的优势，因为它不仅“只是”学习表征，而且还将该表征与语言联系起来，从而实现灵活的zero shot</strong>。在接下来的小节中，我们将详细介绍我们选定的具体方法。</p><h3 id="2-2-Creating-a-Sufficiently-Large-Dataset"><a href="#2-2-Creating-a-Sufficiently-Large-Dataset" class="headerlink" title="2.2 Creating a Sufficiently Large Dataset"></a>2.2 Creating a Sufficiently Large Dataset</h3><p>现有工作主要使用MS-COCO (Lin et al.， 2014)、Visual Genome (Krishna et al.， 2017)和YFCC100M (Thomee et al.， 2016)三个数据集。虽然MS-COCO和Visual Genome是高质量的人群标记数据集，但按照现代标准，它们的规模很小，每个数据集大约有10万张训练照片。相比之下，其他计算机视觉系统在多达35亿张照片上进行了训练(Mahajan et al.， 2018)。YFCC100M, 1亿张照片，是一个可能的替代方案，但每张图像的元数据稀疏且质量参差不齐。许多图像使用自动生成的文件名，如20160716 113957.JPG作为“标题”或包含相机曝光设置的“描述”。经过过滤，只保留带有自然语言标题和&#x2F;或英文描述的图像后，数据集缩小了6倍，只有1500万张照片。这与ImageNet的大小大致相同。</p><p>自然语言监督的一个主要动机是这种形式的大量数据在互联网上公开可用。由于现有的数据集不能充分反映这种可能性，因此只考虑这些数据集的结果会低估这一研究方向的潜力。为了解决这个问题，我们<strong>构建了一个新的数据集，其中包含4亿对(图像，文本)对</strong>，这些数据来自互联网上各种公开可用的资源。为了尝试覆盖尽可能广泛的视觉概念集，我们搜索(图像，文本)对作为构建过程的一部分，其文本包含500,000个查询集中的一个我们通过每个查询包含多达20,000对(图像、文本)来近似地平衡结果。结果数据集的总字数与用于训练GPT-2的WebText数据集相似。我们<strong>将此数据集称为WebImageText的WIT</strong>。</p><h3 id="2-3-Selecting-an-Efficient-Pre-Training-Method"><a href="#2-3-Selecting-an-Efficient-Pre-Training-Method" class="headerlink" title="2.3 Selecting an Efficient Pre-Training Method"></a>2.3 Selecting an Efficient Pre-Training Method</h3><p>最先进的计算机视觉系统使用了大量的计算。Mahajan等人(2018)需要19个GPU年来训练他们的ResNeXt101-32x48d, Xie等人(2020)需要33个TPUv3核心年来训练他们的Noisy Student EfficientNet-L2。当考虑到这两个系统都被训练为只能预测1000个ImageNet类时，从自然语言中学习一组开放的视觉概念的任务似乎令人望而生畏。在我们的努力过程中，我们发现训练效率是成功扩展自然语言监督的关键，我们基于这个指标选择了最终的预训练方法。</p><p>我们最初的方法类似于VirTex，从头开始联合训练图像CNN和文本转换器来预测图像的标题。然而，我们在有效扩展这种方法时遇到了困难。在图2中，我们展示了一个6300万个参数的transformer语言模型，它已经使用了两倍的ResNet-50图像编码器的计算，学习识别ImageNet类的速度比预测相同文本的单词包编码的简单基线慢三倍。</p><p>这两种方法有一个关键的相似之处。他们试图预测每幅图片的准确文字。这是一项艰巨的任务，因为与图像同时出现的描述、评论和相关文本种类繁多。最近在图像对比表征学习方面的工作发现，<strong>对比目标比其等效的预测目标可以更好地学习表征(Tian et al.， 2019)</strong>。其他研究发现，尽管图像的生成模型可以学习高质量的图像表示，但与具有相同性能的对比模型相比，它们需要超过一个数量级的计算(Chen等人，2020a)。注意到这些发现，我们探索训练一个系统来解决潜在的更容易的代理任务，即只预测哪个文本作为一个整体与哪个图像配对，而不是该文本的确切单词。从相同的词袋编码基线开始，我们将预测目标替换为图2中的对比目标，并观察到零射击转移到ImageNet的效率进一步提高了4倍。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_10.png)<br>给定一批N对(图像、文本)对，CLIP被训练来预测N对×N可能的(图像、文本)对中哪对实际发生。为此，CLIP通过联合训练图像编码器和文本编码器来学习多模态嵌入空间，以最大化批处理中N对真实对的图像和文本嵌入的余弦相似度，同时最小化N2−N对错误对的嵌入的余弦相似度。我们在这些相似性得分上优化对称交叉熵损失。在图3中，我们包含了CLIP实现的核心伪代码。据我们所知，这种批量构建技术和目标首先被引入深度度量学习领域，作为多类n对损失Sohn(2016)，由Oord等人(2018)作为InfoNCE损失推广用于对比表示学习，最近被Zhang等人(2020)用于医学成像领域的对比(文本、图像)表示学习。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_2.png)<br>由于我们的预训练数据集规模很大，过度拟合不是主要问题，并且与Zhang等人(2020)的实现相比，训练CLIP的细节得到了简化。我们从头开始训练CLIP，而不使用ImageNet权重初始化图像编码器或使用预训练权重初始化文本编码器。我们没有使用表征和对比嵌入空间之间的非线性投影，这种变化是由Bachman等人(2019)引入的，并由Chen等人(2020b)推广。我们只使用线性投影将每个编码器的表示映射到多模态嵌入空间。我们没有注意到两个版本之间的训练效率差异，并推测非线性投影可能仅在自监督表示学习方法中与当前图像的细节共同适应。我们还从Zhang等人(2020)中删除了文本转换函数tu，该函数从文本中均匀采样单个句子，因为CLIP预训练数据集中的许多(图像，文本)对仅为单个句子。我们还简化了图像变换函数tv。<strong>从调整大小的图像中随机裁剪是训练期间使用的唯一数据增强</strong>。最后，控制softmax中对数范围的温度参数τ在训练过程中被直接优化为对数参数化的乘法标量，以避免变成超参数。</p><h3 id="2-4-Choosing-and-Scaling-a-Model"><a href="#2-4-Choosing-and-Scaling-a-Model" class="headerlink" title="2.4 Choosing and Scaling a Model"></a>2.4 Choosing and Scaling a Model</h3><p>我们考虑了图像编码器的两种不同架构。首先，我们使用ResNet-50 (He et al.， 2016a)作为图像编码器的基础架构，因为它被广泛采用并且性能成熟。我们使用He等人(2019)的ResNetD改进和Zhang(2019)的抗锯齿rect-2模糊池对原始版本进行了一些修改。我们还将全局平均池化层替换为注意力池化机制。注意力池被实现为一个单层的“transformer-style”多头QKV注意力，其中查询是基于图像的全局平均池表示。对于第二个架构，我们使用最近引入的视觉变压器(ViT)进行实验(Dosovitskiy等人，2020)。我们密切关注它们的实现，仅对transformer之前的combined patch和position embeddings添加了额外的层规范化，并使用略有不同的初始化方案。</p><p>文本编码器是一个Transformer (Vaswani et al.， 2017)，其架构修改在Radford et al.(2019)中描述。作为基本尺寸，我们使用一个具有8个注意头的63m参数的12层512宽模型。转换器在49,152词汇表大小的文本的小写字节对编码(BPE)表示上运行(Sennrich et al.， 2015)。为了计算效率，最大序列长度被限制在76。<strong>文本序列用[SOS]和[EOS]令牌括起来</strong>，<strong>[EOS]令牌处变压器最高层的激活被视为文本的特征表示</strong>，文本被层归一化，然后线性投影到多模态嵌入空间中。在文本编码器中使用了隐藏的自注意，以保留使用预训练的语言模型进行初始化或添加语言建模作为辅助目标的能力，尽管对这方面的探索有待于未来的工作。</p><p>虽然以前的计算机视觉研究通常通过单独增加宽度(Mahajan et al.， 2018)或深度(He et al.， 2016a)来缩放模型，但对于ResNet图像编码器，我们采用了Tan和Le(2019)的方法，该方法发现在所有宽度、深度和分辨率上分配额外的计算比仅将其分配到模型的一个维度更好。虽然Tan和Le(2019)调整了分配给其EfficientNet架构的每个维度的计算比例，但我们使用了一个简单的基线，平均分配额外的计算来增加模型的宽度、深度和分辨率。对于文本编码器，我们只缩放模型的宽度，使其与ResNet宽度的计算增量成正比，而根本不缩放深度，因为我们发现<strong>CLIP的性能对文本编码器的容量不太敏感</strong>。</p><h3 id="2-5-Training"><a href="#2-5-Training" class="headerlink" title="2.5 Training"></a>2.5 Training</h3><p>我们训练了一系列的5个resnet和3个Vision transformer。对于resnet，我们训练了一个ResNet-50，一个ResNet-101，然后还有3个遵循效率网风格的模型缩放，并使用大约4倍，16倍和64倍的ResNet-50计算。分别记为RN50x4、RN50x16、RN50x64。对于视觉变压器，我们训练了一个vitb &#x2F;32，一个vitb &#x2F;16和一个vitl &#x2F;14。我们训练了32个epochs的所有模型。我们将<strong>Adam优化器</strong>(Kingma &amp; Ba, 2014)与d<strong>ecoupled weight decay</strong> regularization(Loshchilov &amp; Hutter, 2017)应用于所有非增益或偏差的权重，并使用余弦调度衰减学习率(Loshchilov &amp; Hutter, 2016)。初始超参数设置使用网格搜索，随机搜索和手动调整基线ResNet50模型的组合，当训练为1 epoch时。然后，由于计算约束，超参数被启发式地适应于更大的模型。<strong>可学习的温度参数τ被初始化为(Wu et al.， 2018)中的0.07，并被截断以防止对数缩放超过100</strong>，我们发现这是<strong>防止训练不稳定所必需的</strong>。我们使用非常大的32,768个小批量。混合精度(Micikevicius et al.， 2017)用于加速训练和节省内存。为了节省额外的内存，梯度检查点(Griewank &amp; Walther, 2000;Chen et al.， 2016)、半精度Adam统计(Dhariwal et al.， 2020)和半精度随机四舍五入文本编码器权重。嵌入相似度的计算也被分片，单个gpu只计算其局部批次嵌入所需的成对相似度的子集。最大的ResNet模型RN50x64在592个V100 gpu上花了18天的时间进行训练，而最大的Vision Transformer在256个V100 gpu上花了12天的时间。对于ViT-L&#x2F;14，我们还以更高的336像素分辨率预训练一个额外的epoch，以提高类似于FixRes的性能(Touvron等人，2019)。我们将此模型记为ViT-L&#x2F;14@336px。除非另有说明，<strong>本文报告的所有结果都是“CLIP”，使用我们发现表现最好的模型</strong>。</p><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-Zero-Shot-Transfer"><a href="#3-1-Zero-Shot-Transfer" class="headerlink" title="3.1 Zero-Shot Transfer"></a>3.1 Zero-Shot Transfer</h3><h4 id="3-1-1-MOTIVATION"><a href="#3-1-1-MOTIVATION" class="headerlink" title="3.1.1 MOTIVATION"></a>3.1.1 MOTIVATION</h4><p>在计算机视觉中，zero shot学习通常是指对图像分类中未见过的物体类别进行泛化的研究(Lampert et al.， 2009)。相反，我们在更广泛的意义上使用这个术语，并研究对未见过的数据集的泛化。我们将其作为执行看不见的任务的代理，正如Larochelle等人(2008)在零数据学习论文中所期望的那样。虽然无监督学习领域的许多研究都集中在机器学习系统的表示学习能力上，但我们鼓励研究零迁移作为衡量机器学习系统任务学习能力的一种方法。在此视图中，数据集评估特定分布上任务的性能。然而，许多流行的计算机视觉数据集是由研究界创建的，主要是作为指导通用图像分类方法开发的基准，而不是衡量特定任务的性能。虽然可以合理地说SVHN数据集测量了谷歌街景照片分布上的街道号码转录任务，但尚不清楚CIFAR-10数据集测量的是什么“真正的”任务。然而，很明显，CIFAR-10是从TinyImages中提取的分布(Torralba et al.， 2008)。在这类数据集上，零概率迁移更多的是评价CLIP对分布偏移和领域泛化的鲁棒性，而不是任务泛化。请参见第3.3节对此进行分析。</p><p>据我们所知，Visual N-Grams (Li et al.， 2017)首先以上述方式研究了零镜头转移到现有图像分类数据集的方法。这也是我们所知道的唯一一项使用一般预训练模型研究零射击转移到标准图像分类数据集的工作，并作为上下文化CLIP的最佳参考点。他们的方法学习了一个包含142,806个视觉n-gram(跨越1到5克)的字典的参数，并使用Jelinek-Mercer平滑的不同版本来优化这些n-gram，以最大化给定图像中所有文本n-gram的概率。为了执行zero shot转移，他们首先将数据集的每个类名的文本转换为其n-gram表示，然后根据他们的模型计算其概率，预测得分最高的一个。</p><p>我们之所以关注zero shot作为任务学习的一种评价，是因为受到了NLP领域中任务学习研究的启发。据我们所知，Liu等人(2018)首先将任务学习确定为一种“意想不到的副作用”，即训练生成维基百科文章的语言模型学会了在语言之间可靠地音译名称。虽然GPT-1 (Radford et al.， 2018)专注于将预训练作为一种迁移学习方法来改进监督微调，但它还包括一项烧烧研究，该研究表明，四种启发式zero shot迁移方法的性能在预训练过程中稳步提高，无需任何监督适应。该分析是GPT-2 (Radford et al.， 2019)的基础，GPT-2专注于通过zero shot研究语言模型的任务学习能力。</p><h4 id="3-1-2-USING-CLIP-FOR-ZERO-SHOT-TRANSFER"><a href="#3-1-2-USING-CLIP-FOR-ZERO-SHOT-TRANSFER" class="headerlink" title="3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER"></a>3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER</h4><p>CLIP被预先训练来预测图像和文本片段是否在其数据集中配对在一起。为了执行zero shot分类，我们重用此功能。对于每个数据集，我们使用数据集中所有类的名称作为潜在文本配对的集合，并根据CLIP预测最可能的(图像，文本)配对。更详细地说，我们首先通过各自的编码器计算图像的特征嵌入和可能文本集的特征嵌入。然后计算这些嵌入的余弦相似度，通过温度参数τ缩放，并通过softmax归一化为概率分布。请注意，该预测层是一个多项逻辑回归分类器，具有l2归一化输入、l2归一化权重、无偏差和温度缩放。当以这种方式解释时，图像编码器是计算图像特征表示的计算机视觉骨干，文本编码器是一个超网络(Ha et al.， 2016)，它根据指定类所代表的视觉概念的文本生成线性分类器的权重。Lei Ba等人(2015)首先引入了这种形式的零采样图像分类器，而从自然语言生成分类器的想法至少可以追溯到Elhoseiny等人(2013)。继续这种解释，CLIP预训练的每一步都可以被视为优化随机创建的代理到计算机视觉数据集的性能，该数据集每个类包含1个示例，通过自然语言描述定义了32,768个类。对于zero shot评估，一旦文本编码器计算出zero shot分类器，我们就缓存它，并在所有后续预测中重用它。这允许在数据集中的所有预测中平摊生成它的成本。</p><h4 id="3-1-3-INITIAL-COMPARISON-TO-VISUAL-N-GRAMS"><a href="#3-1-3-INITIAL-COMPARISON-TO-VISUAL-N-GRAMS" class="headerlink" title="3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS"></a>3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS</h4><p>和基于概率统计的方法比较<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_1.png)</p><h4 id="3-1-4-PROMPT-ENGINEERING-AND-ENSEMBLING"><a href="#3-1-4-PROMPT-ENGINEERING-AND-ENSEMBLING" class="headerlink" title="3.1.4 PROMPT ENGINEERING AND ENSEMBLING"></a>3.1.4 PROMPT ENGINEERING AND ENSEMBLING</h4><p>大多数标准图像分类数据集都将命名或描述类的信息视为事后的想法，这使得基于自然语言的零采样传输成为可能。绝大多数数据集仅使用标签的数字id对图像进行注释，并包含将这些id映射回其英文名称的文件。有些数据集，如Flowers102和GTSRB，在其发布版本中似乎根本不包括这种映射，从而完全阻止了zero shot传输对于许多数据集，我们观察到这些标签的选择可能有些随意，并且没有预料到与zero shot转移相关的问题，zero shot转移依赖于任务描述以成功转移。</p><p>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_11.png)<br>一个常见的问题是一词多义。当类的名称是提供给CLIP的文本编码器的唯一信息时，由于缺乏上下文，它无法区分指的是哪个词义。在某些情况下，同一个单词的多个含义可能会作为不同的类包含在同一个数据集中!这发生在ImageNet中，它包含cranes和cranes that fly。另一个例子是在Oxford-IIIT Pet数据集的类中发现的，从上下文来看，单词boxer显然指的是一种狗，但对于缺乏上下文的文本编码器来说，它很可能指的是一种运动员。</p><p>我们遇到的另一个问题是，在我们的预训练数据集中，与图像配对的文本只有一个单词是相对罕见的。通常文本是一个完整的句子，以某种方式描述图像。为了帮助弥合这种分布差距，我们发现使用提示模板“A photo of A {label}.”是一个很好的默认值，可以帮助指定文本是关于图像的内容。与仅使用标签文本的基线相比，这通常可以提高性能。例如，仅仅使用这个提示符就可以将ImageNet上的准确率提高1.3%。</p><p>类似于围绕GPT3的“提示工程”讨论(Brown et al.， 2020;Gao et al.， 2020)，我们还观察到，通过为每个任务定制提示文本可以显著提高零射击性能。下面是一些不详尽的例子。我们在几个细粒度的图像分类数据集中发现，它有助于指定类别。例如，在Oxford-IIIT Pets中，使用“A photo of a {label}, a type of pet.”。来帮助提供背景，效果很好。同样，在Food101上指定一种食物类型和在FGVC飞机上指定一种飞机类型也有所帮助。对于OCR数据集，我们发现在要识别的文本或数字周围加上引号可以提高性能。最后，我们发现在卫星图像分类数据集中，它有助于指定图像是这种形式，我们使用“a satellite photo of a {label}.”的变体。</p><p>我们还尝试了在多个零射击分类器上进行集成，作为提高性能的另一种方法。这些分类器是通过使用不同的上下文提示来计算的，例如‘A photo of a big {label}”和“A photo of a small {label}”。我们<strong>在嵌入空间而不是概率空间上构造集成</strong>。这允许我们缓存一组平均文本嵌入，这样当在许多预测上平摊时，集成的计算成本与使用单个分类器相同。我们已经观察到在许多生成的zero shot分类器上进行集成，以可靠地提高性能，并将其用于大多数数据集。在ImageNet上，我们集成了80个不同的上下文提示，这比上面讨论的单个默认提示额外提高了3.5%的性能。当一起考虑时，prompt工程和集成可以将ImageNet的精度提高近5%。在图4中，我们可视化了与Li等人(2017)所做的直接嵌入类名的无上下文基线方法相比，快速工程和集成如何改变一组CLIP模型的性能。</p><h4 id="3-1-5-ANALYSIS-OF-ZERO-SHOT-CLIP-PERFORMANCE"><a href="#3-1-5-ANALYSIS-OF-ZERO-SHOT-CLIP-PERFORMANCE" class="headerlink" title="3.1.5 ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE"></a>3.1.5 ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE</h4><p>由于计算机视觉的任务无关的零射击分类器已经得到了充分的研究，CLIP提供了一个有希望的机会来更好地理解这种类型的模型。在本节中，我们对CLIP的zero shot分类器的各种属性进行了研究。作为第一个问题，我们简单地看看零命中率分类器的表现如何。为了说明这一点，我们比较了一个简单的现成基线的性能:在规范ResNet-50的特征上拟合一个完全监督的、正则化的逻辑回归分类器。在图5中，我们展示了跨27个数据集的比较。有关数据集和设置的详细信息，请参见附录A。(<strong>这里的resnet的checkpoint应该是再ImageNet上的，比较有点不公平</strong>)<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_12.png)<br>Zero-shot CLIP在27个数据集中的16个数据集上的表现略优于此基线。查看单个数据集会发现一些有趣的行为。在细粒度分类任务上，我们观察到性能上的广泛差异。在其中两个数据集(Stanford Cars和Food101)上，零射击CLIP在ResNet-50特征上的表现比逻辑回归好20%以上，而在另外两个数据集(Flowers102和fgvc - aircraft)上，零射击CLIP的表现比逻辑回归差10%以上。在OxfordPets和Birdsnap上，二者的表现更为接近。我们怀疑这些差异主要是由于WIT和ImageNet之间每个任务监督的数量不同。在ImageNet、CIFAR10&#x2F;100、STL10和PascalVOC2007等“一般”对象分类数据集上，性能相对相似，在所有情况下，零射击CLIP都有轻微的优势。在STL10上，CLIP总体达到99.3%，尽管没有使用任何训练样例，但这似乎是一个新的SOTA。Zeroshot CLIP在视频中测量动作识别的两个数据集上显着优于ResNet-50。在Kinetics700上，CLIP的表现比ResNet-50高出14.5%。在UCF101上，Zeroshot CLIP的性能也比ResNet-50的性能高出7.7%。我们推测这是由于与ImageNet中以名词为中心的对象监督相比，自然语言对涉及动词的视觉概念提供了更广泛的监督。（表现不一致的原因值得分析一下）</p><p>看看零射击CLIP明显表现不佳的地方，我们看到零射击CLIP在几个专门的、复杂的或抽象的任务上相当弱，比如卫星图像分类(EuroSAT和RESISC45)、淋巴结肿瘤检测(PatchCamelyon)、合成场景中的物体计数(CLEVRCounts)、自动驾驶相关的任务，比如德国交通标志识别(GTSRB)、识别到最近的汽车的距离(KITTI distance)。这些结果突出了零射击CLIP在更复杂任务中的较差能力。相比之下，非专业的人类可以稳健地执行其中的一些任务，如计数、卫星图像分类和交通标志识别，这表明还有很大的改进空间。然而，我们需要提醒的是，目前尚不清楚对于学习者之前没有经验的困难任务，如几乎所有人的淋巴结肿瘤分类(可能还有CLIP)，测量零射击转移(而不是少射击转移)是否是有意义的评估。</p><p>虽然将零射击性能与完全监督模型进行比较可以将CLIP的任务学习能力置于上下文环境中，但与少射击方法进行比较是更直接的比较，因为零射击是其极限。在图6中，我们可视化了在许多图像模型(包括最好的公开可用的ImageNet模型、自监督学习方法和CLIP本身)的特征上，零采样CLIP与少采样逻辑回归的比较。虽然直观地认为零射击不如一射击，但我们发现零射击CLIP在相同的特征空间上与四射击逻辑回归的性能相匹配。这可能是由于零射击和少射击方法之间的重要区别。首先，CLIP的零射击分类器是通过自然语言生成的，它允许直接指定视觉概念(“交流”)。相比之下，“正常的”监督学习必须间接地从训练样本中推断概念。无上下文的基于实例的学习有一个缺点，即许多不同的假设可以与数据一致，特别是在一次性案例中。一张图片通常包含许多不同的视觉概念。虽然一个有能力的学习者能够利用视觉线索和启发式，比如假设被演示的概念是图像中的主要对象，但这并不能保证。</p><p>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_3.png)<br>zero-shot和few-shot性能之间这种差异的一个潜在解决方案是使用CLIP的zero-shot分类器作为few-shot分类器权重的先验。虽然对生成的权重添加L2惩罚是这个想法的直接实现，但我们发现超参数优化通常会选择这个正则器的如此大的值，以至于得到的少射分类器“只是”零射分类器。研究将零弹迁移的强度与少弹学习的灵活性更好地结合起来的方法是未来工作的一个有希望的方向。</p><p>当在其他模型的特征上比较零射击CLIP和少射击逻辑回归时，零射击CLIP的性能大致与我们的评估套件中表现最好的16射击分类器的性能相匹配，该评估套件使用了在ImageNet-21K上训练的BiT-M ResNet-152x2的特征。我们确信，在JFT-300M上训练的BiT-L模型会表现得更好，但这些模型尚未公开发布。BiT-M ResNet-152x2在16次射击设置中表现最好，这有点令人惊讶，因为正如第3.2节所分析的那样，Noisy Student EfficientNet-L2在27个数据集的完全监督设置中平均比它高出近5%。</p><p>除了研究零射击CLIP和少射击逻辑回归的平均性能外，我们还研究了单个数据集的性能。在图7中，我们显示了相同特征空间上的逻辑回归分类器需要匹配零采样CLIP性能的每个类的标记示例数量的估计。由于零弹CLIP也是一个线性分类器，这估计了在这种设置下零弹传输的有效数据效率。为了避免训练数千个线性分类器，我们基于1、2、4、8、16个镜头(如果可能的话)的性能的对数插值和在每个数据集上训练的完全监督线性分类器来估计有效的数据效率。我们发现，在每个数据集上，从每个类不到1个标记样本到184个标记样本的效率差异很大。Flowers102和EuroSAT这两个数据集的表现不如一次性模型。一半的数据集需要每个类少于5个样本，中位数为5.4。然而，平均估计的数据效率是每个类20.8个样本。这是由于20%的数据集中，监督分类器为了匹配性能，每个类需要许多标记的示例。在ImageNet上，零射击CLIP与在相同特征空间上训练的16射击线性分类器的性能相匹配。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_5.png)<br>如果我们假设评估数据集足够大，可以很好地估计在其上训练的线性分类器的参数，那么，由于CLIP的零射击分类器也是一个线性分类器，因此完全监督分类器的性能大致设置了零射击转移可以实现的上界。在图8中，我们将CLIP的零射击性能与跨数据集的完全监督线性分类器进行了比较。虚线，y &#x3D; x表示一个“最优”的零射击分类器，其性能与完全监督的等效分类器相匹配。对于大多数数据集，零射击分类器的性能仍然比完全监督分类器差10%到25%，这表明CLIP的任务学习和零射击转移能力仍有很大的提升空间。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_13.png)<br>零射击性能与完全监督性能之间存在0.82 (p值&lt; 10−6)的正相关，这表明CLIP在将底层表征和任务学习与零射击迁移联系起来方面是相对一致的。然而，零射击CLIP仅在5个数据集上接近完全监督性能:STL10, CIFAR10, Food101, OxfordPets和Caltech101。在所有5个数据集上，零射击准确率和完全监督准确率都超过90%。这表明CLIP在零射击转移任务中可能更有效，因为它的底层表征也是高质量的。预测零投性能的线性回归模型的斜率作为完全监督性能的函数估计，在完全监督性能上每提高1%，零投性能就提高1.28%。然而，第95百分位置信区间仍然包括小于1的值(0.93-1.79)。</p><p>在过去的几年里，深度学习系统的实证研究已经证明，性能是可预测的，是训练计算和数据集大小等重要数量的函数(hetness等人，2017;Kaplan et al.， 2020)。到目前为止，GPT系列模型已经在训练计算中增加了1000倍，从而在零射击性能方面取得了一致的改进。在图9中，我们检查CLIP的零射击性能是否遵循类似的缩放模式。我们绘制了5个ResNet CLIP模型在36个不同数据集上的39个评估中的平均错误率，并发现CLIP在模型计算增加44倍的情况下具有类似的对数-对数线性缩放趋势。虽然总体趋势是平稳的，但我们发现，个人评估的表现可能会更加嘈杂。我们不确定这是由于子任务上的单个训练运行之间的高差异(如D ‘Amour等人(2020)所述)掩盖了稳步提高的趋势，还是由于性能实际上是非单调的，作为某些任务的计算函数。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_8.png)</p><h3 id="3-2-Representation-Learning"><a href="#3-2-Representation-Learning" class="headerlink" title="3.2 Representation Learning"></a>3.2 Representation Learning</h3><p>虽然我们在上一节中通过零射击迁移广泛分析了CLIP的任务学习能力，但更常见的是研究模型的表示学习能力。有许多方法可以评估表征的质量，以及对“理想”表征应该具有哪些属性的分歧(Locatello et al.， 2020)。在从模型中提取的表示上拟合线性分类器并测量其在各种数据集上的性能是一种常见的方法。另一种方法是测量模型端到端微调的性能。这增加了灵活性，先前的工作已经令人信服地证明，在大多数图像分类数据集上，微调优于线性分类(Kornblith等人，2019;翟等人，2019)。虽然由于实际原因，微调的高性能激发了它的研究，但出于几个原因，我们仍然选择基于线性分类器的评估。我们的工作重点是开发一种高性能的任务和数据集不可知的预训练方法。微调，因为它在微调阶段适应每个数据集的表示，可以补偿并潜在地掩盖在预训练阶段学习通用和鲁棒表示的失败。线性分类器，由于其有限的灵活性，而是强调这些失败，并在开发过程中提供明确的反馈。对于CLIP，训练监督线性分类器还有一个额外的好处，它与用于零射击分类器的方法非常相似，可以在3.1节中进行广泛的比较和分析。最后，我们的目标是将CLIP与跨许多任务的一组全面的现有模型进行比较。在27个不同的数据集上研究66个不同的模型需要调整1782个不同的评估。 微调打开了一个更大的设计和超参数空间，这使得很难公平评估和计算昂贵，以比较其他大规模实证研究中讨论的各种技术(Lucic等人，2018;Choi et al., 2019).相比之下，线性分类器需要最小的超参数调优，并且具有标准化的实现和评估过程。有关评估的进一步详情，请参阅附录A。</p><p>图10总结了我们的发现。为了尽量减少可能引起确认或报告偏差担忧的选择效应，我们首先研究了Kornblith等人(2019)的12个数据集评估套件的性能。虽然像ResNet-50和ResNet-101这样的小型CLIP模型比在ImageNet-1K (BiT-S和原件)上训练的其他resnet表现更好，但它们比在ImageNet-21K (BiTM)上训练的resnet表现更差。这些小型CLIP模型在具有类似计算需求的情况下，也不如EfficientNet系列中的模型。然而，使用CLIP尺度训练的模型非常好，我们训练的最大模型(ResNet-50x64)在总体得分和计算效率方面略优于表现最好的现有模型(Noisy Student EfficientNet- L2)。我们还发现CLIP视觉变压器的计算效率比CLIP ResNets高3倍，这使我们能够在我们的计算预算内达到更高的整体性能。这些结果定性地重复了Dosovitskiy等人(2020)的研究结果，该研究报告称，在足够大的数据集上训练时，视觉转换器比convnets的计算效率更高。我们最好的整体模型是ViT-L&#x2F;14，它在我们的数据集上以336像素的更高分辨率进行了一次额外的epoch微调。该模型比该评估套件中最好的现有模型平均高出2.6%。<br>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_9.png)<br>如图21定性所示，CLIP模型学习的任务比之前在随机初始化训练的单个计算机视觉模型中所展示的更广泛。这些任务包括地理定位、光学字符识别、面部情感识别和动作识别。Kornblith等人(2019)的评估套件中没有测量这些任务。这可以被认为是Kornblith等人(2019)对与ImageNet重叠的任务的研究中的一种选择偏差。为了解决这个问题，我们还在更广泛的27个数据集评估套件上测量性能。该评估套件(详见附录A)包括代表上述任务的数据集、德国交通标志识别基准(Stallkamp等人，2011)，以及改编自VTAB的其他几个数据集(Zhai等人，2019)。</p><p>略</p><h2 id="4-Comparison-to-Human-Performance"><a href="#4-Comparison-to-Human-Performance" class="headerlink" title="4. Comparison to Human Performance"></a>4. Comparison to Human Performance</h2><p>CLIP与人类表现和人类学习相比如何?为了更好地了解人类在与CLIP类似的评估设置中的表现，我们在一项任务中对人类进行了评估。我们想要了解人类在这些任务中的零射击表现有多强，以及如果向他们展示一两个图像样本，人类的表现会提高多少。这可以帮助我们比较人类和CLIP的任务难度，并确定它们之间的相关性和差异。</p><p>我们让五个不同的人查看牛津IIT宠物数据集(Parkhi等人，2012)的3669张图片中的每一张，并从37个猫或狗品种中选择最匹配图像的哪一个(或者“我不知道”，如果他们完全不确定)。在零命中率的情况下，人类没有得到任何品种的例子，并要求他们在没有互联网搜索的情况下尽其所能给它们贴上标签。在一次实验中，每个品种给人类一张样本图像，在两次实验中，每个品种给人类两张样本图像。</p><p>一个可能的担忧是，人类工人在零射击任务中没有足够的动力。人类在STL-10数据集上的准确率高达94% (Coates et al.， 2011)，在注意力检查图像子集上的准确率为97-100%，这增加了我们对人类工作人员的信任。</p><p>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_7.png)<br>有趣的是，每个类只有一个训练样例，人类的平均性能从54%提高到76%，而额外的训练样例的边际收益是最小的。准确度从0到1的提高几乎完全是基于人类不确定的图像。这表明人类“知道他们不知道什么”，并且能够根据一个例子更新他们对最不确定的图像的先验。鉴于此，CLIP似乎是一种很有前途的零射击性能训练策略(图5)，并且在自然分布转移测试中表现良好(图13)，但人类从少数例子中学习的方式与本文中使用的少数射击方法之间存在很大差异。</p><p>这表明，如Lake等人(2016)等人所指出的，仍有算法改进有待改进，以缩小机器和人类样本效率之间的差距。由于CLIP的这些小镜头评估并没有有效地利用先验知识，而人类却利用了先验知识，因此我们推测，寻找一种将先验知识适当地整合到小镜头学习中的方法是改进CLIP算法的重要一步。据我们所知，在高质量预训练模型的特征之上使用线性分类器对于少镜头学习来说是最先进的(Tian et al.， 2020)，这表明最好的少镜头机器学习方法与人类少镜头学习之间存在差距。</p><p>如果我们将人类的精度与CLIP的零射击精度进行对比(图16)，我们会发现CLIP最难的问题对人类来说也很难。在某种程度上，误差是一致的，我们的假设是，这是由于至少两个因素:数据集中的噪声(包括错误标记的图像)和超出分布的图像对人类和模型来说都很困难。</p><p>![在这里插入图片描述](.&#x2F;2024-07-16-【CLIP】Learning Transferable Visual Models From Natural Language Supervision&#x2F;img_6.png)<br>略</p><h2 id="6-Limitations"><a href="#6-Limitations" class="headerlink" title="6. Limitations"></a>6. Limitations</h2><p>CLIP仍然有许多限制。虽然其中一些将在各个部分中作为分析的一部分进行讨论，但我们在这里总结并收集它们。</p><p>在具有训练分割的数据集上，zero-shot CLIP的性能平均可与基于ResNet-50特征的线性分类器的简单监督基线相媲美。在大多数这些数据集上，该基线的性能现在远低于整体水平。在提高CLIP的任务学习和迁移能力方面还需要做大量的工作。虽然到目前为止，扩展已经稳步提高了性能，并为持续改进提供了一条途径，但我们估计，零射击CLIP要达到最先进的整体性能，需要大约1000倍的计算能力增加。用目前的硬件进行训练是不可行的。进一步研究提高CLIP的计算和数据效率是必要的。（算力要求高）</p><p>在3.1节的分析中发现，CLIP的zero shot性能在几种任务上仍然很弱。与特定任务模型相比，CLIP在几种类型的细粒度分类(如区分汽车模型、花卉种类和飞机变体)上的性能较差。CLIP还在处理更抽象和系统的任务，如计算图像中物体的数量。最后，对于不太可能包含在CLIP预训练数据集中的新任务，例如对照片中最近的汽车的距离进行分类，CLIP的性能可以接近随机。我们相信，在许多任务中，CLIP的零射击性能接近随机水平。</p><p>尽管在3.3节中研究了零射击CLIP对许多自然图像分布的泛化效果很好，但我们已经观察到零射击CLIP对真正不在其分布范围内的数据的泛化效果仍然很差。在附录e中报告的OCR任务中出现了一个说明性的测试示例。CLIP学习了高质量的语义OCR表示，该表示在数字渲染文本上表现良好，这在其预训练数据集中很常见，在渲染SST2上的性能证明了这一点。然而，CLIP在mnist的手写数字上只能达到88%的准确率。对原始像素进行简单的逻辑回归基线，其性能优于零射击CLIP。语义检索和近重复最近邻检索都验证了在我们的预训练数据集中几乎没有与MNIST数字相似的图像。这表明CLIP在解决深度学习模型的脆性泛化的潜在问题方面做得很少。相反，CLIP试图规避这个问题，并希望通过在如此庞大和多样化的数据集上进行训练，所有数据都将有效地分布。这是一个天真的假设，正如MNIST所证明的那样，很容易被违背。</p><p>尽管CLIP可以灵活地为各种各样的任务和数据集生成zero shot分类器，但CLIP仍然仅限于从给定zero shot分类器中的那些概念中进行选择。与真正灵活的方法(如图像字幕)相比，这是一个重大的限制，因为图像字幕可以产生新颖的输出。不幸的是，如2.3节所述，我们发现我们尝试的图像标题基线的计算效率远低于CLIP。一个值得尝试的简单想法是联合训练一个对比和生成目标，希望将CLIP的效率与标题模型的灵活性结合起来。作为另一种替代方案，可以在给定图像的许多自然语言解释的推理时间执行搜索，类似于使用潜在语言学习Andreas等人(2017)中提出的方法。（就是说CLIP不能够做生成）</p><p>CLIP也没有解决深度学习的数据效率差的问题。相反，CLIP通过使用一个可以扩展到数亿个训练示例的监督源来进行补偿。如果在CLIP模型的训练过程中看到的每张图像都以每秒一张的速度呈现，那么在32个训练时代中看到的128亿张图像将需要405年的时间来迭代。结合CLIP与自我监督(Henaff, 2020;Chen et al.， 2020c)和自我训练(Lee;Xie等人，2020)方法是一个有前途的方向，因为它们证明了比标准监督学习提高数据效率的能力。</p><p>我们的方法有几个明显的局限性。尽管我们关注的是zero shot转移，但我们在完整验证集上反复查询性能，以指导CLIP的开发。这些验证集通常有数千个示例，这对于真正的zero shot场景来说是不现实的。在半监督学习领域也提出了类似的担忧(Oliver et al.， 2018)。另一个潜在的问题是我们对评估数据集的选择。虽然我们已经报告了Kornblith等人(2019)的12个数据集评估套件作为标准化集合的结果，但我们的主要结果使用了27个数据集的随机集合，不可否认，这些数据集与CLIP的开发和能力是共同适应的。创建一个新的任务基准，明确地评估广泛的zero shot转移能力，而不是重新使用现有的监督数据集，将有助于解决这些问题。（测试基准缺乏）</p><p>CLIP是通过与互联网上的图像配对的文本进行训练的。这些图像-文本对未经过滤和管理，导致CLIP模型学习许多社会偏见。之前已经在图像标题模型中证明了这一点(Bhargava &amp; Forsyth, 2019)。我们请读者参阅第7节，了解针对CLIP的这些行为的详细分析和量化，以及对潜在缓解策略的讨论。（数据不干净）</p><p>虽然我们在整个工作中强调，通过自然语言指定图像分类器是一种灵活和通用的接口，但它有自己的局限性。许多复杂的任务和视觉概念很难通过文本来指定。实际的训练示例无疑是有用的，但CLIP并不直接优化few shot性能。在我们的工作中，我们回归到在CLIP的特征之上拟合线性分类器。这将导致从零射击过渡到少射击设置时性能的反直觉下降。正如第4节所讨论的，这与人类的表现明显不同，后者显示了从zero到one shot设置的大幅增加。未来的工作需要开发将CLIP强大的zero shot性能与有效的few shot学习相结合的方法。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】GWC-Net代码阅读</title>
      <link href="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/"/>
      <url>/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>代码中关于网络定义的部分其实还是挺好懂的，所以本文的重点是<strong>cancatenation volume</strong>和<strong>group-wise volume</strong>这部分（建议没有阅读过论文的阅读论文后再看）</p><hr><h2 id="1-Cost-volume的构建"><a href="#1-Cost-volume的构建" class="headerlink" title="1. Cost volume的构建"></a>1. Cost volume的构建</h2><h3 id="1-1-在stereo-matching中，构建cost-volume的步骤："><a href="#1-1-在stereo-matching中，构建cost-volume的步骤：" class="headerlink" title="1.1 在stereo matching中，构建cost volume的步骤："></a>1.1 在stereo matching中，构建cost volume的步骤：</h3><p>1）确定一系列离散的假设视差平面<br>2）将提取的每个视图的 2D 特征warp到假设平面<br>3）最后融合在一起构建 3D 或 4D cost volume</p><h3 id="1-2-双目深度中的warp定义："><a href="#1-2-双目深度中的warp定义：" class="headerlink" title="1.2 双目深度中的warp定义："></a>1.2 双目深度中的warp定义：</h3><p>由于左右图像都经过校正，坐标映射由x轴方向的偏移量决定：<br><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_2.png" alt="在这里插入图片描述"><br>其中 Cr(d) 是指视差 d 处右视图的变换 x 轴坐标，而Xl是左视图的源 x 轴坐标。简单地就是说将右图沿x轴移动不同的视差d。</p><hr><h2 id="2-GWC网络结构"><a href="#2-GWC网络结构" class="headerlink" title="2. GWC网络结构"></a>2. GWC网络结构</h2><p><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_1.png" alt="在这里插入图片描述"><br>看不懂的先看论文</p><hr><h2 id="3-Cost-volume"><a href="#3-Cost-volume" class="headerlink" title="3. Cost volume"></a>3. Cost volume</h2><p><strong>group-wise volume</strong>提供了特征向量匹配的相似度，而<strong>concat volume</strong>提供了语义信息作为补充，两者是互补的</p><p>下面结合代码理解<strong>concat volume</strong>和<strong>group-wise volume</strong>的具体构建</p><h3 id="3-1-Feature-volume"><a href="#3-1-Feature-volume" class="headerlink" title="3.1 Feature volume"></a>3.1 Feature volume</h3><p><strong>Feature volume</strong>就是网络结构图中绿色的部分，可以看到它是将三个特征concat起来得到的，这三个特征由三个特征提取块（由卷积层构成）得到，详细见代码中feature_extraction类。</p><p>这里主要看这三个输出（l2,l3,l4）的维度，调用size方法可以看到它们都是四维的：B x C x H x W，即：Batch_size x 通道数 x 高 x 宽。然后在C这个维度将它们concat得到feature volume，维度及大小已经在下面的代码中注释。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    x = self.firstconv(x)</span><br><span class="line">    x = self.layer1(x)</span><br><span class="line">    l2 = self.layer2(x)</span><br><span class="line">    l3 = self.layer3(l2)</span><br><span class="line">    l4 = self.layer4(l3)</span><br><span class="line"></span><br><span class="line">    # Batch_size * 通道数 * 高 * 宽</span><br><span class="line">    # l2： &lt; class &#x27;torch.Tensor&#x27;&gt; torch.Size([1, 64, 96, 312])</span><br><span class="line">    # l3： &lt; class &#x27;torch.Tensor&#x27;&gt; torch.Size([1, 128, 96, 312])</span><br><span class="line">    # l4： &lt; class &#x27;torch.Tensor&#x27;&gt; torch.Size([1, 128, 96, 312])</span><br><span class="line"></span><br><span class="line">    gwc_feature = torch.cat((l2, l3, l4), dim=1)</span><br><span class="line">    # gwc_feature: &lt; class &#x27;torch.Tensor&#x27;&gt; torch.Size([1, 320, 96, 312])</span><br><span class="line"></span><br><span class="line">    if not self.concat_feature:</span><br><span class="line">        return &#123;&quot;gwc_feature&quot;: gwc_feature&#125;</span><br><span class="line">    else:</span><br><span class="line">        concat_feature = self.lastconv(gwc_feature)</span><br><span class="line">        return &#123;&quot;gwc_feature&quot;: gwc_feature, &quot;concat_feature&quot;: concat_feature&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="3-2-Group-wise-volume"><a href="#3-2-Group-wise-volume" class="headerlink" title="3.2 Group-wise volume"></a>3.2 Group-wise volume</h3><p><strong>groupwise_correlation</strong>函数：分组计算特征的相关性。就是fea1与fea2点乘，然后在通道这个维度resize成num_groups x channels_per_group，然后再在channels_per_group这个维度取均值。</p><p><strong>build_gwc_volume</strong>函数：这个就是在不同的视差等级上做warp操作，然后调用<strong>groupwise_correlation</strong>函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def groupwise_correlation(fea1, fea2, num_groups):</span><br><span class="line">    B, C, H, W = fea1.shape</span><br><span class="line">    assert C % num_groups == 0</span><br><span class="line">    channels_per_group = C // num_groups</span><br><span class="line">    # 在PyTorch中view函数作用为重构张量的维度，相当于numpy中的resize()的功能</span><br><span class="line">    # mean函数中的参数dim代表在第几维度求平均数</span><br><span class="line">    # 通道数C = 320，组数num_groups = 40，每组通道数channels_per_group = 8</span><br><span class="line">    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2) # 在通道这个维度C取平均</span><br><span class="line">    assert cost.shape == (B, num_groups, H, W)</span><br><span class="line">    return cost</span><br><span class="line"></span><br><span class="line">def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):</span><br><span class="line">    # refimg_fea为左图的feature volume</span><br><span class="line">    # targetimg_fea为右图的feature volume</span><br><span class="line">    # 这里的maxdisp = 48，不是192</span><br><span class="line">    # num_groups = 40</span><br><span class="line">    B, C, H, W = refimg_fea.shape</span><br><span class="line">    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])</span><br><span class="line">    for i in range(maxdisp):</span><br><span class="line">        if i &gt; 0:</span><br><span class="line">            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],</span><br><span class="line">                                                           num_groups)</span><br><span class="line">        else:</span><br><span class="line">            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)</span><br><span class="line">    # 使用 contiguous 方法则会重新开辟一块内存空间保证数据是在逻辑顺序和内存中是一致的，连续内存布局减少了CPU对对内存的请求次数</span><br><span class="line">    volume = volume.contiguous()</span><br><span class="line">    return volume</span><br></pre></td></tr></table></figure><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>在4维上不好解释，下面将左右图的feature volume看作3维的（也就是Batch_size&#x3D;1）来举例子：<br>这里设channels_per_group &#x3D; 2，就是两个通道作为一组<br><strong>1. 视差disparity&#x3D;0：</strong><br><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_5.png" alt="在这里插入图片描述"><br><strong>2. 视差disparity&#x3D;1：</strong><br>视差为1就是红色方框缩小1格，点乘时只有红色方框内的做点乘。<br><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_3.png" alt="在这里插入图片描述"><br>视差等于其他值时也是一样计算，将结果存在一个5维的volume中（B, num_groups, maxdisp, H, W），不考虑B就是4维。</p><p>这里可以看到这个volume中是有很多零的，点乘时只有红色方框内的做点乘，得到的结果W这个维度是比原来小的，所以要用零填充。</p><hr><h3 id="3-2-Concatenate-volume"><a href="#3-2-Concatenate-volume" class="headerlink" title="3.2 Concatenate volume"></a>3.2 Concatenate volume</h3><p>concat volume由左右图的feature volume先通过一个卷积减少通道数，然后连接在一起。<br>由下面的代码知feature volume经过这个卷积后，特征的维度为[1，12，96，312]。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># concat_feature_channel = 12</span><br><span class="line">if self.concat_feature:</span><br><span class="line">    self.lastconv = nn.Sequential(convbn(320, 128, 3, 1, 1, 1),</span><br><span class="line">                                  nn.ReLU(inplace=True),</span><br><span class="line">                                  nn.Conv2d(128, concat_feature_channel, kernel_size=1, padding=0, stride=1,</span><br><span class="line">                                            bias=False))</span><br></pre></td></tr></table></figure><p>然后看build_concat_volume的构建代码，看懂了group-wise volume的构建过程的话，理解这个也就很简单了，就是简单的移位拼接，不理解的话同样看下面的例子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):</span><br><span class="line">    B, C, H, W = refimg_fea.shape</span><br><span class="line">    # refimg_fea: &lt; class &#x27;torch.Tensor&#x27;&gt; torch.Size([1, 12, 96, 312])</span><br><span class="line">    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])</span><br><span class="line">    for i in range(maxdisp):</span><br><span class="line">        if i &gt; 0:</span><br><span class="line">            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]</span><br><span class="line">            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]</span><br><span class="line">        else:</span><br><span class="line">            volume[:, :C, i, :, :] = refimg_fea</span><br><span class="line">            volume[:, C:, i, :, :] = targetimg_fea</span><br><span class="line">    # 使用 contiguous 方法则会重新开辟一块内存空间保证数据是在逻辑顺序和内存中是一致的，连续内存布局减少了CPU对对内存的请求次数</span><br><span class="line">    volume = volume.contiguous()</span><br><span class="line">    return volume</span><br></pre></td></tr></table></figure><h4 id="举例-1"><a href="#举例-1" class="headerlink" title="举例"></a>举例</h4><p>在4维上不好解释，下面将左右图的feature volume看作3维的（也就是Batch_size&#x3D;1）来举例子：<br><strong>1. 视差disparity&#x3D;0：</strong><br><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_6.png" alt="在这里插入图片描述"><br><strong>2. 视差disparity&#x3D;1：</strong><br><img src="/2024/04/30/18_GWC-Net%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/img_4.png" alt="在这里插入图片描述"><br>视差等于其他值时也是一样计算，将结果存在一个4维的volume中（B, C, maxdisp, H, W），不考虑B就是4维。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法学习记录【四】【并查集、链表、排序】</title>
      <link href="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/"/>
      <url>/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h2 id="10-并查集"><a href="#10-并查集" class="headerlink" title="10. 并查集"></a>10. 并查集</h2><p>（1）判断总共有几个集合：统计 parent[i]&#x3D;iparent[i]&#x3D;iparent[i]&#x3D;i 的数量<br>（2）判断两个节点是否同一个集合：判断 find(index1)find(index1)find(index1) 和 find(index2)find(index2)find(index2) 是否相等</p><h3 id="10-1-省份数量"><a href="#10-1-省份数量" class="headerlink" title="10.1 省份数量"></a>10.1 省份数量</h3><p>该题使用并查集的时间复杂度是O(n2logn)O(n^{2}logn)O(n2logn)，该题也可用暴力dfs或者bfs搜索，时间复杂度为O(n2)O(n^{2})O(n2)。并查集的优势在于多次重复查找，在数据结构建立好后，查找和修改都很快，而暴力搜索则需要不断重复。<br><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_2.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findCircleNum(self, isConnected: List[List[int]]) -&gt; int:</span><br><span class="line">        L = len(isConnected)</span><br><span class="line">        parent = list(range(L))</span><br><span class="line"></span><br><span class="line">        def find(index):</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            寻找父节点，条件为指向自己，找的时候还会进行路径压缩</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            if parent[index] != index:</span><br><span class="line">                parent[index] = find(parent[index])</span><br><span class="line">            return parent[index]</span><br><span class="line"></span><br><span class="line">        def union(index1, index2):</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            将index1的父节点连接到index2的父节点上</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            parent[find(index1)] = find(index2)</span><br><span class="line"></span><br><span class="line">        for i in range(L):</span><br><span class="line">            for j in range(0, i):</span><br><span class="line">                if isConnected[i][j] == 1:</span><br><span class="line">                    union(i, j)</span><br><span class="line"></span><br><span class="line">        # 注意：这里判断有几个集合的条件是有几个节点指向自己，而不是有几个不重复的index</span><br><span class="line">        provinces = sum(parent[i] == i for i in range(L))</span><br><span class="line">        return provinces</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 暴力解代码</span><br><span class="line">class Solution:</span><br><span class="line">    def findCircleNum(self, isConnected: List[List[int]]) -&gt; int:</span><br><span class="line">        L = len(isConnected)</span><br><span class="line"></span><br><span class="line">        used_list = [False] * L</span><br><span class="line"></span><br><span class="line">        def dfs(index):</span><br><span class="line">            if used_list[index] == True:</span><br><span class="line">                return 0</span><br><span class="line">            else:</span><br><span class="line">                used_list[index] = True</span><br><span class="line">                for i in range(L):</span><br><span class="line">                    if isConnected[index][i] == 1 and used_list[i] == False:</span><br><span class="line">                        dfs(i)</span><br><span class="line"></span><br><span class="line">        # def bfs(index):</span><br><span class="line">        #     if used_list[index] == True:</span><br><span class="line">        #         return 0</span><br><span class="line">        #     else:</span><br><span class="line">        #         queue_list = [index]</span><br><span class="line">        #         while len(queue_list) &gt; 0:</span><br><span class="line">        #             p = queue_list.pop(0)</span><br><span class="line">        #             used_list[p] = True</span><br><span class="line">        #             for i in range(L):</span><br><span class="line">        #                 if isConnected[p][i] == 1 and used_list[i] == False:</span><br><span class="line">        #                     queue_list.append(i)</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        for i in range(L):</span><br><span class="line">            if used_list[i] == False:</span><br><span class="line">                dfs(i)</span><br><span class="line">                # bfs(i)</span><br><span class="line">                count += 1</span><br><span class="line"></span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h3 id="10-2-被围绕的区域"><a href="#10-2-被围绕的区域" class="headerlink" title="10.2 被围绕的区域"></a>10.2 被围绕的区域</h3><p>这题同样的，单次搜用dfs或者bfs更快。(DFS思路：以边界上的O点为起点，找边界区域，再反向赋值)<br><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_1.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def solve(self, board: List[List[str]]) -&gt; None:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Do not return anything, modify board in-place instead.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        M = len(board)</span><br><span class="line">        N = len(board[0])</span><br><span class="line"></span><br><span class="line">        parent = [i for i in range(M*N + 1)]  # 增加一个节点表示边界区域</span><br><span class="line"></span><br><span class="line">        def find(index):</span><br><span class="line">            if parent[index] != index:</span><br><span class="line">                parent[index] = find(parent[index])</span><br><span class="line">            return parent[index]</span><br><span class="line"></span><br><span class="line">        def union(index1, index2):</span><br><span class="line">            parent[find(index1)] = find(index2)</span><br><span class="line"></span><br><span class="line">        FLAG_INDEX = M*N</span><br><span class="line">        for i in range(M):</span><br><span class="line">            for j in range(N):</span><br><span class="line">                if board[i][j] == &#x27;O&#x27;:</span><br><span class="line">                    if i==0 or i==M-1 or j==0 or j==N-1:</span><br><span class="line">                        union(i*N+j, M*N)</span><br><span class="line">                    else:</span><br><span class="line">                        for x,y in [[-1,0], [0,1], [1, 0], [0,-1]]:</span><br><span class="line">                            if board[i+x][j+y] == &#x27;O&#x27;:</span><br><span class="line">                                union(i*N+j, (x+i)*N+(j+y))</span><br><span class="line"></span><br><span class="line">        for i in range(M):</span><br><span class="line">            for j in range(N):</span><br><span class="line">                # 并查集判断是否是同类有些特殊，不能够通过存储值直接判断，要用find函数找父节点，判断父节点是否相同</span><br><span class="line">                if find(FLAG_INDEX) == find(i*N+j):</span><br><span class="line">                    board[i][j] = &#x27;O&#x27;</span><br><span class="line">                else:</span><br><span class="line">                    board[i][j] = &#x27;X&#x27;</span><br></pre></td></tr></table></figure><h3 id="10-3-账户合并"><a href="#10-3-账户合并" class="headerlink" title="10.3 账户合并"></a>10.3 账户合并</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_10.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def accountsMerge(self, accounts: List[List[str]]) -&gt; List[List[str]]:</span><br><span class="line">        # 数据预处理，邮箱为key，人名的index为value</span><br><span class="line">        mails_hash_dict = &#123;&#125;</span><br><span class="line">        name_list = [] # 存储人名</span><br><span class="line">        for i in range(len(accounts)):</span><br><span class="line">            mails = accounts[i][1:]</span><br><span class="line">            name = accounts[i][0]</span><br><span class="line">            name_list.append(name)</span><br><span class="line">            for m in mails:</span><br><span class="line">                if mails_hash_dict.__contains__(m):</span><br><span class="line">                    mails_hash_dict[m].append(i)</span><br><span class="line">                else:</span><br><span class="line">                    mails_hash_dict[m] = [i]</span><br><span class="line"></span><br><span class="line">        # 初始化父节点记录数组</span><br><span class="line">        parent = [i for i in range(len(accounts))]</span><br><span class="line"></span><br><span class="line">        def find(index):</span><br><span class="line">            if parent[index] != index:</span><br><span class="line">                parent[index] = find(parent[index])</span><br><span class="line">            return parent[index]</span><br><span class="line"></span><br><span class="line">        def union(index1, index2):</span><br><span class="line">            parent[find(index1)] = find(index2)</span><br><span class="line"></span><br><span class="line">        # 将有相同邮箱的账号合并</span><br><span class="line">        for k,value in mails_hash_dict.items():</span><br><span class="line">            if len(value) &gt; 1:</span><br><span class="line">                f = value.pop(0)</span><br><span class="line">                for v in value:</span><br><span class="line">                    union(f, v)</span><br><span class="line"></span><br><span class="line">        # 统计集合个数</span><br><span class="line">        p_num = sum(parent[i] == i for i in range(len(accounts)))</span><br><span class="line"></span><br><span class="line">        # 初始化一个列表生成结果，有空的，后面再删除</span><br><span class="line">        result_list = [set() for i in range(len(accounts))]</span><br><span class="line"></span><br><span class="line">        # 将结果添加到父节点当中</span><br><span class="line">        for i in range(len(accounts)):</span><br><span class="line">            index = find(i)</span><br><span class="line">            t = accounts[i][1:]</span><br><span class="line">            for u in t:</span><br><span class="line">                # print(u)</span><br><span class="line">                result_list[index].add(u)</span><br><span class="line"></span><br><span class="line">        # 删除空集，并添加人名</span><br><span class="line">        final_result = []</span><br><span class="line">        for i in range(len(result_list)):</span><br><span class="line">            if len(result_list[i]) &gt; 0:</span><br><span class="line">                a = list(result_list[i])</span><br><span class="line">                a.sort() # 结果要求排序</span><br><span class="line">                a.insert(0, name_list[i])</span><br><span class="line">                final_result.append(a)</span><br><span class="line"></span><br><span class="line">        return final_result</span><br></pre></td></tr></table></figure><h3 id="10-4-判断二分图"><a href="#10-4-判断二分图" class="headerlink" title="10.4 判断二分图"></a>10.4 判断二分图</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_3.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def isBipartite(self, graph: List[List[int]]) -&gt; bool:</span><br><span class="line">        L = len(graph)</span><br><span class="line">        parent = [i for i in range(L)]</span><br><span class="line"></span><br><span class="line">        def find(index):</span><br><span class="line">            if parent[index] != index:</span><br><span class="line">                parent[index] = find(parent[index])</span><br><span class="line">            return parent[index]</span><br><span class="line"></span><br><span class="line">        def union(index1, index2):</span><br><span class="line">            parent[find(index1)] = find(index2)</span><br><span class="line"></span><br><span class="line">        # 思路：遍历，判断连接的节点根节点是否相同，如果相同，则返回false，如果不同，则将他们合并</span><br><span class="line">        for i in range(L):</span><br><span class="line">            g = graph[i]</span><br><span class="line">            root = find(i)</span><br><span class="line">            for j in range(len(g)):</span><br><span class="line">                if root == find(g[j]):</span><br><span class="line">                    return False</span><br><span class="line">            for j in range(1, len(g)):</span><br><span class="line">                union(g[j-1],g[j])</span><br><span class="line">        return True</span><br></pre></td></tr></table></figure><h2 id="11-链表"><a href="#11-链表" class="headerlink" title="11. 链表"></a>11. 链表</h2><h3 id="11-1-移除链表元素"><a href="#11-1-移除链表元素" class="headerlink" title="11.1 移除链表元素"></a>11.1 移除链表元素</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_6.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Definition for singly-linked list.</span><br><span class="line"># class ListNode:</span><br><span class="line">#     def __init__(self, val=0, next=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.next = next</span><br><span class="line">class Solution:</span><br><span class="line">    def removeElements(self, head: Optional[ListNode], val: int) -&gt; Optional[ListNode]:</span><br><span class="line">        cur = dummy = ListNode(next=head)</span><br><span class="line">        while cur.next:</span><br><span class="line">            if cur.next.val == val:</span><br><span class="line">                cur.next = cur.next.next</span><br><span class="line">            else:</span><br><span class="line">                cur = cur.next</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure><h3 id="11-2-反转链表"><a href="#11-2-反转链表" class="headerlink" title="11.2 反转链表"></a>11.2 反转链表</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_5.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Definition for singly-linked list.</span><br><span class="line"># class ListNode:</span><br><span class="line">#     def __init__(self, val=0, next=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.next = next</span><br><span class="line">class Solution:</span><br><span class="line">    def reverseList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:</span><br><span class="line">        pre_Node = None</span><br><span class="line">        cur_Node = head</span><br><span class="line">        while cur_Node is not None:</span><br><span class="line">            next_Node = cur_Node.next</span><br><span class="line">            cur_Node.next = pre_Node</span><br><span class="line">            pre_Node = cur_Node</span><br><span class="line">            cur_Node = next_Node</span><br><span class="line">        return pre_Node</span><br></pre></td></tr></table></figure><h3 id="11-3-两两交换链表中的节点"><a href="#11-3-两两交换链表中的节点" class="headerlink" title="11.3 两两交换链表中的节点"></a>11.3 两两交换链表中的节点</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_4.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Definition for singly-linked list.</span><br><span class="line"># class ListNode:</span><br><span class="line">#     def __init__(self, val=0, next=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.next = next</span><br><span class="line">class Solution:</span><br><span class="line">    def swapPairs(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:</span><br><span class="line">        node0 = dummy = ListNode(next=head)  # 用哨兵节点简化代码逻辑</span><br><span class="line">        node1 = head</span><br><span class="line">        while node1 and node1.next:  # 至少有两个节点</span><br><span class="line">            node2 = node1.next</span><br><span class="line">            node3 = node2.next</span><br><span class="line"></span><br><span class="line">            node0.next = node2  # 0 -&gt; 2</span><br><span class="line">            node2.next = node1  # 2 -&gt; 1</span><br><span class="line">            node1.next = node3  # 1 -&gt; 3</span><br><span class="line"></span><br><span class="line">            node0 = node1  # 下一轮交换，0 是 1</span><br><span class="line">            node1 = node3  # 下一轮交换，1 是 3</span><br><span class="line">        return dummy.next  # 返回新链表的头节点</span><br></pre></td></tr></table></figure><h3 id="11-4-环形链表"><a href="#11-4-环形链表" class="headerlink" title="11.4 环形链表"></a>11.4 环形链表</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_7.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Definition for singly-linked list.</span><br><span class="line"># class ListNode:</span><br><span class="line">#     def __init__(self, x):</span><br><span class="line">#         self.val = x</span><br><span class="line">#         self.next = None</span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def detectCycle(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:</span><br><span class="line">        slow = head</span><br><span class="line">        fast = head</span><br><span class="line">        while slow is not None and fast is not None:</span><br><span class="line">            fast = fast.next</span><br><span class="line">            if fast is None:</span><br><span class="line">                return None</span><br><span class="line">            else:</span><br><span class="line">                fast = fast.next</span><br><span class="line">            slow = slow.next</span><br><span class="line">            if slow == fast:</span><br><span class="line">                ptr = head</span><br><span class="line">                while ptr != slow:</span><br><span class="line">                    ptr = ptr.next</span><br><span class="line">                    slow = slow.next</span><br><span class="line">                return ptr</span><br></pre></td></tr></table></figure><h3 id="11-5-删除链表的倒数第N个结点"><a href="#11-5-删除链表的倒数第N个结点" class="headerlink" title="11.5 删除链表的倒数第N个结点"></a>11.5 删除链表的倒数第N个结点</h3><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_8.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -&gt; Optional[ListNode]:</span><br><span class="line">        # 由于可能会删除链表头部，用哨兵节点简化代码</span><br><span class="line">        left = right = dummy = ListNode(next=head)</span><br><span class="line">        for _ in range(n):</span><br><span class="line">            right = right.next  # 右指针先向右走 n 步</span><br><span class="line">        while right.next:</span><br><span class="line">            left = left.next</span><br><span class="line">            right = right.next  # 左右指针一起走</span><br><span class="line">        left.next = left.next.next  # 左指针的下一个节点就是倒数第 n 个节点</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure><h2 id="12-排序"><a href="#12-排序" class="headerlink" title="12. 排序"></a>12. 排序</h2><blockquote><p><a href="https://www.cnblogs.com/onepixel/p/7674659.html">https://www.cnblogs.com/onepixel/p/7674659.html</a></p></blockquote><p><img src="/2024/03/04/14_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E5%9B%9B%E3%80%91%E3%80%90%E5%B9%B6%E6%9F%A5%E9%9B%86%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%91/img_9.png" alt="在这里插入图片描述"></p><h3 id="12-1-冒泡排序"><a href="#12-1-冒泡排序" class="headerlink" title="12.1 冒泡排序"></a>12.1 冒泡排序</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Bubble Sort Algorithm</span><br><span class="line">def bubble_sort(nums):</span><br><span class="line">    n = len(nums)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        for j in range(0, n - i - 1):</span><br><span class="line">            if nums[j] &gt; nums[j + 1]:</span><br><span class="line">                nums[j], nums[j + 1] = nums[j + 1], nums[j]</span><br><span class="line">    return nums</span><br><span class="line"></span><br><span class="line">nums = [3, 2, 1, 5, 6, 4]</span><br><span class="line">print(bubble_sort(nums))</span><br></pre></td></tr></table></figure><h3 id="12-2-快速排序"><a href="#12-2-快速排序" class="headerlink" title="12.2 快速排序"></a>12.2 快速排序</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># QuickSort Implementation in Python</span><br><span class="line">def quick_sort(nums):</span><br><span class="line">    if len(nums) &lt;= 1:</span><br><span class="line">        return nums</span><br><span class="line">    pivot = nums[len(nums) // 2]</span><br><span class="line">    left = [x for x in nums if x &lt; pivot]</span><br><span class="line">    middle = [x for x in nums if x == pivot]</span><br><span class="line">    right = [x for x in nums if x &gt; pivot]</span><br><span class="line">    return quick_sort(left) + middle + quick_sort(right)</span><br><span class="line"></span><br><span class="line">nums = [3, 2, 1, 5, 6, 4]</span><br><span class="line">print(quick_sort(nums))</span><br></pre></td></tr></table></figure><h3 id="12-3-桶排序"><a href="#12-3-桶排序" class="headerlink" title="12.3 桶排序"></a>12.3 桶排序</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">def bucket_sort(arr):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    桶排序算法实现</span><br><span class="line"></span><br><span class="line">    参数:</span><br><span class="line">        arr: 待排序的列表，包含数字（整数或浮点数）</span><br><span class="line"></span><br><span class="line">    返回:</span><br><span class="line">        排序后的列表</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if not arr:</span><br><span class="line">        return arr</span><br><span class="line"></span><br><span class="line">    # 1. 确定数据范围</span><br><span class="line">    min_val = min(arr)</span><br><span class="line">    max_val = max(arr)</span><br><span class="line"></span><br><span class="line">    # 2. 计算桶的数量和每个桶的范围</span><br><span class="line">    bucket_count = len(arr)  # 桶的数量通常与数据量相当</span><br><span class="line">    bucket_range = (max_val - min_val) / bucket_count</span><br><span class="line"></span><br><span class="line">    # 3. 创建空桶</span><br><span class="line">    buckets = [[] for _ in range(bucket_count)]</span><br><span class="line"></span><br><span class="line">    # 4. 将元素分配到对应的桶中</span><br><span class="line">    for num in arr:</span><br><span class="line">        # 计算元素应该放入哪个桶</span><br><span class="line">        index = int((num - min_val) / bucket_range)</span><br><span class="line">        # 处理最大值的特殊情况，确保它被放入最后一个桶</span><br><span class="line">        if index == bucket_count:</span><br><span class="line">            index -= 1</span><br><span class="line">        buckets[index].append(num)</span><br><span class="line"></span><br><span class="line">    # 5. 对每个桶进行排序，并合并结果</span><br><span class="line">    sorted_arr = []</span><br><span class="line">    for bucket in buckets:</span><br><span class="line">        # 对每个桶使用内置排序（也可以递归使用桶排序）</span><br><span class="line">        bucket.sort()</span><br><span class="line">        # 将排序后的桶内容添加到结果列表</span><br><span class="line">        sorted_arr.extend(bucket)</span><br><span class="line"></span><br><span class="line">    return sorted_arr</span><br><span class="line"></span><br><span class="line"># 测试代码</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    # 测试用例1：包含整数和小数的混合列表</span><br><span class="line">    test_arr1 = [4.5, 0.84, 3.25, 2.18, 0.5]</span><br><span class="line">    print(&quot;原始列表1:&quot;, test_arr1)</span><br><span class="line">    sorted_arr1 = bucket_sort(test_arr1)</span><br><span class="line">    print(&quot;排序后列表1:&quot;, sorted_arr1)</span><br><span class="line"></span><br><span class="line">    # 测试用例2：整数列表</span><br><span class="line">    test_arr2 = [10, 5, 3, 8, 2, 7, 1, 9, 6, 4]</span><br><span class="line">    print(&quot;\n原始列表2:&quot;, test_arr2)</span><br><span class="line">    sorted_arr2 = bucket_sort(test_arr2)</span><br><span class="line">    print(&quot;排序后列表2:&quot;, sorted_arr2)</span><br><span class="line"></span><br><span class="line">    # 测试用例3：空列表</span><br><span class="line">    test_arr3 = []</span><br><span class="line">    print(&quot;\n原始列表3:&quot;, test_arr3)</span><br><span class="line">    sorted_arr3 = bucket_sort(test_arr3)</span><br><span class="line">    print(&quot;排序后列表3:&quot;, sorted_arr3)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>算法学习记录【三】【单调栈、双指针、前缀和】</title>
      <link href="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/"/>
      <url>/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h2 id="7-单调栈"><a href="#7-单调栈" class="headerlink" title="7. 单调栈"></a>7. 单调栈</h2><p>对于一个数组：<br> （1）由前往后遍历，栈底到栈顶递增，可获得前面第一个比当前数小的数字；<br> （2）由前往后遍历，栈底到栈顶递减，可获得前面第一个比当前数大的数字；<br> （3）由后往前遍历，栈底到栈顶递增，可获得后面第一个比当前数小的数字；<br> （4）由后往前遍历，栈底到栈顶递减，可获得后面第一个比当前数大的数字；</p><h3 id="7-1-每日温度"><a href="#7-1-每日温度" class="headerlink" title="7.1 每日温度"></a>7.1 每日温度</h3><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_3.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def dailyTemperatures(self, temperatures: List[int]) -&gt; List[int]:</span><br><span class="line">        stack_list = []</span><br><span class="line">        L = len(temperatures)</span><br><span class="line">        next_day = [0] * L</span><br><span class="line"></span><br><span class="line">        for i in range(L):</span><br><span class="line">            # 小于或者空入栈，大于就出栈，并记录结果，永远保持栈顶是目前最大</span><br><span class="line">            if len(stack_list) == 0:</span><br><span class="line">                stack_list.append(i)</span><br><span class="line">            else:</span><br><span class="line">                if temperatures[i] &lt;= temperatures[stack_list[-1]]:</span><br><span class="line">                    stack_list.append(i)</span><br><span class="line">                else:</span><br><span class="line">                    while len(stack_list)&gt;0 and temperatures[i] &gt; temperatures[stack_list[-1]]:</span><br><span class="line">                        a = stack_list.pop()</span><br><span class="line">                        next_day[a] = i - a</span><br><span class="line">                    stack_list.append(i)</span><br><span class="line"></span><br><span class="line">        return next_day</span><br></pre></td></tr></table></figure><h3 id="7-2-柱状图中的最大矩形"><a href="#7-2-柱状图中的最大矩形" class="headerlink" title="7.2 柱状图中的最大矩形"></a>7.2 柱状图中的最大矩形</h3><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_5.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def largestRectangleArea(self, heights: List[int]) -&gt; int:</span><br><span class="line">        L = len(heights)</span><br><span class="line">        stack_list = []</span><br><span class="line">        max_s = 0</span><br><span class="line"></span><br><span class="line">        # 递增入栈，遇到小的出栈并计算面积</span><br><span class="line">        for i in range(L):</span><br><span class="line">            if len(stack_list) == 0:</span><br><span class="line">                stack_list.append(i)</span><br><span class="line">            else:</span><br><span class="line">                if heights[i] &gt;= heights[stack_list[-1]]:</span><br><span class="line">                    stack_list.append(i)</span><br><span class="line">                else:</span><br><span class="line">                    while len(stack_list) &gt; 0 and heights[i] &lt; heights[stack_list[-1]]:</span><br><span class="line">                        # 计算面积要分情况，如果弹出后队列为空了，边长就是i；否则，以队列最后一个元素作为边界</span><br><span class="line">                        # 官方题解中通过再栈中增加一个数值来解决边界问题，减少了代码量</span><br><span class="line">                        if len(stack_list) == 1:</span><br><span class="line">                            t = stack_list.pop(-1)</span><br><span class="line">                            cur_s = i * heights[t]</span><br><span class="line">                            max_s = max(max_s, cur_s)</span><br><span class="line">                        else:</span><br><span class="line">                            t = stack_list.pop(-1)</span><br><span class="line">                            cur_s = (i - stack_list[-1] - 1) * heights[t]</span><br><span class="line">                            max_s = max(max_s, cur_s)</span><br><span class="line">                    stack_list.append(i)</span><br><span class="line"></span><br><span class="line">        # 处理剩余值</span><br><span class="line">        while len(stack_list) &gt; 1:</span><br><span class="line">            t = stack_list.pop(-1)</span><br><span class="line">            cur_s = (L - stack_list[-1] - 1) * heights[t]</span><br><span class="line">            max_s = max(max_s, cur_s)</span><br><span class="line">        if len(stack_list) == 1:</span><br><span class="line">            t = stack_list.pop(-1)</span><br><span class="line">            cur_s = L * heights[t]</span><br><span class="line">            max_s = max(max_s, cur_s)</span><br><span class="line"></span><br><span class="line">        return max_s</span><br></pre></td></tr></table></figure><h2 id="8-双指针"><a href="#8-双指针" class="headerlink" title="8. 双指针"></a>8. 双指针</h2><p>分为快慢双指针和左右双指针</p><h3 id="8-1-移动零"><a href="#8-1-移动零" class="headerlink" title="8.1 移动零"></a>8.1 移动零</h3><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_2.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def moveZeroes(self, nums):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: None Do not return anything, modify nums in-place instead.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        n = len(nums)</span><br><span class="line">        left = right = 0</span><br><span class="line">        while right &lt; n:</span><br><span class="line">            if nums[right] != 0:</span><br><span class="line">                nums[left], nums[right] = nums[right], nums[left]</span><br><span class="line">                left += 1</span><br><span class="line">            right += 1</span><br><span class="line">        return nums</span><br></pre></td></tr></table></figure><h3 id="8-2-三数之和"><a href="#8-2-三数之和" class="headerlink" title="8.2 三数之和"></a>8.2 三数之和</h3><p>在有序数组中，随着第一个元素的递增，第二个元素是递减的，那么就可以使用左右双指针的方法</p><p>（该题可变形为找可组成三角形的组合数）<br> <img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_4.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def threeSum(self, nums):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: List[List[int]]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        result = []</span><br><span class="line">        left, mid, right = 0, 1, 2</span><br><span class="line">        L = len(nums)</span><br><span class="line">        nums.sort()</span><br><span class="line">        for left in range(L):</span><br><span class="line">            if left &gt; 0 and nums[left] == nums[left - 1]:</span><br><span class="line">                continue</span><br><span class="line">            if nums[left] &gt; 0:</span><br><span class="line">                break</span><br><span class="line">            right = L - 1</span><br><span class="line">            target = -nums[left]</span><br><span class="line">            for mid in range(left + 1, L - 1):</span><br><span class="line">                if mid &gt; left + 1 and nums[mid] == nums[mid - 1]:</span><br><span class="line">                    continue</span><br><span class="line">                while mid &lt; right and nums[mid] + nums[right] &gt; target:</span><br><span class="line">                    right = right - 1</span><br><span class="line">                if mid == right:</span><br><span class="line">                    break</span><br><span class="line">                if nums[mid] + nums[right] == target:</span><br><span class="line">                    result.append([nums[left], nums[mid], nums[right]])</span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><h3 id="8-3-盛水最多的容器"><a href="#8-3-盛水最多的容器" class="headerlink" title="8.3 盛水最多的容器"></a>8.3 盛水最多的容器</h3><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_6.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxArea(self, height):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type height: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        left, right = 0, len(height) - 1</span><br><span class="line">        current_S = (right - left) * min(height[left], height[right])</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            area = (right - left) * min(height[left], height[right])</span><br><span class="line">            current_S = max(current_S, area)</span><br><span class="line">            if height[left] &lt; height[right]:</span><br><span class="line">                left = left + 1</span><br><span class="line">            else:</span><br><span class="line">                right = right - 1</span><br><span class="line">        return current_S</span><br></pre></td></tr></table></figure><h2 id="9-前缀和"><a href="#9-前缀和" class="headerlink" title="9. 前缀和"></a>9. 前缀和</h2><p>空间换时间的做法，分为一维前缀和和二维前缀和，此外还可以灵活运用。使用前缀和的一个前提条件是连续</p><h3 id="9-1-区域和检索"><a href="#9-1-区域和检索" class="headerlink" title="9.1 区域和检索"></a>9.1 区域和检索</h3><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_1.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class NumArray:</span><br><span class="line"></span><br><span class="line">    def __init__(self, nums: List[int]):</span><br><span class="line">        self.nums = nums</span><br><span class="line">        L = len(self.nums)</span><br><span class="line">        self.prefix_sum = [0] * (L + 1)</span><br><span class="line">        for i in range(1, L+1):</span><br><span class="line">            self.prefix_sum[i] = self.prefix_sum[i-1] + nums[i-1]</span><br><span class="line">        print(self.prefix_sum)</span><br><span class="line"></span><br><span class="line">    def sumRange(self, left: int, right: int) -&gt; int:</span><br><span class="line">        return self.prefix_sum[right+1] - self.prefix_sum[left]</span><br></pre></td></tr></table></figure><h3 id="9-2-和为K的子数组"><a href="#9-2-和为K的子数组" class="headerlink" title="9.2 和为K的子数组"></a>9.2 和为K的子数组</h3><p>二维前缀和资料：<a href="https://leetcode.cn/circle/discuss/UUuRex/">https://leetcode.cn/circle/discuss/UUuRex/</a></p><p><img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_7.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def subarraySum(self, nums, k):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :type k: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        result = 0</span><br><span class="line">        # 使用哈希表记录前缀和出现次数，避免重新查找</span><br><span class="line">        hash_dict = &#123;&#125;</span><br><span class="line">        # 初始时存一个&#123;0:1&#125;，此时的前缀和刚好等于K </span><br><span class="line">        #（如果没有负数，没想到这样放也行，只需要加个判断；如果有负数，这个&#123;0:1&#125;加得很关键）</span><br><span class="line">        hash_dict.update(&#123;0:1&#125;)</span><br><span class="line">        sum_t = 0</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            sum_t += nums[i]</span><br><span class="line">            if hash_dict.__contains__(sum_t - k):</span><br><span class="line">                # 在哈希表中查找满足的序列数</span><br><span class="line">                result += hash_dict[sum_t - k]</span><br><span class="line">            hash_dict[sum_t] = hash_dict.get(sum_t, 0) + 1</span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><p>这题有一个很有意思的变形【平均数为k的最长连续子数组】，将和变为平均数就不能直接用上面的解法了，因为要查找的值和当前位置有关，所以关键是要将问题转换一下，化“动”为“不动”。如果查找的是平均数为0的最长子数组，那就好办了，0*任意长度还是0，相当于“不动”。所以关键的步骤是将数组所有数字减去K，这样就能将问题转换为【平均数为0的最长子数组】了。（因为需要的是长度，所以哈希表中存的是坐标而不是次数了）</p><h3 id="9-3-统计全为1的正方形子矩阵"><a href="#9-3-统计全为1的正方形子矩阵" class="headerlink" title="9.3 统计全为1的正方形子矩阵"></a>9.3 统计全为1的正方形子矩阵</h3><p>这题还有很多衍生题，用二维前缀和能够减少较多计算量，特比是在子矩阵大的时候会减少很多计算量，但是下面这题用在leetcode当中要5s（python），过不了所有测试用例。所以后面还是要看一下正统解法动态规划…</p><p>在动态规划不可用的时候，或者转移方程过于复杂，二维前缀和还是有用武之地的，比如求0和1数量相等的正方形子矩阵(或更难的去掉正方形这个条件)<br> <img src="/2024/03/03/13_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%89%E3%80%91%E3%80%90%E5%8D%95%E8%B0%83%E6%A0%88%E3%80%81%E5%8F%8C%E6%8C%87%E9%92%88%E3%80%81%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%91/img_8.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def countSquares(self, matrix: List[List[int]]) -&gt; int:</span><br><span class="line">        M = len(matrix)</span><br><span class="line">        N = len(matrix[0])</span><br><span class="line"></span><br><span class="line">        prefix_sum = [[0] * (N+1) for _ in range((M+1))]</span><br><span class="line"></span><br><span class="line">        # 二维前缀和矩阵构建方式：</span><br><span class="line">        # 1. 方式一：先构建每一行的前缀和，再在列方向上相加 (两次遍历，复杂度高)</span><br><span class="line">        # for i in range(1, M+1):</span><br><span class="line">        #     for j in range(1, N+1):</span><br><span class="line">        #         prefix_sum[i][j] = prefix_sum[i][j-1] + matrix[i-1][j-1]</span><br><span class="line">        # for i in range(1, M+1):</span><br><span class="line">        #     for j in range(1, N+1):</span><br><span class="line">        #         prefix_sum[i][j] = prefix_sum[i-1][j] + prefix_sum[i][j]</span><br><span class="line"></span><br><span class="line">        # 2. 方式二：左+上+当前点-重复计算部分</span><br><span class="line">        for i in range(1, M+1):</span><br><span class="line">            for j in range(1, N+1):</span><br><span class="line">                prefix_sum[i][j] = prefix_sum[i-1][j] + prefix_sum[i][j-1] - prefix_sum[i-1][j-1] + matrix[i-1][j-1]</span><br><span class="line"></span><br><span class="line">        # 计算面积，参数为矩阵的左上点坐标和右下点坐标</span><br><span class="line">        def getsubMatrix(x1,y1,x2,y2):</span><br><span class="line">            return prefix_sum[x2 + 1][y2 + 1] - prefix_sum[x1][y2 + 1] - prefix_sum[x2 + 1][y1] + prefix_sum[x1][y1]</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        # 不用从1算起，直接在最后加上就行</span><br><span class="line">        for K in range(2, min(M, N)+1):</span><br><span class="line">            for i in range(M-K+1):</span><br><span class="line">                for j in range(N-K+1):</span><br><span class="line">                    # 记得减1</span><br><span class="line">                    if getsubMatrix(i, j, i+K-1, j+K-1) == K*K:</span><br><span class="line">                        count += 1</span><br><span class="line"></span><br><span class="line">        count += prefix_sum[-1][-1]</span><br><span class="line">        return count</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法学习记录【二】【动态规划、贪心算法、拓扑排序】</title>
      <link href="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/"/>
      <url>/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h2 id="4-动态规划"><a href="#4-动态规划" class="headerlink" title="4. 动态规划"></a>4. 动态规划</h2><p><strong>动态规划问题类型总结</strong></p><ul><li><strong>01背包</strong>：目标和、打家劫舍、分割等和子集<br>从候选集中选择元素，每个元素只能选择一次，使结果满足某个条件。状态转移方程一般形式：<br>dp[i][j]&#x3D;{max(dp[i−1][j−nums[i]]+nums[i],dp[i−1][j]),j&gt;&#x3D;nums[i]dp[i−1][j],j&lt;nums[i]dp[i][j]&#x3D;\left{\begin{matrix}max(dp[i-1][j-nums[i]]+nums[i],dp[i-1][j]),<br>&amp; j&gt;&#x3D;nums[i] \dp[i-1][j],<br>&amp; j&lt;nums[i]<br>\end{matrix}\right.dp[i][j]&#x3D;{max(dp[i−1][j−nums[i]]+nums[i],dp[i−1][j]),dp[i−1][j],​j&gt;&#x3D;nums[i]j&lt;nums[i]​<br>注意：max根据实际需要替换，比如求方案数就用+，是否可以用or</li><li><strong>完全背包</strong>：零钱兑换<br><strong>组合先遍历物品，排列先遍历背包</strong><br>从候选集中选择元素，每个元素可以多选，使结果满足某个条件。结果不考虑顺序时，状态转移方程一般形式：<br>dp[i][j]&#x3D;{dp[i][j−nums[i]]+dp[i−1][j],j&gt;&#x3D;nums[i]dp[i−1][j],j&lt;nums[i]dp[i][j]&#x3D;\left{\begin{matrix}dp[i][j-nums[i]]+dp[i-1][j],<br>&amp; j&gt;&#x3D;nums[i] \dp[i-1][j],<br>&amp; j&lt;nums[i]<br>\end{matrix}\right.dp[i][j]&#x3D;{dp[i][j−nums[i]]+dp[i−1][j],dp[i−1][j],​j&gt;&#x3D;nums[i]j&lt;nums[i]​<br>结果考虑顺序时，状态转移方程一般形式：<br>dp[i]&#x3D;∑nnumsdp[i−n],if(i&gt;&#x3D;n)dp[i]&#x3D;\sum_{n}^{nums}dp[i-n],if(i&gt;&#x3D;n)dp[i]&#x3D;∑nnums​dp[i−n],if(i&gt;&#x3D;n)<br>注意：因为可以多选，选择当前元素时，访问的是当前行的结果，这与01背包是不同的</li><li><strong>线性DP</strong>：最长公共子序列、编辑距离<br>01背包和完全背包是单边的选与不选，线性DP则是两边选或者不选，当前状态一般从左上角的三个状态转移过来</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int:</span><br><span class="line">        # n = len(text1)</span><br><span class="line">        # m = len(text2)</span><br><span class="line">        # @cache</span><br><span class="line">        # def dfs(i, j):</span><br><span class="line">        #     if i &lt; 0 or j &lt; 0:</span><br><span class="line">        #         return 0</span><br><span class="line">        #     if text1[i] == text2[j]:</span><br><span class="line">        #         # return max(dfs(i-1, j), dfs(i, j-1), dfs(i-1, j-1) + 1)</span><br><span class="line">        #         return dfs(i-1, j-1) + 1</span><br><span class="line">        #     else:</span><br><span class="line">        #         # return max(dfs(i-1, j), dfs(i, j-1), dfs(i-1, j-1))</span><br><span class="line">        #         return max(dfs(i-1, j), dfs(i, j-1))</span><br><span class="line">        # return dfs(n-1, m-1)</span><br><span class="line">        m = len(text1)</span><br><span class="line">        n = len(text2)</span><br><span class="line">        dp = [[0] * (n + 1) for _ in range(m + 1)]</span><br><span class="line">        # dp[0][0] = 1</span><br><span class="line">        for i in range(1, m + 1):</span><br><span class="line">            for j in range(1, n + 1):</span><br><span class="line">                if text1[i-1] == text2[j-1]:</span><br><span class="line">                    # dp[i][j] = max(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]+1)</span><br><span class="line">                    dp[i][j] = dp[i-1][j-1]+1</span><br><span class="line">                else:</span><br><span class="line">                    # dp[i][j] = max(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])</span><br><span class="line">                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])</span><br><span class="line">        return dp[-1][-1]</span><br></pre></td></tr></table></figure><ul><li>状态机DP</li><li>区间DP</li><li>树状DP</li></ul><h3 id="4-1-使用最小花费爬楼梯"><a href="#4-1-使用最小花费爬楼梯" class="headerlink" title="4.1 使用最小花费爬楼梯"></a>4.1 使用最小花费爬楼梯</h3><p>比爬楼梯略微多了点花样的基础dp</p><ul><li>状态转移方程：<br>dp[i]&#x3D;min(cost[i]+dp[i−1],cost[i]+dp[i−2])dp[i]&#x3D;min(cost[i]+dp[i-1],cost[i]+dp[i-2])dp[i]&#x3D;min(cost[i]+dp[i−1],cost[i]+dp[i−2])</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_1.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def minCostClimbingStairs(self, cost: List[int]) -&gt; int:</span><br><span class="line">        L = len(cost)</span><br><span class="line"></span><br><span class="line">        if L == 1:</span><br><span class="line">            return cost[0]</span><br><span class="line">        if L == 2:</span><br><span class="line">            return min(cost[0], cost[1])</span><br><span class="line"></span><br><span class="line">        dp_cost = [0] * L </span><br><span class="line">        dp_cost[0], dp_cost[1] = cost[0], cost[1]</span><br><span class="line"></span><br><span class="line">        for i in range(2, L):</span><br><span class="line">            dp_cost[i] = min(cost[i]+dp_cost[i-1], cost[i]+dp_cost[i-2])</span><br><span class="line"></span><br><span class="line">        return min(dp_cost[-1], dp_cost[-2])</span><br></pre></td></tr></table></figure><h3 id="4-2-最大子数组和"><a href="#4-2-最大子数组和" class="headerlink" title="4.2 最大子数组和"></a>4.2 最大子数组和</h3><p>关键是如何定义一个无后效性的子问题：以nums[i]结尾的子数组的和</p><ul><li>状态转移方程：<br>dp[i]&#x3D;max(nums[i],nums[i]+dp[i−1])dp[i]&#x3D;max(nums[i],nums[i]+dp[i-1])dp[i]&#x3D;max(nums[i],nums[i]+dp[i−1])</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_10.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def maxSubArray(self, nums: List[int]) -&gt; int:</span><br><span class="line">        L = len(nums)</span><br><span class="line">        if L == 0:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        dp = [0] * L</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        max_sum = dp[0]</span><br><span class="line"></span><br><span class="line">        for i in range(1, L):</span><br><span class="line">            dp[i] = max(nums[i], nums[i] + dp[i-1])</span><br><span class="line">            max_sum = max(dp[i], max_sum)</span><br><span class="line"></span><br><span class="line">        return max_sum</span><br></pre></td></tr></table></figure><h3 id="4-3-打家劫舍"><a href="#4-3-打家劫舍" class="headerlink" title="4.3 打家劫舍"></a>4.3 打家劫舍</h3><p>子问题：当前位置的最大金额</p><ul><li>状态转移方程：<br>dp[i]&#x3D;max(dp[i−1],nums[i]+dp[i−2])dp[i]&#x3D;max(dp[i-1],nums[i]+dp[i-2])dp[i]&#x3D;max(dp[i−1],nums[i]+dp[i−2])</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_3.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def rob(self, nums):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        L = len(nums)</span><br><span class="line">        if L == 0:</span><br><span class="line">            return 0</span><br><span class="line">        if L == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        if L == 2:</span><br><span class="line">            return max(nums[0], nums[1])</span><br><span class="line"></span><br><span class="line">        dp = [0] * len(nums)</span><br><span class="line">        dp[0], dp[1] = nums[0], max(nums[0], nums[1])</span><br><span class="line"></span><br><span class="line">        for i in range(2, L):</span><br><span class="line">            dp[i] = max(dp[i-1], nums[i] + dp[i-2])</span><br><span class="line"></span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></table></figure><h3 id="4-4-完全平方数"><a href="#4-4-完全平方数" class="headerlink" title="4.4 完全平方数"></a>4.4 完全平方数</h3><p>子问题：当前的数字的最少数量</p><ul><li>状态转移方程：<br>dp[i]&#x3D;minj&#x3D;1idp[i−j2]+1dp[i]&#x3D;min_{j&#x3D;1}^{\sqrt{i} }dp[i-j^{2}] + 1dp[i]&#x3D;minj&#x3D;1i​​dp[i−j2]+1</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_6.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def numSquares(self, n: int) -&gt; int:</span><br><span class="line">        dp = [0] * (n + 1)</span><br><span class="line"></span><br><span class="line">        for i in range(1, n+1):</span><br><span class="line">            min_num = i</span><br><span class="line">            ul = int(pow(i, 0.5)) + 1</span><br><span class="line">            for j in range(1, ul):</span><br><span class="line">                min_num = min(min_num, dp[i-j**2])</span><br><span class="line">            dp[i] = min_num + 1</span><br><span class="line"></span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></table></figure><h3 id="4-5-分割等和子集"><a href="#4-5-分割等和子集" class="headerlink" title="4.5 分割等和子集"></a>4.5 分割等和子集</h3><p>转化为背包问题，能不能放满背包，背包大小为和的一半</p><ul><li>状态转移方程：<br>dp[i][j]&#x3D;{max(dp[i−1][j−nums[i]]+nums[i],dp[i−1][j]),j&gt;&#x3D;nums[i]dp[i−1][j],j&lt;nums[i]dp[i][j]&#x3D;\left{\begin{matrix}max(dp[i-1][j-nums[i]]+nums[i],dp[i-1][j]),<br>&amp; j&gt;&#x3D;nums[i] \dp[i-1][j],<br>&amp; j&lt;nums[i]<br>\end{matrix}\right.dp[i][j]&#x3D;{max(dp[i−1][j−nums[i]]+nums[i],dp[i−1][j]),dp[i−1][j],​j&gt;&#x3D;nums[i]j&lt;nums[i]​<br><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_4.png" alt="在这里插入图片描述"></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def canPartition(self, nums):</span><br><span class="line">        L = len(nums)</span><br><span class="line">        all_sum = sum(nums)</span><br><span class="line">        if all_sum % 2 != 0:</span><br><span class="line">            return False</span><br><span class="line"></span><br><span class="line">        average = int(all_sum / 2)</span><br><span class="line"></span><br><span class="line">        F = [[0] * (average+1) for _ in range(L+1)]</span><br><span class="line"></span><br><span class="line">        for i in range(1, L+1):</span><br><span class="line">            for j in range(1, average+1):</span><br><span class="line">                if j &gt;= nums[i-1]:</span><br><span class="line">                    F[i][j] = max(F[i-1][j], nums[i-1] + F[i-1][j-nums[i-1]])</span><br><span class="line">                else:</span><br><span class="line">                    F[i][j] = F[i-1][j]</span><br><span class="line">                if F[i][j] == average:</span><br><span class="line">                    # print(F)</span><br><span class="line">                    return True</span><br><span class="line">        # print(F)</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure><p>实际上不需要算和，只需要判断能否装满就行，只进行异或运算就行，会快很多</p><ul><li>状态转移方程：<br>dp[i][j]&#x3D;{dp[i−1][j−nums[i]]ordp[i−1][j],j&gt;nums[i]True,j&#x3D;&#x3D;nums[i]dp[i−1][j],j&lt;nums[i]dp[i][j]&#x3D;\left{\begin{matrix} dp[i-1][j-nums[i]] or dp[i-1][j],<br>&amp; j&gt;nums[i] \True,<br>&amp; j&#x3D;&#x3D;nums[i] \ dp[i-1][j],<br>&amp; j&lt;nums[i]<br>\end{matrix}\right.dp[i][j]&#x3D;⎩⎨⎧​dp[i−1][j−nums[i]]ordp[i−1][j],True,dp[i−1][j],​j&gt;nums[i]j&#x3D;&#x3D;nums[i]j&lt;nums[i]​</li></ul><h3 id="4-6-最长有效括号"><a href="#4-6-最长有效括号" class="headerlink" title="4.6 最长有效括号"></a>4.6 最长有效括号</h3><p>状态转移方程需分情况讨论 (这题的其他解法栈、双向扫描也很有意思)</p><ul><li>状态转移方程：<br>待写</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_8.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-7-统计全为1的正方形子矩阵"><a href="#4-7-统计全为1的正方形子矩阵" class="headerlink" title="4.7 统计全为1的正方形子矩阵"></a>4.7 统计全为1的正方形子矩阵</h3><p>dp[i][j]既可以代表数量，也可以表示边长（因为是正方形），因此会有一些基于此的变形题目。</p><ul><li>状态转移方程：(注意nums的判断里减1是因为dp数组添加了一维)<br>dp[i][j]&#x3D;{min(dp[i−1][j],dp[i][j−1],dp[i−1][j−1]),nums[i−1][j−1]&#x3D;10,nums[i−1][j−1]&#x3D;0dp[i][j]&#x3D;\left{\begin{matrix}<br>min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1]), &amp; nums[i-1][j-1]&#x3D;1 &amp; \<br>0, &amp; nums[i-1][j-1]&#x3D;0 &amp;<br>\end{matrix}\right.dp[i][j]&#x3D;{min(dp[i−1][j],dp[i][j−1],dp[i−1][j−1]),0,​nums[i−1][j−1]&#x3D;1nums[i−1][j−1]&#x3D;0​​</li></ul><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_5.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def countSquares(self, matrix: List[List[int]]) -&gt; int:</span><br><span class="line">        M = len(matrix)</span><br><span class="line">        N = len(matrix[0])</span><br><span class="line"></span><br><span class="line">        dp = [[0] * (N+1) for _ in range(M+1)]</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        for i in range(1, M+1):</span><br><span class="line">            for j in range(1, N+1):</span><br><span class="line">                if matrix[i-1][j-1] == 1:</span><br><span class="line">                    dp[i][j] = min(dp[i-1][j-1], dp[i][j-1], dp[i-1][j]) + 1</span><br><span class="line">                else:</span><br><span class="line">                    dp[i][j] = 0</span><br><span class="line">                count += dp[i][j]</span><br><span class="line"></span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h3 id="4-8-统计全1子矩形"><a href="#4-8-统计全1子矩形" class="headerlink" title="4.8 统计全1子矩形"></a>4.8 统计全1子矩形</h3><p>这里要统计的变成了长方形，不能够简单地从先前的dp数组中获取需要的值，因为长宽可变，需要依赖先前的中间结果。具体解法为前缀和统计一边的长度再根据高计算数量。(反复统计边长的步骤可以用单调栈进一步减少计算量，具体见leetcode题解)<br><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_9.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def numSubmat(self, mat: List[List[int]]) -&gt; int:</span><br><span class="line">        M = len(mat)</span><br><span class="line">        N = len(mat[0])</span><br><span class="line"></span><br><span class="line">        row_p_sum = [[0] * (N+1) for _ in range(M+1)]</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        for i in range(1, M+1):</span><br><span class="line">            for j in range(1, N+1):</span><br><span class="line">                if mat[i-1][j-1] == 1:</span><br><span class="line">                    row_p_sum[i][j] = row_p_sum[i][j-1] + 1</span><br><span class="line">                else:</span><br><span class="line">                    row_p_sum[i][j] = 0</span><br><span class="line">                k = i</span><br><span class="line">                cur_min = row_p_sum[i][j]</span><br><span class="line">                # 下面实际上是在统计dp[i][j]</span><br><span class="line">                while row_p_sum[k][j] != 0:</span><br><span class="line">                    cur_min = min(cur_min, row_p_sum[k][j])</span><br><span class="line">                    count += cur_min</span><br><span class="line">                    k -= 1</span><br><span class="line"></span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h2 id="5-贪心算法"><a href="#5-贪心算法" class="headerlink" title="5. 贪心算法"></a>5. 贪心算法</h2><p>每次决策前，都采用最优解</p><h3 id="5-1-跳跃游戏"><a href="#5-1-跳跃游戏" class="headerlink" title="5.1 跳跃游戏"></a>5.1 跳跃游戏</h3><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_7.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def jump(self, nums: List[int]) -&gt; int:</span><br><span class="line">        L = len(nums)</span><br><span class="line">        if L == 1:</span><br><span class="line">            return 0</span><br><span class="line">        cur_pos = 0</span><br><span class="line">        count = 0</span><br><span class="line"></span><br><span class="line">        for _ in range(L):</span><br><span class="line">            max_len = 0</span><br><span class="line">            t_pos = 0</span><br><span class="line">            # 遍历前判断一次</span><br><span class="line">            if cur_pos + nums[cur_pos] &gt;= L-1:</span><br><span class="line">                return count + 1</span><br><span class="line">            for i in range(cur_pos+1, cur_pos+nums[cur_pos]+1):</span><br><span class="line">                if nums[i]+i &gt;= max_len:</span><br><span class="line">                    max_len = nums[i] + i</span><br><span class="line">                    t_pos = i</span><br><span class="line">            cur_pos = t_pos</span><br><span class="line">            count += 1</span><br></pre></td></tr></table></figure><h2 id="6-拓扑排序"><a href="#6-拓扑排序" class="headerlink" title="6. 拓扑排序"></a>6. 拓扑排序</h2><p>涉及到图的先后顺序问题，考虑拓扑排序</p><h3 id="6-1-课程表"><a href="#6-1-课程表" class="headerlink" title="6.1 课程表"></a>6.1 课程表</h3><p><img src="/2024/03/02/12_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%BA%8C%E3%80%91%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%81%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E3%80%91/img_2.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def canFinish(self, numCourses: int, prerequisites: List[List[int]]) -&gt; bool:</span><br><span class="line">        graph_hash_dict = &#123;&#125;  # 存储每个节点的出度，方便查找</span><br><span class="line">        in_degree = [0] * numCourses  # 记录每个节点的入度</span><br><span class="line"></span><br><span class="line">        # 统计节点的出入度</span><br><span class="line">        for t in prerequisites:</span><br><span class="line">            cour, pre_cour = t[0], t[1]</span><br><span class="line">            in_degree[cour] += 1</span><br><span class="line">            if graph_hash_dict.__contains__(pre_cour):</span><br><span class="line">                graph_hash_dict[pre_cour].append(cour)</span><br><span class="line">            else:</span><br><span class="line">                graph_hash_dict[pre_cour] = [cour]</span><br><span class="line"></span><br><span class="line">        # 将入读为0的点加入到队列中</span><br><span class="line">        delete_node_list = []</span><br><span class="line">        for i in range(numCourses):</span><br><span class="line">            if in_degree[i] == 0:</span><br><span class="line">                delete_node_list.append(i)</span><br><span class="line"></span><br><span class="line">        # 进行类似于BFS的查找</span><br><span class="line">        while len(delete_node_list) &gt; 0:</span><br><span class="line">            cour = delete_node_list.pop(0)</span><br><span class="line">            if graph_hash_dict.__contains__(cour):</span><br><span class="line">                out_node_list = graph_hash_dict[cour]</span><br><span class="line">                for node in out_node_list:</span><br><span class="line">                    in_degree[node] -= 1</span><br><span class="line">                    # 有新的节点出度变为0，加入到删除列表中</span><br><span class="line">                    if in_degree[node] == 0:</span><br><span class="line">                        delete_node_list.append(node)</span><br><span class="line"></span><br><span class="line">        # 最后看是否所有节点的入度都变成0</span><br><span class="line">        for i in range(numCourses):</span><br><span class="line">            if in_degree[i] != 0:</span><br><span class="line">                return False</span><br><span class="line">        return True</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法学习记录【一】【DFS、BFS、滑动窗口】</title>
      <link href="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/"/>
      <url>/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h2 id="1-深度优先搜索"><a href="#1-深度优先搜索" class="headerlink" title="1. 深度优先搜索"></a>1. 深度优先搜索</h2><p>python语法中传变量到递归中是形参，传列表则是实参</p><h3 id="1-1-岛屿数量"><a href="#1-1-岛屿数量" class="headerlink" title="1.1 岛屿数量"></a>1.1 岛屿数量</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_10.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def numIslands(self, grid: List[List[str]]) -&gt; int:</span><br><span class="line">        R = len(grid)</span><br><span class="line">        C = len(grid[0])</span><br><span class="line"></span><br><span class="line">        used_list = [[False] * C for _ in range(R)]</span><br><span class="line"></span><br><span class="line">        def dfs(y, x, used_list):</span><br><span class="line">            used_list[y][x] = True</span><br><span class="line">            if x+1&lt;C and used_list[y][x+1] == False and grid[y][x+1] == &#x27;1&#x27;:</span><br><span class="line">                used_list = dfs(y, x+1, used_list)</span><br><span class="line">            if x-1&gt;=0 and used_list[y][x-1] == False and grid[y][x-1] == &#x27;1&#x27;:</span><br><span class="line">                used_list = dfs(y, x-1, used_list)</span><br><span class="line">            if y+1&lt;R and used_list[y+1][x] == False and grid[y+1][x] == &#x27;1&#x27;:</span><br><span class="line">                used_list = dfs(y+1, x, used_list)</span><br><span class="line">            if y-1&gt;=0 and used_list[y-1][x] == False and grid[y-1][x] == &#x27;1&#x27;:</span><br><span class="line">                used_list = dfs(y-1, x, used_list)</span><br><span class="line">            return used_list</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        for i in range(R):</span><br><span class="line">            for j in range(C):</span><br><span class="line">                if used_list[i][j] == False and grid[i][j] == &#x27;1&#x27;:</span><br><span class="line">                    count += 1</span><br><span class="line">                    used_list = dfs(i, j, used_list)</span><br><span class="line"></span><br><span class="line">        # print(used_list)</span><br><span class="line"></span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h3 id="1-2-组合总数"><a href="#1-2-组合总数" class="headerlink" title="1.2 组合总数"></a>1.2 组合总数</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_1.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def combinationSum(self, candidates: List[int], target: int) -&gt; List[List[int]]:</span><br><span class="line">        L = len(candidates)</span><br><span class="line">        # used_list = [False] * L</span><br><span class="line">        cur_can = []</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        def dfs(start):</span><br><span class="line">            sum_cur = sum(cur_can)</span><br><span class="line">            if sum_cur == target:</span><br><span class="line">                result.append(cur_can.copy())</span><br><span class="line">            elif sum_cur &gt; target:</span><br><span class="line">                return 0</span><br><span class="line">            else:</span><br><span class="line">                for i in range(start, L):</span><br><span class="line">                    cur_can.append(candidates[i])</span><br><span class="line">                    dfs(i)</span><br><span class="line">                    cur_can.pop(-1)</span><br><span class="line"></span><br><span class="line">        dfs(0)</span><br><span class="line"></span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><h3 id="1-3-全排列"><a href="#1-3-全排列" class="headerlink" title="1.3 全排列"></a>1.3 全排列</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_4.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def permute(self, nums: List[int]) -&gt; List[List[int]]:</span><br><span class="line">        L = len(nums)</span><br><span class="line">        result = []</span><br><span class="line">        used_list = [False] * L</span><br><span class="line">        st = [0] * L</span><br><span class="line"></span><br><span class="line">        def dfs(i, st):</span><br><span class="line">            # print(st)</span><br><span class="line">            if i == L:</span><br><span class="line">                result.append(st.copy())</span><br><span class="line">            else:</span><br><span class="line">                for j in range(L):</span><br><span class="line">                    if used_list[j] == False:</span><br><span class="line">                        st[i] = nums[j]</span><br><span class="line">                        used_list[j] = True</span><br><span class="line">                        dfs(i+1, st)</span><br><span class="line">                        used_list[j] = False</span><br><span class="line"></span><br><span class="line">        dfs(0, st)</span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><h3 id="1-4-二叉树的前中后序遍历"><a href="#1-4-二叉树的前中后序遍历" class="headerlink" title="1.4 二叉树的前中后序遍历"></a>1.4 二叉树的前中后序遍历</h3><p>前序：中-&gt;左-&gt;右<br>中序：左-&gt;中-&gt;右<br>后序：左-&gt;右-&gt;中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 前序遍历</span><br><span class="line">class Solution:</span><br><span class="line">    def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:</span><br><span class="line">        result = []</span><br><span class="line">        def dfs(node):</span><br><span class="line">            nonlocal result</span><br><span class="line">            if node is None:</span><br><span class="line">                return</span><br><span class="line">            result.append(node.val)</span><br><span class="line">            dfs(node.left)</span><br><span class="line">            dfs(node.right)</span><br><span class="line">        dfs(root)</span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><h3 id="1-5-从中序与后序遍历构建二叉树"><a href="#1-5-从中序与后序遍历构建二叉树" class="headerlink" title="1.5 从中序与后序遍历构建二叉树"></a>1.5 从中序与后序遍历构建二叉树</h3><p>中序和后序、中序和前序都可以唯一确定一棵二叉树，前序和后序有时候无法确定一棵二叉树</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Definition for a binary tree node.</span><br><span class="line"># class TreeNode:</span><br><span class="line">#     def __init__(self, val=0, left=None, right=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.left = left</span><br><span class="line">#         self.right = right</span><br><span class="line">class Solution:</span><br><span class="line">    def buildTree(self, inorder: List[int], postorder: List[int]) -&gt; Optional[TreeNode]:</span><br><span class="line"></span><br><span class="line">        def dfs(inorder, postorder):</span><br><span class="line">            if len(postorder) &gt; 0:</span><br><span class="line">                node = TreeNode(postorder[-1])</span><br><span class="line">                index = inorder.index(postorder[-1])</span><br><span class="line">                # print(inorder[:index], postorder[:index])</span><br><span class="line">                # print(inorder[index+1:], postorder[index:-2])</span><br><span class="line">                node.left = dfs(inorder[:index], postorder[:index])</span><br><span class="line">                node.right = dfs(inorder[index+1:], postorder[index:-1])</span><br><span class="line">                return node</span><br><span class="line">            else:</span><br><span class="line">                return None</span><br><span class="line"></span><br><span class="line">        return dfs(inorder, postorder)</span><br></pre></td></tr></table></figure><h2 id="2-广度优先搜索"><a href="#2-广度优先搜索" class="headerlink" title="2. 广度优先搜索"></a>2. 广度优先搜索</h2><h3 id="2-1-二叉树的层序遍历"><a href="#2-1-二叉树的层序遍历" class="headerlink" title="2.1 二叉树的层序遍历"></a>2.1 二叉树的层序遍历</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_7.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Definition for a binary tree node.</span><br><span class="line"># class TreeNode:</span><br><span class="line">#     def __init__(self, val=0, left=None, right=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.left = left</span><br><span class="line">#         self.right = right</span><br><span class="line">class Solution:</span><br><span class="line">    def levelOrder(self, root: Optional[TreeNode]) -&gt; List[List[int]]:</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        def bfs(root):</span><br><span class="line">            if root is None:</span><br><span class="line">                return []</span><br><span class="line">            queue_list = [root]</span><br><span class="line">            while len(queue_list) &gt; 0:</span><br><span class="line">                t = []</span><br><span class="line">                for i in range(len(queue_list)):</span><br><span class="line">                    p = queue_list.pop(0)</span><br><span class="line">                    t.append(p.val)</span><br><span class="line">                    if p.left is not None:</span><br><span class="line">                        queue_list.append(p.left)</span><br><span class="line">                    if p.right is not None:</span><br><span class="line">                        queue_list.append(p.right)  </span><br><span class="line">                result.append(t)</span><br><span class="line"></span><br><span class="line">        bfs(root)</span><br><span class="line"></span><br><span class="line">        return result</span><br></pre></td></tr></table></figure><h3 id="2-2-岛屿数量"><a href="#2-2-岛屿数量" class="headerlink" title="2.2 岛屿数量"></a>2.2 岛屿数量</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_2.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def numIslands(self, grid: List[List[str]]) -&gt; int:</span><br><span class="line">        R = len(grid)</span><br><span class="line">        C = len(grid[0])</span><br><span class="line"></span><br><span class="line">        used_list = [[False] * C for _ in range(R)]</span><br><span class="line"></span><br><span class="line">        def bfs(i, j, used_list):</span><br><span class="line">            queue_list = [[i, j]]</span><br><span class="line">            used_list[i][j] = True</span><br><span class="line">            while len(queue_list) &gt; 0:</span><br><span class="line">                i,j = queue_list.pop()</span><br><span class="line">                if i-1&gt;=0 and used_list[i-1][j] == False and grid[i-1][j] == &#x27;1&#x27;:</span><br><span class="line">                    used_list[i-1][j] = True</span><br><span class="line">                    queue_list.append([i-1,j])</span><br><span class="line">                if i+1&lt;R and used_list[i+1][j] == False and grid[i+1][j] == &#x27;1&#x27;:</span><br><span class="line">                    used_list[i+1][j] = True</span><br><span class="line">                    queue_list.append([i+1,j])</span><br><span class="line">                if j-1&gt;=0 and used_list[i][j-1] == False and grid[i][j-1] == &#x27;1&#x27;:</span><br><span class="line">                    used_list[i][j-1] = True</span><br><span class="line">                    queue_list.append([i,j-1])</span><br><span class="line">                if j+1&lt;C and used_list[i][j+1] == False and grid[i][j+1] == &#x27;1&#x27;:</span><br><span class="line">                    used_list[i][j+1] = True</span><br><span class="line">                    queue_list.append([i,j+1])</span><br><span class="line">            return used_list</span><br><span class="line"></span><br><span class="line">        count = 0</span><br><span class="line">        for i in range(R):</span><br><span class="line">            for j in range(C):</span><br><span class="line">                if used_list[i][j] == False and grid[i][j] == &#x27;1&#x27;:</span><br><span class="line">                    count += 1</span><br><span class="line">                    used_list = bfs(i, j, used_list)</span><br><span class="line"></span><br><span class="line">        # print(used_list)</span><br><span class="line"></span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h3 id="2-3-腐烂的橘子"><a href="#2-3-腐烂的橘子" class="headerlink" title="2.3 腐烂的橘子"></a>2.3 腐烂的橘子</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_8.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def orangesRotting(self, grid: List[List[int]]) -&gt; int:</span><br><span class="line">        M = len(grid)</span><br><span class="line">        N = len(grid[0])</span><br><span class="line">        bad_orange_list = []</span><br><span class="line">        good_orange_set = set()</span><br><span class="line"></span><br><span class="line">        # 将坏橘子加入队列，并记录好的橘子</span><br><span class="line">        for i in range(M):</span><br><span class="line">            for j in range(N):</span><br><span class="line">                if grid[i][j] == 2:</span><br><span class="line">                    bad_orange_list.append([i, j])</span><br><span class="line">                if grid[i][j] == 1:</span><br><span class="line">                    good_orange_set.add((i, j))</span><br><span class="line"></span><br><span class="line">        def bfs(bad_orange_list, good_orange_set):</span><br><span class="line">            count = 0</span><br><span class="line">            # 直接使用grid就可记录是否使用，不需要新建used_list</span><br><span class="line">            while len(bad_orange_list) &gt; 0:</span><br><span class="line">                Flag = False # 判断当此是否进行了更新</span><br><span class="line">                for itr in range(len(bad_orange_list)):</span><br><span class="line">                    i, j = bad_orange_list.pop(0)</span><br><span class="line">                    d_i = [-1, 0, 1, 0]</span><br><span class="line">                    d_j = [0, -1, 0, 1]</span><br><span class="line">                    for k in range(4):</span><br><span class="line">                        m = i + d_i[k]</span><br><span class="line">                        n = j + d_j[k]</span><br><span class="line">                        if 0&lt;=m&lt;M and 0&lt;=n&lt;N and grid[m][n] == 1:</span><br><span class="line">                            Flag = True</span><br><span class="line">                            grid[m][n] = 2</span><br><span class="line">                            bad_orange_list.append([m, n])</span><br><span class="line">                            good_orange_set.remove((m, n))</span><br><span class="line">                if Flag:</span><br><span class="line">                    count += 1</span><br><span class="line">            return count</span><br><span class="line"></span><br><span class="line">        count = bfs(bad_orange_list, good_orange_set)</span><br><span class="line"></span><br><span class="line">        if len(good_orange_set) &gt; 0:</span><br><span class="line">            return -1</span><br><span class="line">        return count</span><br></pre></td></tr></table></figure><h2 id="3-滑动窗口"><a href="#3-滑动窗口" class="headerlink" title="3. 滑动窗口"></a>3. 滑动窗口</h2><h3 id="3-1-找到字符串中所有字母异位词"><a href="#3-1-找到字符串中所有字母异位词" class="headerlink" title="3.1 找到字符串中所有字母异位词"></a>3.1 找到字符串中所有字母异位词</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_5.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findAnagrams(self, s: str, p: str) -&gt; List[int]:</span><br><span class="line">        need_hash_dict = &#123;&#125;</span><br><span class="line">        window_hash_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        # 统计需要的字符，便于后面进行判断</span><br><span class="line">        for t in p:</span><br><span class="line">            need_hash_dict[t] = need_hash_dict.get(t, 0) + 1</span><br><span class="line"></span><br><span class="line">        left, right = 0, 0</span><br><span class="line">        Ls = len(s)</span><br><span class="line">        Lp = len(p)</span><br><span class="line"></span><br><span class="line">        valid = 0  # 该变量记录达到目标的字符数</span><br><span class="line">        reault = []</span><br><span class="line"></span><br><span class="line">        while right &lt; Ls:</span><br><span class="line">            # 1. 右移动窗口</span><br><span class="line">            c = s[right]</span><br><span class="line">            right += 1</span><br><span class="line"></span><br><span class="line">            # 1.1 右移窗口需要做的一些处理</span><br><span class="line">            if need_hash_dict.__contains__(c):</span><br><span class="line">                window_hash_dict[c] = window_hash_dict.get(c, 0) + 1</span><br><span class="line">                if window_hash_dict[c] == need_hash_dict[c]:</span><br><span class="line">                    valid += 1</span><br><span class="line"></span><br><span class="line">            # 2. 判断是否需要左移窗口</span><br><span class="line">            while right-left&gt;=Lp:</span><br><span class="line">                # 2.1 满足条件添加结果</span><br><span class="line">                if valid == len(need_hash_dict):</span><br><span class="line">                    reault.append(left)</span><br><span class="line">                d = s[left]</span><br><span class="line">                left += 1</span><br><span class="line">                # 2.2 左移窗口的一些处理</span><br><span class="line">                if need_hash_dict.__contains__(d):</span><br><span class="line">                    if window_hash_dict[d] == need_hash_dict[d]:</span><br><span class="line">                        valid -= 1</span><br><span class="line">                    window_hash_dict[d] -= 1</span><br><span class="line"></span><br><span class="line">        return reault</span><br></pre></td></tr></table></figure><h3 id="3-2-最小覆盖子串"><a href="#3-2-最小覆盖子串" class="headerlink" title="3.2 最小覆盖子串"></a>3.2 最小覆盖子串</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_6.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def minWindow(self, s: str, t: str) -&gt; str:</span><br><span class="line">        need_hash_dict = &#123;&#125;</span><br><span class="line">        window_hash_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        for c in t:</span><br><span class="line">            need_hash_dict[c] = need_hash_dict.get(c, 0) + 1</span><br><span class="line"></span><br><span class="line">        left, right = 0, 0</span><br><span class="line">        valid = 0</span><br><span class="line">        Ls = len(s)</span><br><span class="line">        Lt = len(t)</span><br><span class="line"></span><br><span class="line">        # 保存结果，起始坐标和长度</span><br><span class="line">        start = 0</span><br><span class="line">        tlen = Ls</span><br><span class="line">        Flag = False # 用于判断是否存在</span><br><span class="line"></span><br><span class="line">        while right &lt; Ls:</span><br><span class="line">            # 1. 右移窗口</span><br><span class="line">            c = s[right]</span><br><span class="line">            right += 1</span><br><span class="line"></span><br><span class="line">            # 1.1 右移窗口需要做的一些处理</span><br><span class="line">            if need_hash_dict.__contains__(c):</span><br><span class="line">                window_hash_dict[c] = window_hash_dict.get(c, 0) + 1</span><br><span class="line">                if window_hash_dict[c] == need_hash_dict[c]:</span><br><span class="line">                    valid += 1</span><br><span class="line"></span><br><span class="line">            # print(c, valid, left, right, window_hash_dict)</span><br><span class="line"></span><br><span class="line">            # 2. 判断是否需要左移窗口</span><br><span class="line">            while valid == len(need_hash_dict):</span><br><span class="line">                # 2.1 更新结果</span><br><span class="line">                Flag = True</span><br><span class="line">                if right - left &lt; tlen:</span><br><span class="line">                    start = left</span><br><span class="line">                    tlen = right - left</span><br><span class="line">                d = s[left]</span><br><span class="line">                left += 1</span><br><span class="line"></span><br><span class="line">                # 2.2 做移窗口需要做的一些处理</span><br><span class="line">                if need_hash_dict.__contains__(d):</span><br><span class="line">                    if window_hash_dict[d] == need_hash_dict[d]:</span><br><span class="line">                        valid -= 1</span><br><span class="line">                    window_hash_dict[d] -= 1</span><br><span class="line"></span><br><span class="line">        if Flag:</span><br><span class="line">            return s[start:start+tlen]</span><br><span class="line">        else:</span><br><span class="line">            return &#x27;&#x27;</span><br></pre></td></tr></table></figure><h3 id="3-3-字符串的排列"><a href="#3-3-字符串的排列" class="headerlink" title="3.3 字符串的排列"></a>3.3 字符串的排列</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_3.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def checkInclusion(self, s1: str, s2: str) -&gt; bool:</span><br><span class="line">        need_hash_dict = &#123;&#125;</span><br><span class="line">        window_hash_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        for t in s1:</span><br><span class="line">            need_hash_dict[t] = need_hash_dict.get(t, 0) + 1</span><br><span class="line"></span><br><span class="line">        Ls1 = len(s1)</span><br><span class="line">        Ls2 = len(s2)</span><br><span class="line">        left, right = 0, 0</span><br><span class="line">        valid = 0</span><br><span class="line"></span><br><span class="line">        while right &lt; Ls2:</span><br><span class="line">            c = s2[right]</span><br><span class="line">            right += 1</span><br><span class="line"></span><br><span class="line">            if need_hash_dict.__contains__(c):</span><br><span class="line">                window_hash_dict[c] = window_hash_dict.get(c, 0) + 1</span><br><span class="line">                if window_hash_dict[c] == need_hash_dict[c]:</span><br><span class="line">                    valid += 1</span><br><span class="line"></span><br><span class="line">            # print(c, valid, window_hash_dict)</span><br><span class="line"></span><br><span class="line">            while right-left &gt;= Ls1:</span><br><span class="line">                if valid == len(need_hash_dict):</span><br><span class="line">                    return True</span><br><span class="line"></span><br><span class="line">                d = s2[left]</span><br><span class="line">                left += 1</span><br><span class="line"></span><br><span class="line">                if need_hash_dict.__contains__(d):</span><br><span class="line">                    if window_hash_dict[d] == need_hash_dict[d]:</span><br><span class="line">                        valid -= 1</span><br><span class="line">                    window_hash_dict[d] -= 1</span><br><span class="line"></span><br><span class="line">        return False</span><br></pre></td></tr></table></figure><h3 id="3-4-无重复字符的最长子串"><a href="#3-4-无重复字符的最长子串" class="headerlink" title="3.4 无重复字符的最长子串"></a>3.4 无重复字符的最长子串</h3><p><img src="/2024/03/01/11_%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E3%80%90%E4%B8%80%E3%80%91%E3%80%90DFS%E3%80%81BFS%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%91/img_9.png" alt="在这里插入图片描述"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def lengthOfLongestSubstring(self, s: str) -&gt; int:</span><br><span class="line">        window_hash_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        Ls = len(s)</span><br><span class="line">        if Ls == 0:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        left, right = 0, 0</span><br><span class="line">        valid = 0</span><br><span class="line">        max_len = 0</span><br><span class="line"></span><br><span class="line">        while right &lt; Ls:</span><br><span class="line">            c = s[right]</span><br><span class="line">            right += 1</span><br><span class="line"></span><br><span class="line">            if window_hash_dict.__contains__(c):</span><br><span class="line">                window_hash_dict[c] += 1</span><br><span class="line">                if window_hash_dict[c] &gt;= 2:</span><br><span class="line">                    valid += 1</span><br><span class="line">            else:</span><br><span class="line">                window_hash_dict[c] = 1</span><br><span class="line"></span><br><span class="line">            # print(c, valid, window_hash_dict)</span><br><span class="line"></span><br><span class="line">            while valid &gt; 0:</span><br><span class="line">                # print(left)</span><br><span class="line">                d = s[left]</span><br><span class="line">                left += 1</span><br><span class="line">                if window_hash_dict.__contains__(d):</span><br><span class="line">                    if window_hash_dict[d] &gt; 1:</span><br><span class="line">                        valid -= 1</span><br><span class="line">                    window_hash_dict[d] -= 1</span><br><span class="line"></span><br><span class="line">            # 更新结果的地方要写对</span><br><span class="line">            max_len = max(max_len, right - left)</span><br><span class="line"></span><br><span class="line">        return max_len</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer详解</title>
      <link href="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="Transformer详解"><a href="#Transformer详解" class="headerlink" title="Transformer详解"></a>Transformer详解</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>Transformer是一个面向<strong>sequence to sequence</strong>任务的模型，在17年的论文《Attention is all you need》中首次提出。Transformer 是第一个完全依赖自注意力（self-attention）来计算输入和输出的表示，而不使用序列对齐的递归神经网络或卷积神经网络的转换模型。</p><h4 id="1-1-Sequence-to-Sequence"><a href="#1-1-Sequence-to-Sequence" class="headerlink" title="1.1 Sequence to Sequence"></a>1.1 Sequence to Sequence</h4><p>简单地说就是输入一个向量，输出一个向量，两个向量长度不一定相等，如翻译任务等。Decoder这里要注意，其前面的输出会作为当前的输入，然后重复下去。<br> 停止这个重复过程有AT（Autoregressive）和NAT（Non-autoregressive）两种方式。<br> <strong>AT</strong>：增加一个特殊的字符end，当输出为end，就停止<br> <strong>NAT</strong>：让模型输出一定长度的结果后截止，然后从中截取出需要的部分<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_6.png" alt="在这里插入图片描述"></p><h3 id="2-Transformer网络结构"><a href="#2-Transformer网络结构" class="headerlink" title="2. Transformer网络结构"></a>2. Transformer网络结构</h3><p>网络结构如下图所示，左边为编码器（encoder），右边为解码器（decoder）。<br> <strong>编码器</strong>：编码器是由N&#x3D;6个相同的层堆叠而成。每层有两个子层。第一层是一个<code>multi-head self-attention</code>机制，第二层是一个简单的、按位置排列的全连接前馈网络。两个子层都采用了一个<code>residual</code>(残差)连接，然后进行层的归一化。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x)是由子层本身的输出。<br> <strong>解码器</strong>：解码器也是由N&#x3D;6个相同层的堆栈组成。除了每个编码器层的两个子层之外，解码器还插入了第三个子层，它对编码器堆栈的输出进行<code>multi-head self-attention</code>。与编码器类似，两个子层都采用了一个<code>residual</code>(残差)连接，然后进行层的归一化。为确保对位置i的预测只取决于小于i的位置的已知输出，修改了解码器堆栈中的<code>multi-head self-attention</code>层。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_3.png" alt="在这里插入图片描述"></p><h4 id="2-1-Self-attention"><a href="#2-1-Self-attention" class="headerlink" title="2.1 Self-attention"></a>2.1 Self-attention</h4><p>self-attention做的就是每一输出都与所有的输入有关，从而有效地利用上下文信息，如下图所示，a为输入，b为输出。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_1.png" alt="在这里插入图片描述"></p><h5 id="计算步骤：（以b1的计算为例）"><a href="#计算步骤：（以b1的计算为例）" class="headerlink" title="计算步骤：（以b1的计算为例）"></a>计算步骤：（以b1的计算为例）</h5><p><strong>q &#x3D; Wq * a ， k &#x3D; Wk * a ， v &#x3D; Wv * a</strong>；其中Wq、Wk、Wv为三个矩阵（或向量），通过学习得到。</p><ul><li>q与各个输入点乘，得到了它们之间的相关性，即下图中的Alpha（可以理解成分数或权重），一般会再通过Soft-max</li><li>v为各个输入的特征，各个Alpha与v相乘求和得到b1</li><li>b2等的计算类似，最后易得其实就是矩阵运算<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_5.png" alt="在这里插入图片描述"></li></ul><h5 id="计算公式："><a href="#计算公式：" class="headerlink" title="计算公式："></a>计算公式：</h5><p>Q、K、V即上面的Wq、Wk、Wv，dk是缩放因子<br> 除以dk的原因：点积得到的结果维度很大，使得结果处于softmax函数梯度很小的区域，除以一个缩放因子，可以一定程度上减缓这种情况。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_8.png" alt="在这里插入图片描述"></p><h5 id="加上位置信息（Positional-Encoding）："><a href="#加上位置信息（Positional-Encoding）：" class="headerlink" title="加上位置信息（Positional Encoding）："></a>加上位置信息（Positional Encoding）：</h5><p>可以看到，上面各个输入之间没有位置信息，所以需要先进行positional encoding再进行计算，以利用位置信息。如下图所示，先给输入加上一个位置编码e，再计算attention。<br> 位置编码e可以是认为设置，也可以尝试通过学习的方法得到。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_9.png" alt="在这里插入图片描述"></p><h4 id="2-2-Multi-head-self-attention"><a href="#2-2-Multi-head-self-attention" class="headerlink" title="2.2 Multi-head self-attention"></a>2.2 Multi-head self-attention</h4><p>Multi-head self-attention是self-attention的进阶版本，其实就是每个输入的q、k、v的数量变多，解决的是相关性的形式可能不是一种，所以想要提取到更多的相关性。<br> 下面是Head&#x3D;2时的示意图。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_4.png" alt="在这里插入图片描述"></p><h4 id="2-3-Masked-Multi-head-self-attention"><a href="#2-3-Masked-Multi-head-self-attention" class="headerlink" title="2.3 Masked Multi-head self-attention"></a>2.3 Masked Multi-head self-attention</h4><p>self-attention计算了所有输入的相关性，但是有些任务中，输入是存在时间（或空间）差异的，即有的输入不需要与后面的输入计算相关性，所以采用masked的做法，遮掩其后面的输入。<br> 如下图所示，b2只与a1和a2有关，与a3、a4无关。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_7.png" alt="在这里插入图片描述"></p><h4 id="2-4-Residual（残差）"><a href="#2-4-Residual（残差）" class="headerlink" title="2.4 Residual（残差）"></a>2.4 Residual（残差）</h4><p>Residual的做法就是将输入加到输出上，作为最后的输出，这种思想在Resnet中提出。<br> 这样做的好处：解决了深度神经网络的退化问题，同等层数的前提下残差网络也收敛得更快（这里可以理解为通过计算残差，下一层中只需继续优化未匹配的地方，所以收敛快）</p><h4 id="2-5-Layer-normalization"><a href="#2-5-Layer-normalization" class="headerlink" title="2.5 Layer normalization"></a>2.5 Layer normalization</h4><p>Layer normalization是数据归一化的一种方式，计算均值和方差。即Transformer结构图中的Norm。</p><h3 id="3-Self-attention与CNN、RNN的对比"><a href="#3-Self-attention与CNN、RNN的对比" class="headerlink" title="3. Self-attention与CNN、RNN的对比"></a>3. Self-attention与CNN、RNN的对比</h3><h4 id="3-1-优点"><a href="#3-1-优点" class="headerlink" title="3.1 优点"></a>3.1 优点</h4><ul><li>Self-attention可并行计算</li><li>CNN是Self-attention的一个子集</li></ul><p>参考文献：<br> 《On the Relationship between Self-Attention and Convolutional Layers》<br> 《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</p><h4 id="3-2-缺点"><a href="#3-2-缺点" class="headerlink" title="3.2 缺点"></a>3.2 缺点</h4><p>attention只能处理固定长度的文本字符串。在输入系统之前，文本必须被分割成一定数量的段或块。这种文本块会导致上下文碎片化。例如，如果一个句子从中间分隔，那么大量的上下文就会丢失。</p><h3 id="4-Vision-Transformer中的Attention计算"><a href="#4-Vision-Transformer中的Attention计算" class="headerlink" title="4. Vision Transformer中的Attention计算"></a>4. Vision Transformer中的Attention计算</h3><p>下图中的Linear可以是线性层，也可以是卷积层，核心是改变通道数C产生Q、K、V，其他部分计算如下图所示。<br> <img src="/2023/06/05/17_Transformer%E8%AF%A6%E8%A7%A3/img_2.png" alt="在这里插入图片描述"></p><h3 id="5-Attention的pytorch实现"><a href="#5-Attention的pytorch实现" class="headerlink" title="5. Attention的pytorch实现"></a>5. Attention的pytorch实现</h3><p>代码来自<a href="https://github.com/Meituan-AutoML/Twins">https://github.com/Meituan-AutoML/Twins</a>，提供了local attention和标准attention的实现。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from functools import partial</span><br><span class="line">from timm.models.layers import DropPath, to_2tuple, trunc_normal_</span><br><span class="line">from timm.models.registry import register_model</span><br><span class="line">from timm.models.vision_transformer import _cfg</span><br><span class="line">from timm.models.vision_transformer import Block as TimmBlock</span><br><span class="line">from timm.models.vision_transformer import Attention as TimmAttention</span><br><span class="line"></span><br><span class="line">class GroupAttention(nn.Module):</span><br><span class="line">    def __init__(self, dim, num_heads=1, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., ws=1, sr_ratio=1.0):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        ws 1 for stand attention 这个注释有问题？？ws=1应该是单像素attention了</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(GroupAttention, self).__init__()</span><br><span class="line">        assert dim % num_heads == 0, f&quot;dim &#123;dim&#125; should be divided by num_heads &#123;num_heads&#125;.&quot;</span><br><span class="line"></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale or head_dim ** -0.5</span><br><span class="line"></span><br><span class="line">        # self.q = nn.Linear(dim, dim, bias=qkv_bias)</span><br><span class="line">        # self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)</span><br><span class="line">        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line">        self.ws = ws</span><br><span class="line"></span><br><span class="line">    def forward(self, x, H, W):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        There are two implementations for this function, zero padding or mask. We don&#x27;t observe obvious difference for</span><br><span class="line">        both. You can choose any one, we recommend forward_padding because it&#x27;s neat. However,</span><br><span class="line">        the masking implementation is more reasonable and accurate.</span><br><span class="line">        Args:</span><br><span class="line">            x:</span><br><span class="line">            H:</span><br><span class="line">            W:</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return self.forward_mask(x, H, W)</span><br><span class="line"></span><br><span class="line">    def forward_mask(self, x, H, W):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        pad_l = pad_t = 0</span><br><span class="line">        pad_r = (self.ws - W % self.ws) % self.ws</span><br><span class="line">        pad_b = (self.ws - H % self.ws) % self.ws</span><br><span class="line">        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))</span><br><span class="line">        _, Hp, Wp, _ = x.shape</span><br><span class="line">        _h, _w = Hp // self.ws, Wp // self.ws</span><br><span class="line">        mask = torch.zeros((1, Hp, Wp), device=x.device)</span><br><span class="line">        mask[:, -pad_b:, :].fill_(1)</span><br><span class="line">        mask[:, :, -pad_r:].fill_(1)</span><br><span class="line"></span><br><span class="line">        x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3)  # B, _h, _w, ws, ws, C</span><br><span class="line">        mask = mask.reshape(1, _h, self.ws, _w, self.ws).transpose(2, 3).reshape(1,  _h*_w, self.ws*self.ws)</span><br><span class="line">        attn_mask = mask.unsqueeze(2) - mask.unsqueeze(3)  # 1, _h*_w, ws*ws, ws*ws</span><br><span class="line">        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-1000.0)).masked_fill(attn_mask == 0, float(0.0))</span><br><span class="line">        qkv = self.qkv(x).reshape(B, _h * _w, self.ws * self.ws, 3, self.num_heads,</span><br><span class="line">                                            C // self.num_heads).permute(3, 0, 1, 4, 2, 5) # n_h, B, _w*_h, nhead, ws*ws, dim</span><br><span class="line">        q, k, v = qkv[0], qkv[1], qkv[2]  # B, _h*_w, n_head, ws*ws, dim_head</span><br><span class="line">        attn = (q @ k.transpose(-2, -1)) * self.scale  # B, _h*_w, n_head, ws*ws, ws*ws</span><br><span class="line">        attn = attn + attn_mask.unsqueeze(2)</span><br><span class="line">        attn = attn.softmax(dim=-1)</span><br><span class="line">        attn = self.attn_drop(attn)  # attn @v -&gt;  B, _h*_w, n_head, ws*ws, dim_head</span><br><span class="line">        attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)</span><br><span class="line">        x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)</span><br><span class="line">        if pad_r &gt; 0 or pad_b &gt; 0:</span><br><span class="line">            x = x[:, :H, :W, :].contiguous()</span><br><span class="line">        x = x.reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def forward_padding(self, x, H, W):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        pad_l = pad_t = 0</span><br><span class="line">        pad_r = (self.ws - W % self.ws) % self.ws</span><br><span class="line">        pad_b = (self.ws - H % self.ws) % self.ws</span><br><span class="line">        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))</span><br><span class="line">        _, Hp, Wp, _ = x.shape</span><br><span class="line">        _h, _w = Hp // self.ws, Wp // self.ws</span><br><span class="line">        x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3)</span><br><span class="line">        qkv = self.qkv(x).reshape(B, _h * _w, self.ws * self.ws, 3, self.num_heads,</span><br><span class="line">                                            C // self.num_heads).permute(3, 0, 1, 4, 2, 5)</span><br><span class="line">        q, k, v = qkv[0], qkv[1], qkv[2]</span><br><span class="line">        attn = (q @ k.transpose(-2, -1)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-1)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line">        attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)</span><br><span class="line">        x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)</span><br><span class="line">        if pad_r &gt; 0 or pad_b &gt; 0:</span><br><span class="line">            x = x[:, :H, :W, :].contiguous()</span><br><span class="line">        x = x.reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">class Attention(nn.Module):</span><br><span class="line">    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        assert dim % num_heads == 0, f&quot;dim &#123;dim&#125; should be divided by num_heads &#123;num_heads&#125;.&quot;</span><br><span class="line"></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale or head_dim ** -0.5</span><br><span class="line"></span><br><span class="line">        self.q = nn.Linear(dim, dim, bias=qkv_bias)</span><br><span class="line">        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        self.sr_ratio = sr_ratio</span><br><span class="line">        if sr_ratio &gt; 1:</span><br><span class="line">            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)</span><br><span class="line">            self.norm = nn.LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, H, W):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)</span><br><span class="line"></span><br><span class="line">        if self.sr_ratio &gt; 1:</span><br><span class="line">            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)</span><br><span class="line">            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)</span><br><span class="line">            x_ = self.norm(x_)</span><br><span class="line">            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)</span><br><span class="line">        else:</span><br><span class="line">            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)</span><br><span class="line">        k, v = kv[0], kv[1]</span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-2, -1)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-1)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(1, 2).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model = GroupAttention(dim=128, ws=2).cuda()</span><br><span class="line">    img = torch.randn((1, 16 * 32, 128)).cuda()</span><br><span class="line">    model.forward_padding(img, 16, 32)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda常用命令</title>
      <link href="/2022/12/19/15_conda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2022/12/19/15_conda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="conda常用命令-windows"><a href="#conda常用命令-windows" class="headerlink" title="conda常用命令(windows)"></a>conda常用命令(windows)</h2><p><strong>1. 查看conda版本</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure><p><strong>2. 更新至最新版本</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure><p><strong>3. 更新所有包</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update --all</span><br></pre></td></tr></table></figure><p><strong>4. 查看安装了哪些包</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><p><strong>5. 更新指定的包</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update package_name</span><br></pre></td></tr></table></figure><p><strong>6. 显示所有已经创建的环境</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info -e</span><br></pre></td></tr></table></figure><p><strong>7. 创建新环境</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n your_env_name python=x.x</span><br></pre></td></tr></table></figure><p><strong>8. 激活或者切换虚拟环境</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate your_env_name</span><br></pre></td></tr></table></figure><p><strong>9. 删除虚拟环境</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n your_env_name --all</span><br></pre></td></tr></table></figure><p><strong>10. 删除环境中的某个包</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name your_env_name package_name</span><br></pre></td></tr></table></figure><p><strong>11.安装与删除包</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 安装</span><br><span class="line">conda install package_name</span><br><span class="line"># 删除</span><br><span class="line">conda uninstall package_name</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>timm库使用</title>
      <link href="/2022/08/10/16_timm%E5%BA%93%E4%BD%BF%E7%94%A8/"/>
      <url>/2022/08/10/16_timm%E5%BA%93%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="一、timm库简介"><a href="#一、timm库简介" class="headerlink" title="一、timm库简介"></a>一、timm库简介</h2><p>PyTorch Image Models，简称timm，是一个巨大的PyTorch代码集合，整合了常用的models、layers、utilities、optimizers、schedulers、data-loaders&#x2F;augmentations和reference training&#x2F;validation scripts。</p><h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install timm</span><br></pre></td></tr></table></figure><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><ol><li>查看所有模型</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_list = timm.list_models()</span><br><span class="line">print(model_list)</span><br></pre></td></tr></table></figure><ol start="2"><li>查看具有预训练参数的模型</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_pretrain_list = timm.list_models(pretrained=True)</span><br><span class="line">print(model_pretrain_list)</span><br></pre></td></tr></table></figure><ol start="3"><li>检索特定模型</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_resnet = timm.list_models(&#x27;*resnet*&#x27;)</span><br><span class="line">print(model_resnet)</span><br></pre></td></tr></table></figure><ol start="4"><li>创建模型</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((1, 3, 256, 512))</span><br><span class="line">modle_mobilenetv2 = timm.create_model(&#x27;mobilenetv2_100&#x27;, pretrained=True)</span><br><span class="line">out = modle_mobilenetv2(x)</span><br><span class="line"># print(out.shape)</span><br><span class="line"># torch.Size([1, 1000])</span><br></pre></td></tr></table></figure><ol start="5"><li>创建模型–改变输出类别数</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((1, 3, 256, 512))</span><br><span class="line">modle_mobilenetv2 = timm.create_model(&#x27;mobilenetv2_100&#x27;, pretrained=True, num_classes=100)</span><br><span class="line">out = modle_mobilenetv2(x)</span><br><span class="line"># print(out.shape)</span><br><span class="line"># torch.Size([1, 100])</span><br></pre></td></tr></table></figure><ol start="6"><li>创建模型–改变输入通道数</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((1, 10, 256, 512))</span><br><span class="line">modle_mobilenetv2 = timm.create_model(&#x27;mobilenetv2_100&#x27;, pretrained=True, in_chans=10)</span><br><span class="line">out = modle_mobilenetv2(x)</span><br><span class="line"># print(out.shape)</span><br><span class="line"># torch.Size([1, 1000])</span><br></pre></td></tr></table></figure><ol start="7"><li>创建模型–只提取特征</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((1, 3, 256, 512))</span><br><span class="line">modle_mobilenetv2 = timm.create_model(&#x27;mobilenetv2_100&#x27;, pretrained=True, features_only=True)</span><br><span class="line">out = modle_mobilenetv2(x)</span><br><span class="line"># for o in out:</span><br><span class="line">#     print(o.shape)</span><br><span class="line"># torch.Size([1, 16, 128, 256])</span><br><span class="line"># torch.Size([1, 24, 64, 128])</span><br><span class="line"># torch.Size([1, 32, 32, 64])</span><br><span class="line"># torch.Size([1, 96, 16, 32])</span><br><span class="line"># torch.Size([1, 320, 8, 16])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
